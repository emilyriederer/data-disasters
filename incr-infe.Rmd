# Incredible Inferences (TODO) {#incr-infe}

Previously, we have seen how different inputs like data, tools, and methods can add risks to our data analysis. However, the battle is not won simply when we get our first set of *outputs*. In this chapter, we will explore common errors in interpreting the results of our analysis by exploring aspects of bias, missingness, and confounding. 

## Feature Leakage

```{r}
n <- 1000
minutes_month1 <- runif(n, 60, 1200)
minutes_month2 <- runif(n, 60, 1200) 
minutes_tot <- minutes_month1 + minutes_month2
df <- data.frame(minutes_month1, minutes_month2, minutes_tot)
```

```{r echo = FALSE}
corr_sep <- cor(minutes_month1, minutes_month2)
corr_sum <- cor(minutes_month1, minutes_tot)
```

Figure \@ref(fig:sep-sum) shows...

```{r sep-sum, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Correlation of idependent versus cumulative quantities'}
library(ggplot2)
library(patchwork)

gg_sep <- 
  ggplot(df) +
  aes(minutes_month1, minutes_month2) +
  geom_point() +
  labs(title = sprintf('Month 1 vs Month 2 \n (Corr: %f)', round(corr_sep, 2))) +
  theme(
    axis.title = element_blank(),
    plot.title = element_text(hjust = 0.5))

gg_sum <-
  ggplot(df) +
  aes(minutes_month1, minutes_tot) +
  geom_point() +
  labs(title = sprintf('Month 1 vs Months 1-2 \n (Corr: %f)', round(corr_sum, 2))) +
  theme(
    axis.title = element_blank(),
    plot.title = element_text(hjust = 0.5))

gg_sep + gg_sum
```





<!--

## No law to use ALL the data

## Ascribing characteristics at wrong granularity

ecological fallacy

(does this belong in causation chapter?)

## Finding policy-induced relationships

selection bias

## Ignoring heterogeneity

## "If trends continue"

## Analyzing time-to-event data

immortal time bias

## Answering the right question

Don't let available tools dictate the questions of interest

*The Cult of Statistical Significance* [@ziliak_mccloskey]

"Mindless Statistics" [@GIGERENZER2004587]

## Misguided Rigor

```{r}
set.seed(123)
t <- t.test(rnorm(100), rnorm(100))
print(t)
t$p.value
```

```{r}
pvals <- vapply(1:1000, FUN = function(x) t.test(rnorm(100), rnorm(100))$p.value, FUN.VALUE = numeric(1)) 
alpha <- 0.05
sum(pvals > (1-alpha/2) | pvals < alpha/2) / length(pvals)
```

```{r}
get_prop_sign <- function(n = 1000, alpha = 0.05) {

  pvals <- vapply(1:n, FUN = function(x) t.test(rnorm(100), rnorm(100))$p.value, FUN.VALUE = numeric(1))
  prop <- sum(pvals > (1-alpha/2) | pvals < alpha/2) / length(pvals)
  return(prop)

}
```


Data dredging, p-hacking

## Sample splitting

The **nullabor** [@R-nullabor] R package

## Age Period Cohort

## Strategies

-->
