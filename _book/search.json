[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Training data analysis often begins Statistics 101 course.\nStudents learn “happy path” answer data adheres specific assumptions (“independent identically distributed Normal density”) answers pre-specified questions (notably, infamous null hypothesis significance test).\n, venture world real-world data analysis non-experimental data rarely well behaved questions asked far nuanced.one course aim teach students everything know statistics.\nfact, one best parts career statistics responsibility privilege life-long learning.\nHowever, flaw introductory statistics ’s incomplete, ’s obvious complete.\nStatistics bad salesman.\n’s season finale, cliff hanger, teasing hinting promising better come.\nStudent may leave studies believing answering complex data analysis questions trivially easy (relying one-size-fits-“panacea” learned) intractably difficult (assumptions method met.)book attempts add color dimensions data analysis showcasing nuances throughout true life cycle data analysis using two strategies.First, attempts highlight common pitfalls parts data analysis: data management computation visualization, interpretation, modeling even communication collaboration.\nData analysis fundamentally creative task, rarely canonical one-size-fits-solutions.\nCuriously, however, plenty canonical issues even require different solutions different settings.\nThus, goal book highlight common data disasters , , help students cultivate intuition detect common problems occur important analysis.Second, exploring data disasters, humbly put forth (woefully incomplete!) literature review advanced methods statistics quantitative disciplines (e.g. economics, epidemiology), help learners build “mental index” terms search techniques study encounter relevant problem.","code":""},{"path":"index.html","id":"main-topics","chapter":"Preface","heading":"0.1 Main Topics","text":"particular, aim help avoid eleven types data disasters:Data Dalliances: Misinterpreting misuing data based collected representsComputational Quandaries: Letting computers said meantEgregious Aggregations: Losing critical information information condensedVexing Visualization: Confusing others plotting choicesIncredible Inferences: Drawing incorrect conclusions analytical resultsCavalier Causality: Falling prey spurious correlations masquerading causalityMindless Modeling: Failing get value models tailoring features, targets, performance metricsAlternative Algorithms: Lacking understanding alternative methods may better suited problem handFutile Findings: Asking answering questions aren’t usefulComplexifying Code: Making projects unwieldy difficult understand necessaryRejecting Reproducibility: Working inefficiently instead efficient, reproducible, sharable workflow","code":""},{"path":"index.html","id":"common-themes","chapter":"Preface","heading":"0.2 Common Themes","text":"chapter, see numerous examples disaster consider strategies help us mitigate.\nAlong way, ’ll emphasize:importance domain knowledge data-generating process decide want doThe utility simulation tool explore , fact, itThe exploration counterexamples build intuition common patterns problems even common solutions don’t existAs go, notice three common themes challenge focus introductory statistics:Summary statistics mask interesting stories see focusing variationSimilarly, observations variables rarely independent; story covarianceAssumptions Normality, broadly symmetry, often appropriate wonky, highly skewed world","code":""},{"path":"about-the-author.html","id":"about-the-author","chapter":"About the Author","heading":"About the Author","text":"Emily Riederer …Find website Twitter","code":""},{"path":"introduction-todo.html","id":"introduction-todo","chapter":"1 Introduction (TODO)","heading":"1 Introduction (TODO)","text":"Statistics synonymous data analysis; rigor vs practicality“Evaluating Success Data Analysis” (Hicks Peng 2019)“Data Alone Ground Truth” (Bassa 2017)","code":""},{"path":"introduction-todo.html","id":"what-is-data","chapter":"1 Introduction (TODO)","heading":"1.1 What is data?","text":"Data …","code":""},{"path":"introduction-todo.html","id":"what-is-analysis","chapter":"1 Introduction (TODO)","heading":"1.2 What is analysis?","text":"Analysis process turning information insight…S","code":""},{"path":"introduction-todo.html","id":"what-is-workflow","chapter":"1 Introduction (TODO)","heading":"1.3 What is workflow?","text":"Workflows intentional process accomplishing complex goal…","code":""},{"path":"introduction-todo.html","id":"what-is-data-analysis","chapter":"1 Introduction (TODO)","heading":"1.4 What is data analysis?","text":"Data analysis altogether …","code":""},{"path":"introduction-todo.html","id":"what-are-data-disasters","chapter":"1 Introduction (TODO)","heading":"1.5 What are data disasters?","text":"Data disasters occur …","code":""},{"path":"data-dall.html","id":"data-dall","chapter":"2 Data Dalliances (WIP)","heading":"2 Data Dalliances (WIP)","text":"first step data analysis , fact, data. may seem obvious, statistics textbooks often dodge detail.\nDiscussions regression analysis often begin statement like:“Let \\(X\\) \\(n x p\\) design matrix independent variables…”practice statement absurd writing book win basketball game, assuming team already 20 point lead 1 minute left play.’s convenient typically incorrect assume data happen ideal (, humbly, sufficient) data questions wish analyze.\nspecific vagaries data vary greatly domain, commonality across many fields (political science, economics, epidemiology, market research) often called work found data (, formally, “observational data”) administrative sources production systems.\ncontrast artisanally crafted data experimental data (like carefully controlled agricultural experiments motivated many early methods developments statistics), data generated neither us us.\nquote Angela Bassa, head data science e-commerce company: “Data isn’t ground truth. Data artifacts systems” (Bassa 2017).analytical implications observational versus experimental data well explored field causal inference (discuss Chapters 6 7).\nHowever, distinction implications far earlier data analysis process, well.\nname :Records fields many represent entities measures conducive analysisData collection methods may capture different subset events different frequency expected, leading systemic biasesData movement systems can insert errors (, minimum, challenges intuition)Data transformations may fragile transient, reflecting primary purpose system unrelated analytical useIn chapter, explore data structures full data generating process better understand different types data challenges emerge.\n, hone sharper intuition data can deceive us watch beginning analysis.","code":""},{"path":"data-dall.html","id":"preliminaries","chapter":"2 Data Dalliances (WIP)","heading":"2.1 Preliminaries","text":"begin exploration data dalliances, must first establish baseline understanding data structure, data production, data quality.","code":""},{"path":"data-dall.html","id":"data-structure-basics","chapter":"2 Data Dalliances (WIP)","heading":"2.1.1 Data Structure Basics","text":"","code":""},{"path":"data-dall.html","id":"relational-data-structure","chapter":"2 Data Dalliances (WIP)","heading":"2.1.1.1 Relational Data Structure","text":"Understanding content structure data using critical prerequisite analysis.\nbook, focus tabular, structured data like one might find Excel spreadsheet relational database.1In particular, many tools work best R developer Hadley Wickham describes “tidy data” (Wickham 2014). Namely:variable forms columnEach observation forms rowEach type observational unit forms tableThis analogous one generally finds data arranged database statisticians used conceptualizing .\nexample, design matrix linear model consists one column data independent variable included model one row observation.2\nWickham points , also similar called “3rd normal form” world relational database management systems.Using data structure valuable similar many modern data tools expect, also provides us framework think critically defined observation variable dataset.","code":""},{"path":"data-dall.html","id":"schemas-todo","chapter":"2 Data Dalliances (WIP)","heading":"2.1.1.2 Schemas (TODO)","text":"","code":""},{"path":"data-dall.html","id":"data-production-processes","chapter":"2 Data Dalliances (WIP)","heading":"2.1.2 Data Production Processes","text":"statistical modeling discuss data generating process: can build models describe mechanisms create observations.\ncan broaden notion think generating process steps data production.Regardless type data (experimental, observational, survey, etc.), generally four main steps production: collection, extraction, loading, transformation.3Collect: way signals real world captured data. include logging (e.g. web traffic system monitoring), sensors (e.g. temperature collection), surveys, moreExtract: process removing data place originally captured preparation moving somewhere analysis can doneLoad: process loading extracted data final destinationTransform: process modeling transforming data structure useful analysis variables interpretableTo better theorize data quality issues, ’s useful think four DGPs (shown Figure 2.1): real-world DGP, data collection/extraction DGP4, data loading DGP, data transformation DGP.\nFIGURE 2.1: schematic data production process\n","code":""},{"path":"data-dall.html","id":"e-commerce-data-example","chapter":"2 Data Dalliances (WIP)","heading":"2.1.2.1 E-Commerce Data Example","text":"example, consider role four DGPs e-commerce data:Real-world DGP: Supply, demand, marketing, range factors motivate consumer visit website make purchaseData collection DGP: Parts website instrumented log certain customer actions. log extracted different operational system (login platforms, payment platforms, account records) used analysisData loading DGP: Data recorded different systems moved data warehouse processing sort manual, scheduled, orchestrated job. different systems may make data available different frequencies.Data transformation DGP: arrive final data presentation requires creating data model describe domain-specific attributes key variables crafted data transformations","code":""},{"path":"data-dall.html","id":"subway-data-example","chapter":"2 Data Dalliances (WIP)","heading":"2.1.2.2 Subway Data Example","text":", consider role four DGPs subway ridership data5:Real-world DGP: Riders motivated use public transportation commute, run errands, visit friends. Different motivating factors may cause different weekly annual seasonalityData collection DGP: ride subway, riders go station enter exit turnstiles. mechanical rotation turnstile caused rider passing recordedData loading DGP: Data recorded turnstile collected centralized computer system station. week, station uploads flat file data data lake owned city’s Department TransportationData transformation DGP: Turnstiles different companies may different data formats. Transformation may include harmonizing disparate sources, coding system-generated codes (e.g. Station XYZ) semantically meaningful names (e.g. Main Street Station), publishing final unified representation across stations across timeThroughout chapter, ’ll explore understanding key concepts DGPs can help guide intuition look problems.","code":""},{"path":"data-dall.html","id":"data-quality-dimension","chapter":"2 Data Dalliances (WIP)","heading":"2.1.3 Data Quality Dimension","text":"guide discussion data production can affect aspects data quality, need guiding definition data quality.\nchallenging data quality subjective task-specific.\nmatters much data “fit purpose” operates way transparent users moreso meeting preordained quality standard.Regardless, ’s useful discussion think general dimensions data quality. , rely six dimensions data quality outlined Data Management Association (“Six Dimensions Data Quality Assessment” 2020).\nofficial definitions :Completeness: proportion stored data potential “100% complete”Uniqueness: Nothing recorded based upon thing identified. inverse assessment level duplicationTimeliness: degree data represent reality required point timeValidity: Data valid conforms syntax (format, type, range) definitionAccuracy: degree data correctly describes “real world” object event\ndescribed.Consistency: absence difference, comparing two representations \nthing definition","code":""},{"path":"data-dall.html","id":"questions-to-ask-todo","chapter":"2 Data Dalliances (WIP)","heading":"2.1.4 Questions to Ask (TODO)","text":"goal understanding data ensure can assess data quality fit purpose. Understanding structure production process helps accomplish .","code":""},{"path":"data-dall.html","id":"data-collection","chapter":"2 Data Dalliances (WIP)","heading":"2.2 Data Collection","text":"One tricky nuances data collection understanding precisely getting captured logged first place.\nmatter robust sensors, loggers, mechanisms record dataset, data still unfit purpose long analyst fully understand represents.\nnext section, see data gets collected (understanding ) can alter notions data completion must handle computations.","code":""},{"path":"data-dall.html","id":"what-makes-a-record-row","chapter":"2 Data Dalliances (WIP)","heading":"2.2.1 What Makes a Record (Row)","text":"first priority starting work dataset understanding single record (row) represents causes generated.Consider something simple login system users must enter credentials, endure Captcha-like verification process prove robot, enter multi-factor authentication code. Figure 2.2 depicts process.\nFIGURE 2.2: diagram illustrating multi-step process user login website app\nevents gets collected recorded significant impact subsequent data processing.\ntechnical sense, inclusion/exclusion decision incorrect, per say, producers’ choices don’t match consumers’ understandings, can lead misleading results.example, analyst might seek logins table order calculate rate successful website logins.\nReasonably enough, might compute rate sum successful events total.\nNow, suppose two users attempt login account, ultimately, one succeeds accessing private information doesn’t.\nanalyst probably hope compute report 50% login success rate. However, depending data represented, quite easily compute nearly value 0% 100%.thought experiment, can consider types events might logged:Per Attempt: data logged per overall login attempt, successful attempts trigger one event, user forgot password may try (fail) login multiple times. case illustrated , deflates successful login rate 25%.Per Event: logins table contains row every login-related event, ‘success’ trigger large number positive events ‘failure’ trigger negative event preceded zero positive events. case illustrated , inflates successful login rate 86%.Per Conditional: collector decided look downstream events, perhaps circumvent record duplication, might decide create record denote success failure final step login process (MFA). However, login attempts failed upstream step generate record stage ’ve already fallen funnel. case, computed rate reach 100%Per Intermediate: Similarly, login defined specifically successful password verification, computed rate 100% even users subsequently fail MFAThese different situations illustrated Figure 2.3, calculations shown Table 2.1.\nFIGURE 2.3: Login events recorded different data collection paradigms\nTABLE 2.1: Success rate naively computed different data collection schemes(Note explicitly “simulate” data , workflow largely . imagine real-world process, might translate digital representation, created numerical example understand implications. simulations require fancy code; sometimes paper pencil thought experiment works just fine.)humans shared intuition concepts like user, session, login , act collecting data forces us map intuition onto atomic event.\nmisunderstanding precisely definition can massive impact perceived data quality; “per event” data appear heavily duplicated assumed “per session” data.cases, obvious detect.\nsystem outputs fields incredibly specific (e.g. hyperbole, imagine step_in_the_login_process field values taking human-readable descriptions fifteen processes listed image ), depending source organized (e.g. contrast , fields like sourceid processid unintuitive alphanumeric encoded values) defined, nearly impossible understand nuances without uncovering quality metadata talking data producer.","code":""},{"path":"data-dall.html","id":"what-doesnt-make-a-record-row","chapter":"2 Data Dalliances (WIP)","heading":"2.2.2 What Doesn’t Make a Record (Row)","text":"Along thinking count (gets logged), ’s equally important understand systemically generate record. Consider users intent desire login (motivated real-world DGP) find login page, users load login page never click button know ’ve forgotten password see way request .\nOften, corner cases may critical informative (e.g. , demonstrating major flaws UI).\n’s hard computationally validate data doesn’t exist, conceptual data validation critical.","code":""},{"path":"data-dall.html","id":"records-versus-keys","chapter":"2 Data Dalliances (WIP)","heading":"2.2.3 Records versus Keys","text":"preceding discussion types real-world observations generate records resulting dataset related distinct another important concept world relational databases: primary keys.Primary keys minimal subset variables dataset define unique record.\nexample, previous discussion customer logins might consist natural keys6 combination session_id timestamp surrogate keys7 global event_id generated every time system logs event.Understanding table’s primary keys can useful many reasons.\nname reasons, fields often useful linking data one table another identifying data errors (uniqueness fields upheld).\nalso can suggestive true granularity table.However, simply knowing table’s primary keys resolve issues discussed prior two sections.\nmany different data collection strategies considered unique session timestamp;\nhowever, ’ve seen, guarantee must contain every session timestamp universe events.","code":""},{"path":"data-dall.html","id":"what-defines-a-variable-column","chapter":"2 Data Dalliances (WIP)","heading":"2.2.4 What Defines a Variable (Column)","text":"Just critical understanding constitutes record (row) dataset understanding precise definition variable (column).\nSuperficially, task seems easier: , variable name hopefully includes semantic information. However, quite often information can provide false sense security.\nJust identify variable promising sounding name, mean relevant data analysis.example, consider wanting analyze patterns customer spend amounts across orders e-commerce website.\nmight find table orders field called amt_spend. might mean?dataset sourced payment processor, likely includes total amount billed customers’ credit card: including item prices less discounts, shipping costs, taxes, etc. Alternatively, order split across gift card credit card, field might reflect amount charged credit cardIf dataset created Finance, might perhaps include total item prices less discounts best corresponded data Finance team needs revenue reportingSomeone, somewhere, point might assigned amt_spend name variable containing gross spend (accounting discounts) might different variable amt_spend_net accounts discounts appliedIt’s critical understand variable actually means.\nupside forces analysts think crisply research questions ideal variables analysis .\n’ve seen, concepts like “spend” may seem deceptively simple, unambiguous.","code":""},{"path":"data-dall.html","id":"data-extraction-loading","chapter":"2 Data Dalliances (WIP)","heading":"2.3 Data Extraction & Loading","text":"Checking data contains expected expected records (, completeness, uniqueness, timeliness) one common first steps data validation.\nHowever, superficially simple act loading data data warehouse updating data tables can introduce variety risks data completeness require different strategies detect.\nData loading errors can result data stale, missing, duplicate, inconsistently --date across sources, complete subset range think.data quality principles completeness, uniqueness, timeliness suggest records exist , reality many haphazard data loading process means data may appear sometime zero handful times. Data loads can occur many different ways.\nexample, might :manually executedscheduled (like cron job)orchestrated (tool like Airflow Prefect)approach free challenges.\nexample, scheduled jobs risk executing upstream process completed (resulting stale missing data);\npoorly orchestrated jobs may prevented working due one missing dependency might allow multiple stream get sync (resulting multisource missing data).\nRegardless method, approaches must carefully configured handle failures gracefully avoid creating duplicates, frequency executed may cause partial loading issues incompatible granularity source data.","code":""},{"path":"data-dall.html","id":"data-load-failure-modes","chapter":"2 Data Dalliances (WIP)","heading":"2.3.0.1 Data Load Failure Modes","text":"develop understanding true data generating process formulate theories data broken (validate), useful understand different ways data extraction loading can fail.Figure 2.4 illustrates number examples. Suppose row boxes diagram represents one day records table.\nFIGURE 2.4: Different modes data loading failure\ndataset might susceptible :Stale data occurs data --date expected regular refresh cadence. happen manual step skipped, scheduled job executed upstream source available, orchestrated data checks found errors quarantined new recordsMissing data occurs one data load fails subsequent loads succeededDuplicate data occurs one data load executed multiple timesMultisource missing data occurs table loaded multiple sources, continued update expected others notPartial data occurs table loaded correctly intended producer contains less data expected consumer (e.g. table loads ever 12 hours data given date, user assumes relevant records date loaded)differences failure modes become important analyst attempts assess data completeness.\nOne first approaches analyst might consider simply check min() max() event dates table.\nHowever, can help detect stale data.\ncatch missing data, analyst might instead attempt count number distinct days represented data; detect duplicate data, analyst might need count records day examine pattern.case like toy example correct number rows per date highly predictable number dates small, eyeballing feasible;\nhowever expected number records varies day--day time series long, approach becomes subjective, error-prone, intractable.\nAdditionally, still might hard catch errors mutli-source data partial loads lower number records still within bounds reasonable deviation series.\nlast two types deserve exploration.","code":""},{"path":"data-dall.html","id":"multi-source","chapter":"2 Data Dalliances (WIP)","heading":"2.3.0.2 Multi-Source","text":"effective strategy assessing data completeness requires better understanding data collected loaded.\ncase multi-source data, one single source stopping loading may big enough change disrupt aggregate counts still jeopardize meaningful analysis.\nuseful conduct completeness checks subgroup identify discrepancies.subgroup ;\nsubgroup must correspond various data sources.\nexample, suppose run e-commerce store wish look sales past month category.\nNaturally, might think check completeness data category.\nsales data sourced three separate locations: Shopify site (80%), Amazon Storefront (15%), phone sales (5%).\nUnless explicitly check completeness channel (dimension don’t particularly care analysis), easy miss data source phone sales stopped working loads different frequency.Another interesting aspect multi-source data, multiple sources can contribute either different rows/records different columns/variables.\nTable-level frequency counts won’t help us latter case since sources might create right total number records result specific fields records missing inaccurate.","code":""},{"path":"data-dall.html","id":"partial-loads","chapter":"2 Data Dalliances (WIP)","heading":"2.3.0.3 Partial Loads","text":"Partial loads really data errors , still important detect since can jeopardize analysis.\ncommon scenario might occur job loads new data every 12 hours (say, data morning afternoon day n-1 loads day n 12AM 12PM, respectively).\nanalyst retrieving data 11AM may concerned see approximate ~50% drop sales past day, despite confirming data looks “complete” since maximum record date , fact, day n-1.\ncourse, concern somewhat easily allayed checked timestamp field, field might exists might used validation since harder anticipate appropriate maximum timestamp maximum date.","code":""},{"path":"data-dall.html","id":"delayed-or-transient-records","chapter":"2 Data Dalliances (WIP)","heading":"2.3.0.4 Delayed or Transient Records","text":"interaction choices made data collection data loading phases can introduce sets problems.Consider orders table e-commerce company analysts may use track customer orders.\nmight contain one record per order_id x event (placement, processing, shipment), one record per order placed, one record per order shipping, one record per order status field changes time denote order’s current stage life. options illustrated Figure 2.5.\nFIGURE 2.5: Illustration alternative data collection extraction strategies order data\nmodeling choices seem reasonable difference might appear immaterial.\nconsider collection choice record report shipped events.\nPerhaps might operationally easier shipment come one source system whereas orders come many.\nHowever, interesting thing shipments often lagged variable way order date.Suppose e-commerce company question offers three shipping speeds checkout. Figure 2.6 shows range possible shipment dates based order dates three different speeds (shown different bars/colors).\nFIGURE 2.6: conceptual chart different classes real-world events might materialize records dataset\nmight effect perceived data quality?Order data appear stale timely since orders given order_date load days later shippedSimilar missing multisource data, data range table lead deceptive incomplete data validation orders later order date might ship (thus logged) orders previous order datePut another way, multiple order dates demonstrating partial data loadsThese features data might behave inconsistently across time due seasonality (e.g. shipping weekends federal holidays), heuristics developed clean data based small number observations failFrom analytical perspective, orders faster shipping disproportionately overrepresented “tail” (recent) data. shipping category correlated characteristics like total order spend, create artificial trend dataOnce , understanding data collected point shipment reasoning shipment timing varies impacts loading necessary successful validation.thought experiment seems vague, can make concrete mocking dataset experiment.simplest version, simply suppose one order submited 10 days dates (represented convenience integers calendar dates) given dt_subm vector.\nSuppose shipping always takes three days, can easily calculate shipment date (dt_ship) based submission date.\nshipment date date data logged loaded (dt_load).Suppose analyst living day 5 wonder many orders submitted day 3.\ncan observe shipments loaded day 5 filter data accordingly.\nHowever, count many records exist day 3 find none.\nInstead, move ahead analysis date day 7, able observe orders submitted day 3.(Note conditions checked much succinctly base R expression sum(df$dt_load < 7 & df$dt_subm == 3).\nHowever, sometimes virtue option readable code even less compact.\n, prefer verbose option claritfy exposition.\ntrade-offs, general thoughts coding style, explored Chapter 11.)Now, may seem trivial. Clearly, zero records day, catch data validation, right?\ncan make synthetic data slightly realistic better illustrate problem.\nLet’s imagine 10 orders day, order shipped sometime 2 4 days order equal probability.repeat prior analysis, now see records orders submitted day 3 time begin analysis day 5.\ncase, might easily tricked believe orders.\nHowever, repeat analysis day 7, see number orders day 3 increased.course, can imagine real world yet much complicated example.\nreality, random number orders day.\nAdditionally, might mixture different types orders.\nmight high-priced orders customers tended willing pay faster shipping,\nlow-priced orders customers tend chose slower shipping.\ncase like , might naive validation miss lack data completeness, sample shipments begin see day 5 unrepresentative population orders placed day 3.\ntype selection bias examine Chapter 6 (Incredible Inferences).","code":"\n# data simulation: single orders + deterministic ship dates ----\ndt_subm <- 1:10\ndays_to_ship <- 3\ndt_ship <- dt_subm + days_to_ship\ndt_load <- dt_ship\ndf <- data.frame(dt_subm, dt_ship, dt_load)\nhead(df)##   dt_subm dt_ship dt_load\n## 1       1       4       4\n## 2       2       5       5\n## 3       3       6       6\n## 4       4       7       7\n## 5       5       8       8\n## 6       6       9       9\nlibrary(dplyr)\n\n# how many day-3 orders do we observe as of day-5? ----\ndf %>% \n  filter(dt_load <= 5) %>% \n  filter(dt_subm == 3) %>% \n  nrow()## [1] 0\n# how many day-3 orders do we observe as of day-7? ----\ndf %>% \n  filter(dt_load <= 7) %>% \n  filter(dt_subm == 3) %>% \n  nrow()## [1] 1\n# data simulation: multiple orders + random ship dates ----\ndt_subm <- rep(x = 1:10, each = 10)\ndays_to_ship <- sample(x = 2:4, size = length(dt_subm), replace = TRUE)\ndt_ship <- dt_subm + days_to_ship\ndt_load <- dt_ship\ndf <- data.frame(dt_subm, dt_ship, dt_load)\nhead(df)##   dt_subm dt_ship dt_load\n## 1       1       5       5\n## 2       1       3       3\n## 3       1       5       5\n## 4       1       5       5\n## 5       1       3       3\n## 6       1       3       3"},{"path":"data-dall.html","id":"data-encoding-transformation-wip","chapter":"2 Data Dalliances (WIP)","heading":"2.4 Data Encoding & Transformation (WIP)","text":"data loaded suitable location processing analysis (data warehouse), often undergoes numerous transformations change shape, structure, content suited analytical use.example, recall logins table discussed . might filtered cleaner versions represents subset events, event identifiers like '1436' might recoded human-readable names like 'mfa-success'.Unfortunately, although steps attempt increase data’s usability, also immune inserting bugs.","code":""},{"path":"data-dall.html","id":"data-encoding","chapter":"2 Data Dalliances (WIP)","heading":"2.4.1 Data Encoding","text":"","code":""},{"path":"data-dall.html","id":"data-types","chapter":"2 Data Dalliances (WIP)","heading":"2.4.1.1 Data Types","text":"One critical set decisions data encoding sort data types field interest . Data types integers, reals, character strings, logicals, dates, times determine data stored types manipulations can done .’ll see examples computational implications data types Chapter 3 (Computational Quandaries). chapter specifically explores unique complexities string date types.","code":""},{"path":"data-dall.html","id":"indicator-variables-todo","chapter":"2 Data Dalliances (WIP)","heading":"2.4.1.2 Indicator Variables (TODO)","text":"positive case?“bunch zeros coded ones ones coded zeroes.”(https://retractionwatch.com/2013/01/04/paper--evidence--environmental-racism--epa-polluter-fines-retracted--coding-error/)","code":""},{"path":"data-dall.html","id":"general-representation-todo","chapter":"2 Data Dalliances (WIP)","heading":"2.4.1.3 General Representation (TODO)","text":"“data sets often multiple files …unclear sometimes duplicative variables. complexities commonplace among many data systems… surprised coding errors fairly common, ones discovered constitute “tip iceberg.”\n”(https://retractionwatch.com/2015/09/10/divorce-study-felled---coding-error-gets--second-chance/)","code":""},{"path":"data-dall.html","id":"the-many-meanings-of-null","chapter":"2 Data Dalliances (WIP)","heading":"2.4.1.4 The Many Meanings of Null","text":"Another major encoding decision handle null values.\nPreviously, discussion Data Collection, considered presence absence full records.\nHowever, preparing data analysis, data produces consumers need decide cope presence absence individual fields.records contain relevant information, may published explicitly missing fields full record may published . difference implicit explicit missingness resulting data illustrated Figure 2.7.\nFIGURE 2.7: comparison explicit versus implicit missingness\nUnderstanding system implies explicitly missing data field also critical validation analysis.\nChecks data completeness usually include counting null values, null data isn’t always incorrect.\nfact, null data can highly informative know means. meanings null data might include:Field relevant: Perhaps logins table reports mobile phone operating system (iOS Android) used access login page track platform-specific issues. However, valid value thisRelevant value known: logins table might also account_id field attempts match login attempts known accounts/customers using different metadata like cookies IP addresses. theory, almost everyone trying log account identifier, methods may good enough identify casesRelevant value null: course, sometimes someone without account might try log reason. case, correct value account_id field truly nullRelevant value recorded incorrectly: Sometimes systems glitches. Without doubt, every single login attempt timestamp, field null data somehow lost corrupted sourceSimilarly, different systems might might report nulls different ways :True nulls: Literally entry resulting dataset nullNull-like non-nulls: Blank values like empty string ('') contain null amount information won’t detected counting null valuesPlaceholder values: Meaningless values like account_id 00000000 unidentified accounts preserve data validity (expected structure) intrinsic meaningSentinel/shadow values: Abnormal values attempt indicate reasons null-ness account_id -1 browser cookies found -2 cookies found help link specific customer recordEach encoding choices changes definitions appropriate completeness validity field , even critically, impacts expectations assertions form data accuracy.\ncan’t expect 100% completeness nulls relevant value; can’t check validity ranges easily sentinel values used values outside normal range (hopefully, much bigger problems!)\n, understanding upstream systems work essential assessing work.Similarly, understanding null data collected significant implications subsequently process . discuss Chapter 3 (Computational Quandaries).","code":""},{"path":"data-dall.html","id":"data-transformation","chapter":"2 Data Dalliances (WIP)","heading":"2.4.2 Data Transformation","text":"Finally, data roughly want , likely undergoes many transformations translate system-generated fields discussed data collection semantically-relevant dimensions analytical consumers. course, types transformations done innumerable far variation data loading. , ’ll just look examples common failure patterns.","code":""},{"path":"data-dall.html","id":"pre-aggregation","chapter":"2 Data Dalliances (WIP)","heading":"2.4.2.1 Pre-Aggregation","text":"Data transformations may include aggregating data higher levels granularity easier analysis. example, transformation might add item-level purchase data make easier analyst look spend per order specific user.Data transformations transform data, also transform dimensions data quality manifest. data completeness uniqueness issues discussed data loading pre-aggregated, problems can turn problems accuracy. example, duplicate partial data loads discussed aggregated suggest inaccurately high low quantities respectively.","code":""},{"path":"data-dall.html","id":"field-encoding","chapter":"2 Data Dalliances (WIP)","heading":"2.4.2.2 Field Encoding","text":"assess data consistency across tables,Categorical fields data set might created number ways including:Directly taken sourceCoded transformation scriptTransformed logic shared user-defined function (UDFs) macroJoined shared look-tableEach approach different implications data consistency usability.Using fields source simply – ’s subjectivity room manual human error. multiple tables come source, ’s likely guaranteed encoded way.Coding transformations ELT process easy data producers. ’s need coordinate across multiple processes use cases, transformation can immediately modified needed. However, lack coordination can lead different results fields .Alternatively, macros, UDFs, look-tables provided centralized ways map source data inputs desired analytical data outputs systemic consistent way. course, centralization challenges. something source data changes, process updating centralized UDF look-table may slowed need seek consensus collaborate. , data consistent potentially less accurate.Regardless, engineered values require scrutiny – paticularly used key join multiple tables – distinct values carefully examined.","code":""},{"path":"data-dall.html","id":"updating-transformations","chapter":"2 Data Dalliances (WIP)","heading":"2.4.2.3 Updating Transformations","text":"course, data consistency problem across different data sources within one data source. Regardless method field encoding used previous step, intersection data loading data transformation strategies can introduce data consistency errors time.Often, computation efficiency, analytical tables loaded using incremental loading strategy. means new records (determined time period, set unique keys, criteria) upstream source loaded downstream table. contrast full refresh entire downstream table recreated update.Incremental loads many advantages. Rebuilding tables entirety can time consuming computationally expensive. particular, non-cloud data warehouses able scale computing power demand, sort heavy duty processing job can noticeably drain resources queries trying run database. Additionally, upstream staging data ephemeral, fully rebuilding table mean failing retain history.However, case data transformations change, incremental loads may introduce inconsistency data overtime new records created inserted new logic.also problem broadly short-term error discovered either data loading transformation historical data. Incremental strategies may always update include corrected version data.Regardless, underscores need validate entire datasets re-validate repulling data.","code":""},{"path":"data-dall.html","id":"more-on-missing-data-todo","chapter":"2 Data Dalliances (WIP)","heading":"2.5 More on Missing Data (TODO)","text":"MCAR / MAR / MNAR","code":""},{"path":"data-dall.html","id":"other-types-of-data-todo","chapter":"2 Data Dalliances (WIP)","heading":"2.6 Other Types of Data (TODO)","text":"","code":""},{"path":"data-dall.html","id":"survey-data","chapter":"2 Data Dalliances (WIP)","heading":"2.6.1 Survey Data","text":"","code":""},{"path":"data-dall.html","id":"human-generated","chapter":"2 Data Dalliances (WIP)","heading":"2.6.2 Human-Generated","text":"","code":""},{"path":"data-dall.html","id":"strategies-todo","chapter":"2 Data Dalliances (WIP)","heading":"2.7 Strategies (TODO)","text":"","code":""},{"path":"data-dall.html","id":"understand-the-intent","chapter":"2 Data Dalliances (WIP)","heading":"2.7.1 Understand the intent","text":"data originally collected? purpose?clear, unique definitions key concepts (e.g. entites, measures)? ?Documentation (metadata, dictionaries)","code":""},{"path":"data-dall.html","id":"understand-the-execution","chapter":"2 Data Dalliances (WIP)","heading":"2.7.2 Understand the execution","text":"learn real-world systemsunderstand key steps data production processdocumentation (lineage, provenance)","code":""},{"path":"data-dall.html","id":"seek-expertise","chapter":"2 Data Dalliances (WIP)","heading":"2.7.3 Seek expertise","text":"talking experts (upstream downstream)","code":""},{"path":"data-dall.html","id":"trust-but-verify","chapter":"2 Data Dalliances (WIP)","heading":"2.7.4 Trust but verify","text":"always data validation\nsummaries\ncontext-informed assertions\nsummariescontext-informed assertionsexploratory analysis","code":""},{"path":"data-dall.html","id":"real-world-disasters","chapter":"2 Data Dalliances (WIP)","heading":"2.8 Real World Disasters","text":"","code":""},{"path":"data-dall.html","id":"data-loading-artificially-spikes-covid-cases","chapter":"2 Data Dalliances (WIP)","heading":"2.8.1 Data loading artificially spikes COVID cases","text":"Data loading, particularly new process introduced can go astray lead spurious results.\ncity El Paso discovered reporting exceedingly high number daily COVID cases (Borunda 2020):El Paso city public health officials Thursday admitted major blunder, saying 3,100 new cases reported day earlier incorrect.number result two-day upload cases one day public health department went imputing data manually automatic data upload intended increase efficiency, Public Health Director Angela Mora said news conference.instance demonstrates data goes wrong also easy trust discrepancies find explanations real-world phenomena.\narticle goes suggest extremely high case numbers immediately caught given overall “alarming” levels:Mora said accepted responsibility taken deeper look noticing abnormally high number new cases.El Paso County still seeing 1,000 new cases daily basis, officials said. correct number new cases Wednesday 1,537.“numbers still high still alarming high reported,” Mora said.Similar data artifacts also arose variety factors besides system changes. example, delays 9--5 office paperwork (real-world events triggering creation key data elements) holidays led artificial trends data reporting around end--year holidays:238 deaths reported Illinois since start pandemic, IDPH noted release data , “delayed weekends, including past holiday weekend.”- Illinois sees biggest spike reported COVID-19 deaths date holidays delay data, officials say, WGN9 (TODO CITATION)","code":""},{"path":"data-dall.html","id":"data-encoding-leads-to-incorrect-bmi-calculation","chapter":"2 Data Dalliances (WIP)","heading":"2.8.2 Data encoding leads to incorrect BMI calculation","text":"Data encoded inconsistently may mishandled automated decision processes.\nexample, data manually entered electronic health records (EHR) countless care provides might compiled single source although meanings inconsistent.early days COVID19 vaccination push, regions chose prioritize vaccine supply certain individual characteristics including high body mass index (BMI), function one’s weight relative height.One 6ft 2in tall man’s data misinterpreted suggest 6.2 centimeters tall; caused get high priority vaccine.described BBC (“Covid: Man Offered Vaccine Error Lists 6.2cm Tall” 2021):man 30s underlying health conditions offered Covid vaccine NHS error mistakenly listed just 6.2cm height. Liam Thorp told qualified jab measurements gave body mass index 28,000.","code":""},{"path":"comp-quan.html","id":"comp-quan","chapter":"3 Computational Quandaries (WIP)","heading":"3 Computational Quandaries (WIP)","text":"gaining confidence one’s data (, least, making peace ), next step data analysis often start cleaning exploring data summary statistics, plots, models.\nGenerally, requires computational tool like SQL, R, python.process computation can fraught challenges.\nComputational tools extremely literal; excellent precisely told often analysts might meant wished .\nAdditionally, moment analyst begins use tool, conversation longer data;\nsuddenly, mental model every single tool developer thought might want analysis affects tools’ behaviors analysts’ results.chapter, explore common ways tools may something technically correct, reasonable, -intended much analysts may expect.\nAlong way, see computational methods interact data encoding choices discussed Chapter 2 (Data Dalliances).","code":""},{"path":"comp-quan.html","id":"preliminaries---data-computation","chapter":"3 Computational Quandaries (WIP)","heading":"3.1 Preliminaries - Data Computation","text":"think specific tools failure modes, can first consider common types operations analytical tools allow us data.","code":""},{"path":"comp-quan.html","id":"single-table-operations","chapter":"3 Computational Quandaries (WIP)","heading":"3.1.1 Single Table Operations","text":"Given single data table, may wish operations (illustrated Figure 3.1) :Filtering: Extracting subset dataset analysis based certain inclusion criteria recordAggregation: Grouping data table one variables condensing information across records aggregate functions like counts, sums, averagesTransformation: Create new columns modifying existing columns represent complex domain-specific context\nFIGURE 3.1: Illustration basic single-table data wrangling operations\n","code":""},{"path":"comp-quan.html","id":"multiple-table-operations","chapter":"3 Computational Quandaries (WIP)","heading":"3.1.2 Multiple Table Operations","text":"Often, can get additional value analysis combining multiple types information difference tables.\nworking multiple tables, may interested :Combining Row-wise: Taking multiple tables schemas (column names data types) creating single table contains union (records), intersection (matching), difference (one) records two tablesCombining Column-wise: Appending additional fields existing records joining (also known merging) multiple tables","code":""},{"path":"comp-quan.html","id":"mechanics","chapter":"3 Computational Quandaries (WIP)","heading":"3.1.3 Mechanics","text":"operations rely core computational tasks:Arithmetic: Basic addition, subtraction, multiplication, division aggregate transform dataEquality: Comparing whether two values equal critical data filtering, column-wise combination, certain types data transformationCasting: Converting data types different elements comparable format necessary row-wise combination often prerequisite certain equality arithmetic tasksWhile operations may seem simple, behavior within certain tools employed certain data types may sometimes lead unintuitive misleading results.","code":""},{"path":"comp-quan.html","id":"null-values","chapter":"3 Computational Quandaries (WIP)","heading":"3.2 Null Values","text":"Chapter 2 (Data Dalliances), discuss null values may represent many different concepts encoded multiple different ways.\naddition semantic challenges, various representations null values may cause different computational problems.8\nsection, explore potential failure modes.","code":""},{"path":"comp-quan.html","id":"types-of-null-values","chapter":"3 Computational Quandaries (WIP)","heading":"3.2.1 Types of Null Values","text":"can null values represent many different things (explored Chapter 2), also may represented many different ways. Understanding nulls encoded one’s dataset critical prerequisite attempting computations described subsequent sections.","code":""},{"path":"comp-quan.html","id":"language-representations","chapter":"3 Computational Quandaries (WIP)","heading":"3.2.1.1 Language representations","text":"Different programming languages offer versions null values – sometimes one. example, R language includes NA, typed NAs (e.g. NA_integer, NA_character), NaN, NULL; meanwhile, core python None numpy module provides nan.different values carry different semantic functional meanings. example R’s NA generally means “presence absence” whereas NULL “absence presence.” articulated clearly examine lengths objects observe NA length 1 whereas NULL length 0.proof interchangeable, may use helper functions .na() .null(). ’s false NA NULL essentially unable evaluated NULL NA NULLs truly nothing.9To complicate matters, NaN (“number”), along -Inf Inf, generally arise attempt abuse R’s calculator. Somewhat charmingly, Inf -Inf may used rudimentary calculations limit returned.10","code":"\nc(length(NA), length(NULL))## [1] 1 0\nc(\n  is.na(NA),\n  is.null(NULL),\n  is.na(NULL),\n  is.null(NA)\n)## [1]  TRUE  TRUE FALSE\nc(\n  1/0,   # returns Inf\n  0/0,   # returns NaN\n  1/Inf  # returns 0\n)## [1] Inf NaN   0"},{"path":"comp-quan.html","id":"data-encoding-choices-todo","chapter":"3 Computational Quandaries (WIP)","heading":"3.2.1.2 Data encoding choices (TODO)","text":"Beyond null types offered natively different programming languages, also many different data management conventions null values. null values can many meanings, sometimes missing fields encoded “range” values intend suggest type missingness.example, US Census Bureau’s Medical Expenditure Panel Survey uses following reserved codes denote different types missingness: (TODO: cite p10 https://www.meps.ahrq.gov/data_stats/download_data/pufs/h206a/h206adoc.pdf)approach preserves lot relevant information , time, readily apparent values valid data manually inspect. Unfortunately, manually inspecting every data field rarely possible, sentinel values may go undetected looking higher-level summaries.Consider survey population retired adults age coded 999 provided. , simulate 100,000 observations uniformly distributed age 65 95 (hence, expected value 80). Next, replace merely half percent “null” values 999. Taking mean false values results mean 85. number alone might raise alarm; , know dataset’s population older adults. However, accidentally treating valid values biases results somewhat remarkable five years., first order business null values understanding encoded translation computationally appropriate form. However, beginning story.","code":"- -1 INAPPLICABLE Question was not asked due to skip pattern\n- -7 REFUSED Question was asked and respondent refused to answer question\n- -8 DK Question was asked and respondent did not know answer\n- -14 NOT YET TAKEN/USED Respondent answered that the medicine has not yet been used\n- -15 CANNOT BE COMPUTED Value cannot be derived from data\nset.seed(123)\n\nn <- 100000\np <- 0.005\nages <- runif(n, 65, 95)\n\nages_nulls <- ages\nages_nulls[1:(n*p)] <- 999\n\nc(mean(ages), mean(ages_nulls))## [1] 79.98 84.57"},{"path":"comp-quan.html","id":"aggregation","chapter":"3 Computational Quandaries (WIP)","heading":"3.2.2 Aggregation","text":"null values handled simple aggregation data varies across different languages across different functions within language.\nbetter understand problems might cause, look examples R SQL.explore aggregation, let’s build simple dataset. suppose working subscription-based e-commerce service looking monthly_spend dataset one record per customer information amount spent returned given month:compute average amount spent (AMT_SPEND) dplyr package, analyst might first reasonably write following summarize() statement.\nHowever, can see, due presence null values within AMT_SPEND column, result aggregation whole quantity AVG_SPEND set value NA.glance documentation mean() function reveals na.rm parameter , set true, removes null values dataset.\nAdding argument previous statement allows us reach numerical answer.However, right numerical answer?\nna.rm = TRUE drop null values set numbers averaged.\nHowever, suppose null values represent purchases made.\n, zero dollars spent.\neffect, removed non-purchasers data averaged.precisely, switched taking average\\[ \\frac{ \\sum_{1}^{n} Spend }{\\sum_{1}^{n} 1} \\] \\(n\\) customersto taking average\\[ \\frac{ \\sum_{Spend > 0} Spend }{\\sum_{Spend > 0} 1} \\] customers spendAt face value, say code giving incorrect answer; dropping low (zero) purchase amounts, average amount spend per customer inflated.\nsecond perspective, someone philosophically troubling, tiny change code fixed obvious problem (returning null value) introduced non-obvious problem fundamentally changing question asking.\ndropping accounts table made purchases, longer answering “average amount spent new registrant?” rather “average amount spent actively engaged customer?”\ntechnical quirk significant analytical impact.answer real question hand, instead couple options.\nmanually sum() amount spent option drop nulls divide correct denominator (observations – just spend) explicitly recode null values AMT_SPEND zero taking average.11\nEither options lead correct conclusion lower average spend amount.well good just accept behaviors simply nulls work, complexity comes see industry standard across tools.\nexample, SQL code shows, SQL’s avg() function behaves like R’s mean() na.rm = TRUE option set.\n, default behavior SQL operate valid available values.However, suggest null values also destructive SQL.\naggregation functions (compute rows/records) like sum() avg() drop nulls, operators like + - (compute across columns/variables row/record) exhibit behavior.\nConsider, example, wish calculate average net purchase amount (purchases minus returns) instead gross (total) purchase amount.Despite learned SQL’s avg() function, query returns null value.\nhappened?\nspend dataset, amt_return column completely null (representing returns).\nsubtraction occurs average taken, subtracting real numbers amt_spend null values amt_return creates column null values fed avg() function.\nprocess shown step--step .","code":"\nspend <-\n  data.frame(\n    AMT_SPEND = c(10, 20, NA),\n    AMT_RETURN = rep(NA, 3)\n  )\nsummarize(spend, \n          AVG_SPEND = mean(AMT_SPEND),\n          AVG_SPEND_NARM = mean(AMT_SPEND, na.rm = TRUE))##   AVG_SPEND AVG_SPEND_NARM\n## 1        NA             15\nsummarize(\n    spend,\n    AVG_SPEND_MANUAL = sum(AMT_SPEND, na.rm = TRUE) / n(),\n    AVG_SPEND_RECODE = mean(coalesce(AMT_SPEND, 0))\n  )##   AVG_SPEND_MANUAL AVG_SPEND_RECODE\n## 1               10               10SELECT avg(amt_spend) as AVG_SPEND\nFROM spend##   AVG_SPEND\n## 1        15SELECT avg(amt_spend-amt_return) as AVG_SPEND_NET\nFROM spend##   AVG_SPEND_NET\n## 1            NASELECT\n  amt_spend, \n  amt_return, \n  amt_spend-amt_return \nFROM spend##   AMT_SPEND AMT_RETURN amt_spend-amt_return\n## 1        10         NA                   NA\n## 2        20         NA                   NA\n## 3        NA         NA                   NA"},{"path":"comp-quan.html","id":"comparison","chapter":"3 Computational Quandaries (WIP)","heading":"3.2.3 Comparison","text":"Null values don’t just introduce complexity arithmetic. Difficulties also arise time multiple variables assessed equality inequality. Since null value unknown, programming languages generally consider nulls comparable nulls.can simple examples R SQL.toy examples, outcomes may seem perfectly logical.\nHowever, reasoning can arise sneakier ways lead uninteded results equality evaluations implicit task hand instead singular focus.\n’ll now see examples data filtering, joining, transformation.","code":"\nc(\n  NA == 3, \n  NA > 10, \n  NA == NA\n  )## [1] NA NA NASELECT\n  (NULL = 3) as NULL_EQ_NUM,\n  (NULL > 10) as NULL_GT_NUM,\n  (NULL = NULL) as NULL_EQ_NULL##   NULL_EQ_NUM NULL_GT_NUM NULL_EQ_NULL\n## 1          NA          NA           NA"},{"path":"comp-quan.html","id":"filtering","chapter":"3 Computational Quandaries (WIP)","heading":"3.2.3.1 Filtering","text":"Suppose want split dataset two datasets based high low values spend.\nmight assume following two lines code create clear partition12 results.However, examining resulting datasets, see neither contains null records.situation results SQL.Thus, whenever data null values, common act data filtering risks excluding important information.","code":"\nspend_lt20 <- filter(spend, AMT_SPEND < 20)\nspend_gte20 <- filter(spend, AMT_SPEND >= 20)\nspend_lt20##   AMT_SPEND AMT_RETURN\n## 1        10         NA\nspend_gte20##   AMT_SPEND AMT_RETURN\n## 1        20         NASELECT *\nFROM spend\nWHERE AMT_SPEND < 20##   AMT_SPEND AMT_RETURN\n## 1        10         NASELECT *\nFROM spend\nWHERE AMT_SPEND >= 20##   AMT_SPEND AMT_RETURN\n## 1        20         NA"},{"path":"comp-quan.html","id":"joining","chapter":"3 Computational Quandaries (WIP)","heading":"3.2.3.2 Joining","text":"phenomenon described also happens joining multiple datasets.Suppose multiple datasets wish merge based columns denoting record’s name date birthday.\nease exploration, make simplest possible dataset simply try merge .\n(may seem silly, often trying understand computationally complex things, good idea make scenario simple possible.\nfact, idea core concept computational unit tests discuss end chapter.)SQL, try join table, records row 1 match 'Anne' == 'Anne' '2000-01-01' == '2000-01-01'.\nHowever, poor Bob’s record eliminated birthdate logged null, NA == NA false.contrast, R’s dplyr::inner_join() function default.\nfunction lets us specifically control nulls matches na_matches argument, default option match NA values.\n(may read argument typing ?dplyr::inner_join R console pull documentation.)example cautionary tale null values may unintentionally corrupt data transformations also “brittle” knowledge intuition may moving tools.\nNeither default behaviors strictly better worse, definitely different real implications analysis.","code":"\nbday <- data.frame(NAME = c('Anne', 'Bob'), BIRTHDAY = c('2000-01-01', NA))\nbday##   NAME   BIRTHDAY\n## 1 Anne 2000-01-01\n## 2  Bob       <NA>SELECT a.*\nFROM\n  bday as a\n  INNER JOIN\n  bday as b\n  ON\n  a.NAME = b.NAME and\n  a.BIRTHDAY = b.BIRTHDAY##   NAME   BIRTHDAY\n## 1 Anne 2000-01-01\ninner_join(bday, bday, by = c('NAME', 'BIRTHDAY'))##   NAME   BIRTHDAY\n## 1 Anne 2000-01-01\n## 2  Bob       <NA>"},{"path":"comp-quan.html","id":"transformation","chapter":"3 Computational Quandaries (WIP)","heading":"3.2.3.3 Transformation","text":"common task data analysis aggregate results subgroup.\nexample, might want summarize many customers (rows/records) spent less $10. discern , might create categorical variable high versus low purchase amounts, group variable count.psuedocode read something like :define HIGH_LOW variable, might use function like ifelse(), dplyr::if_else(), dplyr::case_when().\nHowever, , issue values partitioned nulls included.\nrecode records AMT_SPEND less equal 10 “Low” default rest “High,” accidentally count null values “High” group.Instead, accurate transparent (unless know specifically null values mean group part ) let one “core” categories “default” case logic.\ncan explicitly encode residual values something like “” “ERROR” help us see problem requiring extra attention.","code":"data %>%\n  mutate(HIGH_LOW = << transform AMT_SPEND >>) %>%\n  group_by(HIGH_LOW) %>%\n  count()\nspend %>%\n  mutate(HIGH_LOW = case_when(\n    AMT_SPEND <= 10 ~ \"Low\", \n    TRUE ~ \"High\")\n    ) %>%\n  group_by(HIGH_LOW) %>% \n  count()## # A tibble: 2 x 2\n## # Groups:   HIGH_LOW [2]\n##   HIGH_LOW     n\n##   <chr>    <int>\n## 1 High         2\n## 2 Low          1\nspend %>%\n  mutate(HIGH_LOW = case_when(\n    AMT_SPEND <= 10 ~ \"Low\", \n    AMT_SPEND > 10 ~ \"High\",\n    TRUE ~ \"OTHER\")\n    ) %>%\n  group_by(HIGH_LOW) %>% \n  count()## # A tibble: 3 x 2\n## # Groups:   HIGH_LOW [3]\n##   HIGH_LOW     n\n##   <chr>    <int>\n## 1 High         1\n## 2 Low          1\n## 3 OTHER        1"},{"path":"comp-quan.html","id":"strings-wip","chapter":"3 Computational Quandaries (WIP)","heading":"3.3 Strings (WIP)","text":"String data can inherently appealing. best, strings used bring readable human interpretable values dataset. However, string data processing thereof comes challenges.First, unlike numbers, human language strings can ambiguously defined. 2 number represent value two. However, incorporation human language means many different words, phrases, formatting choices can represent concept. confounded instances string data manually entered, case user-input data.13Secondly, string data one flexible datatypes can contain types information – --logical values (\"yes\"/\"\", \"true\"/\"false\"), --numeric values (\"27\"), --date values (\"2020-01-01\"), even complex data encodings like JSON blobs (\"{\"name\":{\"first\":\"emily\",\"last\":\"riederer\"},\"social\":{\"twitter\":\"emilyriederer\",\"github\":\"emilyriederer\",\"linkedin\":\"emilyriederer\"}}\" hideous formatting emphasis.) data publisher, may convenience, see can turn frustration liability functions comparison operations attempted strings semantically represent different type value.","code":""},{"path":"comp-quan.html","id":"dirty-strings-todo","chapter":"3 Computational Quandaries (WIP)","heading":"3.3.1 Dirty Strings (TODO)","text":"whitespace“fancy” characters (alternate encodings like ms word)special characters display versus values","code":"\n\"a\" == \"a\"\n\"a b\" == \"a b\"\n\"a b\" == \"a  b\"\n\"a b\" == \"a b \"## [1] TRUE\n## [1] TRUE\n## [1] FALSE\n## [1] FALSE\n' \" ' == ' \" '\n' “ ' == ' \" '## [1] TRUE\n## [1] FALSE\nx <- \"a\\tb\"\ncat(x) # what you see...\nx == \"a    b\" # ...is not what you get## a    b[1] FALSE"},{"path":"comp-quan.html","id":"regular-expressions-todo","chapter":"3 Computational Quandaries (WIP)","heading":"3.3.2 Regular Expressions (TODO)","text":"promised solution oriented, butnot knowing regex disaster trying work string data…","code":""},{"path":"comp-quan.html","id":"comparison-1","chapter":"3 Computational Quandaries (WIP)","heading":"3.3.3 Comparison","text":"TODO","code":""},{"path":"comp-quan.html","id":"string-ordering","chapter":"3 Computational Quandaries (WIP)","heading":"3.3.3.1 String ordering","text":"Strings ranked based alphabetical order just like dictionary. properties ordering include :numbers smaller letters (1 < \"\")lower-case smaller upper case (\"\" < \"\")fewer characters smaller characters (\"\" < \"aa\")rules make perfect sense true characters. However, strings used “catch ” represent structures, typical comparison operators can produce odd results. example, generally uncontroverisal ninety-one less one hundred twenty. However, string \"91\" greater \"120\" chacter \"9\" compared character \"1\".14When strings used represent strings, comparison operators may may work depending precise formatting convertions. , see “YYYYQQ”-formats sort correctly information hierarchically nested; millenia compared centuries, centuries decades, decades years, years quarters. However, many string representations dates, like “QQ-YYYY” order correctly. Related topics discussed “Dates Times” section.examples demonstrate shouldn’t rely sorting schemes follow different rules. comparisons types, safer bet cast format truly representative types. , reason, wish keep strings, second example shows wise format conducive format possible things just work.","code":"\n91 < 120\n\"91\" < \"120\"## [1] TRUE\n## [1] FALSE\n\"20190Q4\" < \"2020Q3\" # string (alphabetic) ordering same as semantic ordering\n\"Q4-2019\" < \"Q3-2020\" # string and semantic orderings are different## [1] TRUE\n## [1] FALSE"},{"path":"comp-quan.html","id":"type-coercion","chapter":"3 Computational Quandaries (WIP)","heading":"3.3.3.2 Type coercion","text":"discussed string comparison looking “dirty” strings. unexpected behavior arises strings compared across different data types. Many computing programs attempt coerce objects similar comparable type. Sometimes, can handy operations “just work,” always cost convenience. ’ll see, delegating important decisions computing engine may always capture semantic relationships interested .example, consider compare string number. make comparable, R convert strings checking equality. Thus, number 2020 equivalent string \"2020\" string \"02020\".constrast, SQLite15 thinks string '2020' greater number 2020 two quantities equal.^TODO: cause problems (FIPS example?)","code":"\n\"2020\" == 2020\n\"02020\" == 2020## [1] TRUE\n## [1] FALSE\nsqldf(\"select \n      case when '2020' = 2020 then 1 else 0 end as is_eq,\n      case when not '2020' == 2020 then 1 else 0 end as not_eq,\n      case when '2020' < 2020 then 1 else 0 end as is_lt,\n      case when '2020' > 2020 then 1 else 0 end as is_gt\n      \")##   is_eq not_eq is_lt is_gt\n## 1     0      1     0     1"},{"path":"comp-quan.html","id":"dates-and-times-wip","chapter":"3 Computational Quandaries (WIP)","heading":"3.4 Dates and Times (WIP)","text":"Unlike character strings, dates times seem like well defined distinct, quantifiable components like years, months, days. However, many different conventions date formatting underlying storage formats exist. leads similar challenges dates times saw strings .common formats wild :YYYYMMDDYYYYMMMMDDYYYYDDMMYYYYMM/DD/YYYYMM/DD/YYDD/MM/YYYYYYYY-MM-DD (ISO8601)complicate matters , many formats may represented either native date types various programs basic data types (integers first four strings last four). addition, analogous formats exist timestamps encode calendar date time day (hour, minute, second information).TODO: ISO8601?","code":""},{"path":"comp-quan.html","id":"comparison-2","chapter":"3 Computational Quandaries (WIP)","heading":"3.4.1 Comparison","text":"Automatic conversion data types\nDates versus timestampsnone equal nothing returns filteringthe thing happens sqlin way aren’t equal? understand helpful know computer encodes dateswith .numeric() R can see numeric representation datethis works way SQLthis implication things date inequality relationship languagesNote can affect filters joinsand similarly causes general problem comparing date date---integer","code":"\ndf_dt <-\ndata.frame(\n  DT_ENROLL = as.Date(\"2020-01-01\"),\n  DT_PURCH  = 20200101,\n  DT_LOGIN  = as.POSIXlt(\"2020-01-01T12:00:00\") \n  )\nfilter(df_dt, DT_ENROLL == DT_PURCH) %>% nrow()## [1] 0\nfilter(df_dt, DT_ENROLL == DT_LOGIN) %>% nrow()## Warning in mask$eval_all_filter(dots, env_filter):\n## Incompatible methods (\"Ops.Date\", \"Ops.POSIXt\") for\n## \"==\"## [1] 0\nsqldf(\"select * from df_dt where DT_ENROLL = DT_PURCH\")## [1] DT_ENROLL DT_PURCH  DT_LOGIN \n## <0 rows> (or 0-length row.names)\nsqldf(\"select * from df_dt where DT_ENROLL = DT_LOGIN\")## [1] DT_ENROLL DT_PURCH  DT_LOGIN \n## <0 rows> (or 0-length row.names)\nas.numeric(df_dt$DT_ENROLL)## [1] 18262\nsqldf(\"select cast(DT_ENROLL as integer), cast(DT_PURCH as integer) from df_dt\")##   cast(DT_ENROLL as integer) cast(DT_PURCH as integer)\n## 1                      18262                  20200101\nfilter(df_dt, DT_ENROLL < DT_PURCH) %>% nrow()## [1] 1\nsqldf(\"\n  select \n    cast(DT_ENROLL as integer), \n    case when DT_ENROLL < 18000 then 1 else 0 end as lt_18000,\n    case when DT_ENROLL < 19000 then 1 else 0 end as lt_19000,\n    case when DT_ENROLL < DT_PURCH then 1 else 0 end as lt_purch\n  from df_dt\")##   cast(DT_ENROLL as integer) lt_18000 lt_19000\n## 1                      18262        0        1\n##   lt_purch\n## 1        1\nas.Date(\"2020-01-01\") > 20160501## [1] FALSE\nsqldf(\"\n  select cast('2020-01-01' as date) > 20160501\n  \")##   cast('2020-01-01' as date) > 20160501\n## 1                                     0"},{"path":"comp-quan.html","id":"logicals-todo","chapter":"3 Computational Quandaries (WIP)","heading":"3.5 Logicals (TODO)","text":"","code":"\n\"TRUE\" == TRUE## [1] TRUE\n1 == TRUE## [1] TRUE\n\"True\" == TRUE## [1] FALSE\n\"TRUE\"*5## Error in \"TRUE\" * 5: non-numeric argument to binary operator\nsqldf(\"select \n      (TRUE == 1) as is_int_true,\n      (TRUE == 'TRUE') as is_char_true,\n      TRUE*5 as true_times_five,\n      FALSE*5 as false_times_five\n      \")##   is_int_true is_char_true true_times_five\n## 1           1            0               5\n##   false_times_five\n## 1                0"},{"path":"comp-quan.html","id":"order-of-operations-wip","chapter":"3 Computational Quandaries (WIP)","heading":"3.6 Order of Operations (WIP)","text":"PEMDAS sometimes still ambiguous","code":"\n1 + 1  * 2 / 3 - 1\n(1 + 1) * 2 / 3 - 1\n1 + 1 * 2 / (3 - 1)## [1] 0.6667\n## [1] 0.3333\n## [1] 2"},{"path":"comp-quan.html","id":"object-references-wip","chapter":"3 Computational Quandaries (WIP)","heading":"3.7 Object References (WIP)","text":"Copying modifying object overview\nFIGURE 3.2: Different relationships named variables values\nmight preferred?risks don’t understand ?Pythonpandas DataFrame methods inplace arg (False default)RFrom https://stackoverflow.com/questions/10225098/understanding-exactly---data-table---reference--vs--copy--another","code":"x = [1,2,3]\ny = x\ny.append(4)\nprint(y)\nprint(x)## [1, 2, 3, 4]\n## [1, 2, 3, 4]z = x.copy()\nz.append(5)\nprint(z)\nprint(x)## [1, 2, 3, 4, 5]\n## [1, 2, 3, 4]\nlibrary(data.table)\n\nDT <- data.table(a=c(1,2), b=c(11,12))\nprint(DT)\n\nnewDT <- DT        # reference, not copy\nnewDT[1, a := 100] # modify new DT\n\nprint(DT)          # DT is modified too.\n\nDT = data.table(a=c(1,2), b=c(11,12))\nnewDT <- DT        \nnewDT$b[2] <- 200  # new operation\nnewDT[1, a := 100]\n\nprint(DT)##    a  b\n## 1: 1 11\n## 2: 2 12\n##      a  b\n## 1: 100 11\n## 2:   2 12\n##    a  b\n## 1: 1 11\n## 2: 2 12"},{"path":"comp-quan.html","id":"trusting-tools","chapter":"3 Computational Quandaries (WIP)","heading":"3.8 Trusting Tools","text":"","code":""},{"path":"comp-quan.html","id":"delegating-decisions","chapter":"3 Computational Quandaries (WIP)","heading":"3.8.1 Delegating decisions","text":"theme throughout book fundamentally social nature data analysis. Data analysis fraught without understanding countless decisions made along way generated (whose data reflected), collected , migrated , posed questions . one hand, beautiful aspect analysis; hand, means analysts analyses subject cognitive social psychological biases everyday humans.One bias “social proof”: assuming tool behaves certain way, must correct.Assuming tools know best admittedly attractive proposition. appeals desire think someone, somewhere “charge” , perhaps critically, helps us avoid domino effect distrust (don’t trust tools can trust results? can’t trust results, can trust anything ?) Unfortunately, many reasons tools might know best. example, tool’s developer might :Made mistakeHad different analysis problem mind different optimal approachBeen optimizing different constraint (e.g. explainability vs. accuracy, speed vs. theoretical properties)Come community different normsBeen affording users flexibility things many ways even don’t agreeBuilt certain feature different purpose using itNot thought allAs concrete examples popular open source tools. ’ll look briefly prominent python library scikitlearn machine learning Apache Spark, engine large-scale distributed data processing.","code":""},{"path":"comp-quan.html","id":"defaults-in-scikitlearn","chapter":"3 Computational Quandaries (WIP)","heading":"3.8.1.1 Defaults in scikitlearn","text":"scikitlearn’s default behavior logistic regression modeling16 automatically applies L2 regularization. might might know means, might might want apply problem. ’s fine. important thing change estimates predictions, part classical definition algorithm (modelers coming statistical background.)course, ’s nothing inherently wrong choice; library authors just different goals typical statistical. scikitlearn developer Olivier Grisel explains Twitter choice (others library) explained “Scikit-learn always designed make easy get good predictive accuracy (eg measured CV) rather statistical inference library.” Additionally, choice documented bold function documentation.However, analyst easily miss nuance read documentation. , misinterpret choice social proof regularization always right approach, might make best choice analysis.","code":""},{"path":"comp-quan.html","id":"algorithms-in-spark","chapter":"3 Computational Quandaries (WIP)","heading":"3.8.1.2 Algorithms in Spark","text":"second example, according 2015 Jira ticket, developers Spark considered multiple methodologies use adding functionality compute feature importance random forest. Ultimately, core contributor advised permutation importance due computational cost.\nFIGURE 3.3: JIRA ticket Spark discussion random forest variable importance algorithm implement\nClearly, one wants workflow costly timely run. , , right wrong. However, since every approach feature importance biases, pitfalls, challenges interpretation, ’s mistake end-user carefully understand algorithm used .","code":""},{"path":"comp-quan.html","id":"off-label-use-todo","chapter":"3 Computational Quandaries (WIP)","heading":"3.8.2 “Off-Label” Use (TODO)","text":"coined https://www.rstudio.com/resources/rstudioglobal-2021/maintaining--house--tidyverse-built/","code":""},{"path":"comp-quan.html","id":"security-todo","chapter":"3 Computational Quandaries (WIP)","heading":"3.8.3 Security (TODO)","text":"namespace squattingexecutable code","code":""},{"path":"comp-quan.html","id":"inefficient-processing-todo","chapter":"3 Computational Quandaries (WIP)","heading":"3.9 Inefficient Processing (TODO)","text":"","code":""},{"path":"comp-quan.html","id":"strategies-wip","chapter":"3 Computational Quandaries (WIP)","heading":"3.10 Strategies (WIP)","text":"Paragraph 1 TODOSome computational quandaries inherent tools , often function tools ways chose use . strategies related writing robust resilient code discussed Chapter 11 (Complexify Code).","code":""},{"path":"comp-quan.html","id":"understand-the-intent-1","chapter":"3 Computational Quandaries (WIP)","heading":"3.10.1 Understand the intent","text":"read docslook examples","code":""},{"path":"comp-quan.html","id":"understand-the-execution-1","chapter":"3 Computational Quandaries (WIP)","heading":"3.10.2 Understand the execution","text":"test simple examples (like ’ve )specificlly try corner cases","code":""},{"path":"comp-quan.html","id":"be-explicit-not-implicit","chapter":"3 Computational Quandaries (WIP)","heading":"3.10.3 Be explicit not implicit","text":"default argumentsexamples casting, coalescing","code":""},{"path":"comp-quan.html","id":"real-world-disasters-wip","chapter":"3 Computational Quandaries (WIP)","heading":"3.11 Real World Disasters (WIP)","text":"https://www.theguardian.com/politics/2020/oct/05/-excel-may--caused-loss--16000-covid-tests--englandThe data error, led 15,841 positive tests left official daily figures, means 50,000 potentially infectious people may missed contact tracers told self-isolate.","code":""},{"path":"eg-agg.html","id":"eg-agg","chapter":"4 Egregious Aggregations (WIP)","heading":"4 Egregious Aggregations (WIP)","text":"armed understanding data tools available analysis, common start analysis exploring data aggregation.\nheart, sort data analysis process condensing raw data something manageable useful giving little information possible.Many elementary tools task much better comprehension task preservation one.\nlearn rigorous assumptions consider validate studying linear regression, basic arithmetic aggregation presents agnostic welcome type data.\nHowever, underlying distributions variables relationships significant impact informative interpretable various summarizations .chapter, explore different ways univariate multivariate aggregations can naive uninformative.","code":""},{"path":"eg-agg.html","id":"similar-in-summary","chapter":"4 Egregious Aggregations (WIP)","heading":"4.1 Similar in Summary","text":"Anscombe’s QuartetThe datasaurus dozen (Matejka Fitzmaurice 2017) datasauRus R package (Locke D’Agostino McGowan 2018)TABLE 4.1: Summary statistics Datasaurus Dozen datasets\nFIGURE 4.1: Scatterplots Datasaurus Dozen datasets\nclearly contrived example (, chose check paper, cleverly contrived example!), also cautionary tale. Summary statistics just insufficient focus central tendency (e.g. mean) instead spread. example, even examination variation covariation led overly simplistic view underlying data.","code":"\nlibrary(datasauRus)\ndf <- datasauRus::datasaurus_dozen\nhead(df)## # A tibble: 6 x 3\n##   dataset     x     y\n##   <chr>   <dbl> <dbl>\n## 1 dino     55.4  97.2\n## 2 dino     51.5  96.0\n## 3 dino     46.2  94.5\n## 4 dino     42.8  91.4\n## 5 dino     40.8  88.3\n## 6 dino     38.7  84.9"},{"path":"eg-agg.html","id":"averages","chapter":"4 Egregious Aggregations (WIP)","heading":"4.2 Averages","text":"","code":""},{"path":"eg-agg.html","id":"averaging-skewed-data","chapter":"4 Egregious Aggregations (WIP)","heading":"4.2.1 Averaging skewed data","text":"Arithmetic average versus colloquial meaning average “typical”Skewed dataMultimodal data / mixture models","code":""},{"path":"eg-agg.html","id":"no-average-observation","chapter":"4 Egregious Aggregations (WIP)","heading":"4.2.2 No “average” observation","text":"previous section, average represented point relevant data range even perhaps one representative “typical” observation.\ndiscussed situations quantity may reasonable answer certain types questions aid certain types decisions.However, seek average profile multiple variables, problems averages compounded.\nmay end set “average” summary statistics representative part population.see , let’s assume working data company subscription business model.\nmight interested profiling age account (long subscriber) activity (measured amount spent e-commerce platform, files downloaded streaming service, etc.)following code simulates set observations:\n80% accounts 0 3 years age average activity level 100 20% accounts older 3 years age average activity level 500.\n(Don’t -think specific probability distributions lived .\nconcerned interrogating properties average simulating realistic data generating process.\nGiving permission wrong “lazy” unimportant things gives us energy focus matters.)Figure 4.2 shows scatterplot relationship account age (x-axis) activity level (y-axis).\nMeanwhile, marginal rug plots shows univariate distribution variable.\nsole red dot denotes coordinates average age average activity.\nNotably, dot exists region “zero density”;\n, representative customer.\nStrategic decisions made sort observation mind “typical” might destined success.\nFIGURE 4.2: scatterplot two variables averages\n","code":"\nset.seed(123)\n\n# define simulation parameters ----\n## n: total observations\n## p: proportion of observations in group 1\nn <- 5000\np <- 0.8\nn1 <- n*p\nn2 <- n*(1-p)\n\n# generate fake dataset with two groups ----\ndf <- \n  data.frame(\n    age = c(runif(n1,   0,  3), runif(n2,   3, 10)),\n    act = c(rnorm(n1, 100, 10), rnorm(n2, 500, 10))\n  )"},{"path":"eg-agg.html","id":"the-product-of-averages","chapter":"4 Egregious Aggregations (WIP)","heading":"4.2.3 The product of averages","text":"example shows, averages multivariate data can produce poor summaries – particularly variables interrelated17.second implication observation deriving additional computations based pre-averaged numbers likely obtain inaccurate results.example, consider wish estimate average dollar amount returns per e-commerce order.\nOrders may generally mixture low-price orders (around $50 average) high-price orders (around $250 average).\nLow-price orders may 10% probability returned high price orders 20% probability.\n(, numbers, distributions, relationships hyper-realistic?\n.\nHowever, telling story just reason numerical properties, give permission focus irrelevant details.)true average amount returned across orders 36.0438 (average_of_product variable).\nHowever, instead already knew average spend amount average return proportion, might inclined compute product_of_average method returns value 26.9923.\n(difference 9.05 relative average purchase amount 150.)first, may seem unintuitive write formulas realize metrics , fact, two different quantities:\\[ \\frac{ \\sum_{1}^{n} Spend }{\\sum_{1}^{n} 1} \\ * \\frac{ \\sum_{1}^{n} (Return) }{\\sum_{1}^{n} 1} \\] \\(n\\) ordersversus\\[ \\frac{\\sum_{1}^{n} Spend * (Return)}{\\sum_{1}^{n} 1} \\]still feels counterintuitive, can see much difference accounted interrelation two variables.\nfollowing code, break relationship variables randomly reordering ind_return variable longer true relationship corresponding amt_spend variable.redoing calculations, find th two values much closer.\naverage_of_product now 24.1041 product_of_average now 26.9923.\nnotably still number mean two equations equivalent variables unrelated;\nhowever, second result illustrates extent interrelations can defy naive intuitions.","code":"\nset.seed(123)\n\n# define simulation parameters ----\n## n: observations per group\n## pr[1|2]: mean price per group\nn <- 100\npr1 <- 50\npr2 <- 250\npr_sd <- 5\nre1 <- 0.1\nre2 <- 0.2\n\n# simulate spend amounts and return indicators ----\namt_spend  <- c(rnorm(n, pr1, pr_sd), rnorm(n, pr2, pr_sd))\nind_return <- c(rbinom(n, 1, re1),    rbinom(n, 1, re2))\n\n# compute summary statistics ----\naverage_of_product <- mean(amt_spend * ind_return)\nproduct_of_average <- mean(amt_spend) * mean(ind_return)\n# randomly reorder one of two variables to break relationships ----\nind_return <- sample(ind_return, size = 200)\n\n# recompute variables ----\naverage_of_product <- mean(amt_spend * ind_return)\nproduct_of_average <- mean(amt_spend) * mean(ind_return)"},{"path":"eg-agg.html","id":"average-over-what-todo","chapter":"4 Egregious Aggregations (WIP)","heading":"4.2.4 Average over what? (TODO)","text":"thing unweighted average (just sometimes weights equal)formal definition expected value forces pick probability distributioneg avg mpg time vs mileage?strictly error language allows ill-defined problem","code":""},{"path":"eg-agg.html","id":"ratios","chapter":"4 Egregious Aggregations (WIP)","heading":"4.3 Ratios","text":"","code":""},{"path":"eg-agg.html","id":"picking-the-right-denominator","chapter":"4 Egregious Aggregations (WIP)","heading":"4.3.1 Picking the right denominator","text":"","code":""},{"path":"eg-agg.html","id":"sample-size-effects","chapter":"4 Egregious Aggregations (WIP)","heading":"4.3.2 Sample size effects","text":"","code":""},{"path":"eg-agg.html","id":"misc---deciding-where-this-goes-todo","chapter":"4 Egregious Aggregations (WIP)","heading":"4.4 Misc - Deciding where this goes (TODO)","text":"","code":""},{"path":"eg-agg.html","id":"unobservable-censored-data","chapter":"4 Egregious Aggregations (WIP)","heading":"4.4.1 Unobservable (censored) data","text":"time--eventbut observe given point timeso can ?biasexample censored data","code":"\nn <- 1000\ntimes <- runif(n, 1, 5)\nmean(times)## [1] 3.013\ncurrent_time <- 2\n# only use observable values? \nmean(times[times < current_time])## [1] 1.487\n# use current time for \nmean(pmax(times, current_time))## [1] 3.137"},{"path":"eg-agg.html","id":"trends","chapter":"4 Egregious Aggregations (WIP)","heading":"4.5 Trends","text":"","code":""},{"path":"eg-agg.html","id":"if-trends-continue","chapter":"4 Egregious Aggregations (WIP)","heading":"4.5.1 “If trends continue…”","text":"","code":""},{"path":"eg-agg.html","id":"seasonality","chapter":"4 Egregious Aggregations (WIP)","heading":"4.5.2 Seasonality","text":"","code":""},{"path":"eg-agg.html","id":"strategies-todo-1","chapter":"4 Egregious Aggregations (WIP)","heading":"4.6 Strategies (TODO)","text":"","code":""},{"path":"eg-agg.html","id":"real-world-disasters-todo","chapter":"4 Egregious Aggregations (WIP)","heading":"4.7 Real World Disasters (TODO)","text":"Straight vs weighted averages COVID positivity rates (May 2020)changes result real-world differences Hoosiers, state uses county’s positivity rate one numbers determine restrictions county face. restrictions determine many people may gather, among items.Hoosiers may see loosened restrictions changes. Box said county-level impact mixed, predicted smaller counties see decline positivity rate changes.“change methodology calculate seven-day positivity rate counties. past, similar many states, ’ve added day’s positivity rate seven days divided seven obtain week’s positivity rate. Now add positive tests week divide total tests done week determine week’s positivity rate. help minimize effect high variability number tests done day can week’s overall positivity, especially smaller counties.”three issues herefirst straight versus weighted averagesthen back data matters.data distribution, increase variance shouldn’t effect meanRecall standard deviation sample proportion \\(\\sqrt(p*(1-p)/n)\\)link discussions sample size different types averagesbut low sample days based real world probably also sign different distribution (urgent cases get tested?)","code":"\navg_of_ratios <- (10/100 + 90/100) / 2\n\nratio_of_sums <- (10 + 90) / (100 + 100)\n\navg_of_ratios == ratio_of_sums\n\navg_of_ratios_uneq <- (10/100 + 180 / 200) / 2\n\nratio_of_sums_uneq <- (10 + 180) / (100 + 200)\n\navg_of_ratios_uneq == ratio_of_sums_uneq\n\nweightavg_of_ratios_uneq <- (100/300)*(10/100) + (200/300)*(180/200)\n\nratio_of_sums_uneq == weightavg_of_ratios_uneq## [1] TRUE\n## [1] FALSE\n## [1] TRUE\nset.seed(123)\n\n# define simulation parameters ----\n## n: total draws from binomial distribution\n## p: proportion of successes\np <- 0.5\nn <- 1000\n\n# sample from binomials of different size ----\ns010 <- rbinom(n,  10, p) /  10\ns100 <- rbinom(n, 100, p) / 100\ns500 <- rbinom(n, 500, p) / 500\n\n# set results as dataframe for inspection ----\ndf <- data.frame(\n  s = rep(c(10, 100, 500), each = n),\n  x = c(s010, s100, s500)\n)\nlibrary(ggplot2)\n\nggplot(data = df) +\n  aes(x = x, col = as.character(s)) +\n  geom_density() +\n  geom_vline(xintercept = p, col = 'darkgrey', linetype = 2) +\n  labs(\n    title = \"Sampling Distribution for p = 0.5\",\n    col = \"Sample Size\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = \"bottom\"\n  )"},{"path":"vex-viz.html","id":"vex-viz","chapter":"5 Vexing Visualization (TODO)","heading":"5 Vexing Visualization (TODO)","text":"","code":""},{"path":"incr-infe.html","id":"incr-infe","chapter":"6 Incredible Inferences (TODO)","heading":"6 Incredible Inferences (TODO)","text":"Previously, seen different inputs like data, tools, methods can add risks data analysis. However, battle won simply get first set outputs. chapter, explore common errors interpreting results analysis exploring aspects bias, missingness, confounding.","code":""},{"path":"incr-infe.html","id":"common-biases","chapter":"6 Incredible Inferences (TODO)","heading":"6.1 Common Biases","text":"","code":""},{"path":"incr-infe.html","id":"policy-induced-relationships","chapter":"6 Incredible Inferences (TODO)","heading":"6.2 Policy-induced relationships","text":"","code":"\nset.seed(123)\n\nn <- 1000\nx1 <- runif(n)\nx2 <- runif(n)\ny <- x1 + x2 > 1\ndf <- data.frame(x1, x2, y)\n\nwith(df, cor(x1, x2))\nwith(df[df$y,], cor(x1, x2))## [1] -0.05928\n## [1] -0.5003\nlibrary(ggplot2)\n\nggplot(df) +\n  aes(x = x1, y = x2, col = y) +\n  geom_point()"},{"path":"incr-infe.html","id":"feature-leakage","chapter":"6 Incredible Inferences (TODO)","heading":"6.3 Feature leakage","text":"Figure 6.1 shows…\nFIGURE 6.1: Correlation independent versus cumulative quantities\n","code":"\nn <- 1000\nminutes_month1 <- runif(n, 60, 1200)\nminutes_month2 <- runif(n, 60, 1200) \nminutes_tot <- minutes_month1 + minutes_month2\ndf <- data.frame(minutes_month1, minutes_month2, minutes_tot)"},{"path":"incr-infe.html","id":"diligent-data-dredging","chapter":"6 Incredible Inferences (TODO)","heading":"6.4 “Diligent” data dredging","text":"Success! ..success?sample splitting “train”(obviously ugly way , ’s point)“test”","code":"\nset.seed(123)\n\nn <- 1000\nx <- rnorm(n)\n\nrandom_test <- function(x) {\n  \n  indices <- sample(1:length(x), length(x)/2, replace = FALSE)\n  group1 <- x[indices]\n  group2 <- x[-indices]\n  tt <- t.test(group1, group2)\n  return(tt$p.value)\n  \n}\n\np <- vapply(1:10000, FUN = function(...) {random_test(x)}, FUN.VALUE = numeric(1))\nsum(p < 0.05)## [1] 500\nn_obsv <- 1000\nn_vars <- 100\nmat_cat <- matrix(\n  data = rbinom(n_obsv * n_vars, 1, 0.5),\n  nrow = n_obsv,\n  ncol = n_vars\n  )\nmat_all <- cbind(x, mat_cat)\ndf <- as.data.frame(mat_all)\nnames(df) <- c(\"x\", paste0(\"v\", 1:n_vars))\nhead(df)##          x v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 v11 v12 v13\n## 1 -0.56048  1  1  0  1  1  0  1  0  0   1   1   0   1\n## 2 -0.23018  1  1  0  0  0  0  0  1  1   1   1   0   0\n## 3  1.55871  1  0  0  0  1  0  1  1  0   1   0   0   0\n## 4  0.07051  1  0  1  1  0  0  1  0  1   1   1   0   0\n## 5  0.12929  1  1  1  0  0  0  0  0  0   1   1   1   1\n## 6  1.71506  1  0  0  1  1  0  1  1  1   1   1   0   1\n##   v14 v15 v16 v17 v18 v19 v20 v21 v22 v23 v24 v25 v26\n## 1   1   0   1   0   0   0   1   0   0   1   1   1   0\n## 2   1   1   1   1   1   0   0   1   0   1   0   1   0\n## 3   1   1   1   1   1   0   0   1   0   1   1   0   1\n## 4   0   0   0   1   0   0   0   1   1   0   1   1   0\n## 5   0   1   1   0   0   0   1   1   0   0   0   1   0\n## 6   1   0   0   0   0   1   1   0   0   0   1   1   0\n##   v27 v28 v29 v30 v31 v32 v33 v34 v35 v36 v37 v38 v39\n## 1   0   0   0   1   0   0   1   0   0   0   1   1   1\n## 2   1   0   0   1   1   0   1   0   0   0   1   0   0\n## 3   0   0   0   1   0   1   0   0   1   1   1   1   0\n## 4   0   0   0   1   1   1   1   0   0   1   0   0   0\n## 5   1   0   0   1   0   0   0   0   1   0   1   0   1\n## 6   0   1   1   0   1   1   0   0   0   1   1   0   1\n##   v40 v41 v42 v43 v44 v45 v46 v47 v48 v49 v50 v51 v52\n## 1   0   0   0   1   1   1   0   1   0   0   1   1   0\n## 2   0   0   0   0   0   0   0   1   0   1   0   1   1\n## 3   0   0   1   0   1   1   0   0   1   0   1   1   1\n## 4   0   1   0   1   0   0   0   1   0   0   0   0   1\n## 5   0   0   1   0   1   0   1   1   1   1   0   1   0\n## 6   1   1   1   1   0   0   0   1   0   0   1   0   0\n##   v53 v54 v55 v56 v57 v58 v59 v60 v61 v62 v63 v64 v65\n## 1   0   0   0   1   0   1   1   0   0   1   0   0   0\n## 2   0   0   1   0   1   0   1   1   0   1   0   1   1\n## 3   1   1   1   0   0   1   0   1   0   1   1   1   1\n## 4   1   1   0   0   0   0   1   0   1   1   1   1   1\n## 5   0   0   1   1   1   0   0   0   0   0   1   0   1\n## 6   1   1   0   0   1   0   1   1   0   1   1   0   1\n##   v66 v67 v68 v69 v70 v71 v72 v73 v74 v75 v76 v77 v78\n## 1   1   1   1   0   0   0   0   1   0   0   1   0   0\n## 2   1   1   0   1   1   0   1   1   1   1   1   1   1\n## 3   1   0   1   0   1   1   0   1   1   0   0   0   0\n## 4   0   1   0   0   0   1   0   1   0   0   0   0   0\n## 5   0   0   0   1   0   1   1   1   0   1   0   0   0\n## 6   0   1   0   1   0   0   0   1   1   0   1   0   0\n##   v79 v80 v81 v82 v83 v84 v85 v86 v87 v88 v89 v90 v91\n## 1   1   0   1   0   0   0   0   1   1   1   0   0   1\n## 2   1   1   0   1   1   1   0   0   1   1   0   0   0\n## 3   1   0   0   1   0   0   0   0   1   0   1   0   0\n## 4   0   0   0   0   1   0   1   1   0   0   0   0   0\n## 5   1   1   1   0   0   1   1   0   0   1   1   0   1\n## 6   1   0   1   1   0   0   0   0   1   1   0   1   0\n##   v92 v93 v94 v95 v96 v97 v98 v99 v100\n## 1   1   1   0   0   0   1   1   0    0\n## 2   0   1   0   1   1   0   1   0    1\n## 3   1   0   0   0   1   0   1   0    1\n## 4   0   0   1   1   0   0   1   1    1\n## 5   0   1   1   1   1   1   0   1    0\n## 6   0   0   0   0   1   1   0   0    1\nt.test(x ~ v1, data = df)$p.value\nt.test(x ~ v2, data = df)$p.value\nt.test(x ~ v3, data = df)$p.value\nt.test(x ~ v4, data = df)$p.value\n# etc.## [1] 0.09771\n## [1] 0.8734\n## [1] 0.02182\n## [1] 0.1525\nt.test(x ~ v1, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v2, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v3, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v4, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v5, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v6, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v7, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v8, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v9, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v10, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v11, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v12, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v13, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v14, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v15, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v16, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v17, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v18, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v19, data = df[1:(n_obsv/2),])$p.value## [1] 0.6022\n## [1] 0.4947\n## [1] 0.196\n## [1] 0.3682\n## [1] 0.2115\n## [1] 0.7113\n## [1] 0.3127\n## [1] 0.8142\n## [1] 0.9033\n## [1] 0.8212\n## [1] 0.4416\n## [1] 0.2564\n## [1] 0.5292\n## [1] 0.1715\n## [1] 0.0855\n## [1] 0.2285\n## [1] 0.6277\n## [1] 0.01318\n## [1] 0.2556\nt.test(x ~ v18, data = df[(n_obsv/2 + 1):n_obsv,])$p.value## [1] 0.1691"},{"path":"incr-infe.html","id":"regression-to-the-mean","chapter":"6 Incredible Inferences (TODO)","heading":"6.5 Regression to the mean","text":"simulate truly independent spend amounts across two periods","code":"\nset.seed(123)\n\nn  <- 1000\nmu <- 100\nsd <- 10\nspend1 <- rnorm(n, mu, sd)\nspend2 <- rnorm(n, mu, sd)\n\ndf <- data.frame(spend1, spend2)\nlibrary(dplyr)\n\ndf %>% \n  group_by(spend1 > mu) %>%\n  summarize_at(vars(starts_with(\"spend\")), mean) %>%\n  mutate(pct_change = round((spend2 - spend1) / spend1, 3))## # A tibble: 2 x 4\n##   `spend1 > mu` spend1 spend2 pct_change\n##   <lgl>          <dbl>  <dbl>      <dbl>\n## 1 FALSE           92.2   99.7      0.081\n## 2 TRUE           108.   101.      -0.063\ndf %>%\n  mutate(spend1_bin = cut(spend1, 5)) %>%\n  group_by(spend1_bin) %>%\n  summarize_at(vars(starts_with(\"spend\")), mean) %>%\n  mutate(pct_change = round((spend2 - spend1) / spend1, 3))## # A tibble: 5 x 4\n##   spend1_bin spend1 spend2 pct_change\n##   <fct>       <dbl>  <dbl>      <dbl>\n## 1 (71.8,84]    80.5   97.8      0.215\n## 2 (84,96.1]    91.1  100.       0.098\n## 3 (96.1,108]  102.   101.      -0.012\n## 4 (108,120]   113.   101.      -0.101\n## 5 (120,132]   124.   103.      -0.167\ndf %>%\n  mutate(spend1_bin = cut(spend1, 5)) %>%\n  group_by(spend1_bin) %>%\n  summarize(corr = cor(spend1, spend2))## `summarise()` ungrouping output (override with `.groups` argument)## # A tibble: 5 x 2\n##   spend1_bin    corr\n##   <fct>        <dbl>\n## 1 (71.8,84]   0.281 \n## 2 (84,96.1]  -0.0149\n## 3 (96.1,108]  0.0438\n## 4 (108,120]   0.101 \n## 5 (120,132]  -0.165\nmean(spend1 > spend2)\nmean(spend1 < spend2)## [1] 0.49\n## [1] 0.51\nsum((spend1 > mu) * (spend1 > spend2)) / sum(spend1 > mu)\nsum((spend1 < mu) * (spend1 < spend2)) / sum(spend1 > mu)## [1] 0.7168\n## [1] 0.7267\nlibrary(ggplot2)\n\nggplot(df) +\n  aes(x = spend1, y = spend2) + \n  geom_point()"},{"path":"cava-caus.html","id":"cava-caus","chapter":"7 Cavalier Causality (TODO)","heading":"7 Cavalier Causality (TODO)","text":"Chapter 6 (Incredible Inferences), began see can tricked biases lack causal thinking underlying theory data generating process. chapter, revisit disasters introduce specific frameworks help us rigorously explore analysis errors biases , even better, strategize best ways fix .","code":""},{"path":"mind-mod.html","id":"mind-mod","chapter":"8 Mindless Modeling (TODO)","heading":"8 Mindless Modeling (TODO)","text":"","code":""},{"path":"mind-mod.html","id":"features","chapter":"8 Mindless Modeling (TODO)","heading":"8.1 Features","text":"","code":""},{"path":"mind-mod.html","id":"targets","chapter":"8 Mindless Modeling (TODO)","heading":"8.2 Targets","text":"","code":""},{"path":"mind-mod.html","id":"evaluation-metrics","chapter":"8 Mindless Modeling (TODO)","heading":"8.3 Evaluation Metrics","text":"","code":""},{"path":"mind-mod.html","id":"unsupervised-learning","chapter":"8 Mindless Modeling (TODO)","heading":"8.4 Unsupervised Learning","text":"","code":""},{"path":"mind-mod.html","id":"lifecycle-management","chapter":"8 Mindless Modeling (TODO)","heading":"8.5 Lifecycle Management","text":"","code":""},{"path":"mind-mod.html","id":"fair-and-ethical-modeling","chapter":"8 Mindless Modeling (TODO)","heading":"8.6 Fair and Ethical Modeling","text":"","code":""},{"path":"alt-alg.html","id":"alt-alg","chapter":"9 Alternative Algorithms (TODO)","heading":"9 Alternative Algorithms (TODO)","text":"consummate showman, P.T. Barnum often quoted saying “Leave wanting .” Unfortunately, statistics professors less flare drama. Introductory statistics courses typically introduce types models (example, linear perhaps logistic regression), ’s wrap. ’s often students start taking subsequent courses exposed true limitations previous techniques taught demand .chapter attempts flip paradigm briefly surveying broad number modeling techniques. goal go rigorous deals one understand use models. Instead, hope build “mental toolbox” techniques know focus study encounter problem real world.","code":""},{"path":"alt-alg.html","id":"not-modeling","chapter":"9 Alternative Algorithms (TODO)","heading":"9.1 Not Modeling","text":"","code":""},{"path":"alt-alg.html","id":"first-principles","chapter":"9 Alternative Algorithms (TODO)","heading":"9.1.1 First Principles","text":"","code":""},{"path":"alt-alg.html","id":"simple-analyses","chapter":"9 Alternative Algorithms (TODO)","heading":"9.1.2 Simple Analyses","text":"","code":""},{"path":"alt-alg.html","id":"extending-linear-regression","chapter":"9 Alternative Algorithms (TODO)","heading":"9.2 Extending Linear Regression","text":"","code":""},{"path":"alt-alg.html","id":"modeling-binary-outcomes","chapter":"9 Alternative Algorithms (TODO)","heading":"9.2.1 Modeling Binary Outcomes","text":"","code":""},{"path":"alt-alg.html","id":"modeling-counts","chapter":"9 Alternative Algorithms (TODO)","heading":"9.2.2 Modeling Counts","text":"","code":""},{"path":"alt-alg.html","id":"modeling-time-until-an-event","chapter":"9 Alternative Algorithms (TODO)","heading":"9.2.3 Modeling Time Until an Event","text":"","code":""},{"path":"alt-alg.html","id":"modeling-repeated-measures-on-a-population","chapter":"9 Alternative Algorithms (TODO)","heading":"9.2.4 Modeling Repeated Measures on a Population","text":"","code":""},{"path":"alt-alg.html","id":"modeling-observations-in-a-nested-hierarchy","chapter":"9 Alternative Algorithms (TODO)","heading":"9.2.5 Modeling Observations in a Nested Hierarchy","text":"","code":""},{"path":"alt-alg.html","id":"causal-analysis-patterns","chapter":"9 Alternative Algorithms (TODO)","heading":"9.3 Causal Analysis Patterns","text":"Similar https://emilyriederer.netlify.app/post/causal-design-patterns/","code":""},{"path":"alt-alg.html","id":"special-data-types","chapter":"9 Alternative Algorithms (TODO)","heading":"9.4 Special Data Types","text":"","code":""},{"path":"alt-alg.html","id":"duration-analysis","chapter":"9 Alternative Algorithms (TODO)","heading":"9.4.1 Duration Analysis","text":"","code":""},{"path":"alt-alg.html","id":"time-space-data","chapter":"9 Alternative Algorithms (TODO)","heading":"9.4.2 Time & Space Data","text":"","code":""},{"path":"alt-alg.html","id":"bayesian-methods","chapter":"9 Alternative Algorithms (TODO)","heading":"9.5 Bayesian Methods","text":"","code":""},{"path":"alt-alg.html","id":"simulation-methods","chapter":"9 Alternative Algorithms (TODO)","heading":"9.6 Simulation Methods","text":"","code":""},{"path":"alt-alg.html","id":"agent-based","chapter":"9 Alternative Algorithms (TODO)","heading":"9.6.1 Agent-Based","text":"","code":""},{"path":"alt-alg.html","id":"discrete-event","chapter":"9 Alternative Algorithms (TODO)","heading":"9.6.2 Discrete Event","text":"","code":""},{"path":"alt-alg.html","id":"clustering-beyond-k-means","chapter":"9 Alternative Algorithms (TODO)","heading":"9.7 Clustering (beyond K-Means)","text":"","code":""},{"path":"alt-alg.html","id":"density-based","chapter":"9 Alternative Algorithms (TODO)","heading":"9.7.1 Density-Based","text":"","code":""},{"path":"alt-alg.html","id":"mixture-models","chapter":"9 Alternative Algorithms (TODO)","heading":"9.7.2 Mixture Models","text":"","code":""},{"path":"futi-find.html","id":"futi-find","chapter":"10 Futile Findings (TODO)","heading":"10 Futile Findings (TODO)","text":"","code":""},{"path":"comp-code.html","id":"comp-code","chapter":"11 Complexifying Code (TODO)","heading":"11 Complexifying Code (TODO)","text":"","code":""},{"path":"comp-code.html","id":"making-code-unreadable","chapter":"11 Complexifying Code (TODO)","heading":"11.1 Making code unreadable","text":"","code":""},{"path":"comp-code.html","id":"naming","chapter":"11 Complexifying Code (TODO)","heading":"11.1.1 Naming","text":"structure sorting, ordering, clear semantics","code":""},{"path":"comp-code.html","id":"whitespace","chapter":"11 Complexifying Code (TODO)","heading":"11.1.2 Whitespace","text":"linters / stylers","code":""},{"path":"comp-code.html","id":"making-a-monolith","chapter":"11 Complexifying Code (TODO)","heading":"11.2 Making a monolith","text":"using functions / files / templates / variables","code":""},{"path":"comp-code.html","id":"making-code-inflexible-variables","chapter":"11 Complexifying Code (TODO)","heading":"11.2.1 Making code inflexible (variables)","text":"","code":""},{"path":"comp-code.html","id":"making-useful-code-chunks-hard-to-reuse-functions","chapter":"11 Complexifying Code (TODO)","heading":"11.2.2 Making useful code chunks hard to reuse (functions)","text":"","code":""},{"path":"comp-code.html","id":"project-organization","chapter":"11 Complexifying Code (TODO)","heading":"11.3 Project organization","text":"","code":""},{"path":"comp-code.html","id":"decoding","chapter":"11 Complexifying Code (TODO)","heading":"11.4 Decoding","text":"rubber duck decoding","code":""},{"path":"reje-repr.html","id":"reje-repr","chapter":"12 Rejecting Reproducibility (TODO)","heading":"12 Rejecting Reproducibility (TODO)","text":"“Good Enough Practices Computational Reproducibility” (Wilson et al. 2017)Package managment (Ushey 2020)environment management","code":""},{"path":"useful-data-generation-functions-todo.html","id":"useful-data-generation-functions-todo","chapter":"A Useful Data Generation Functions (TODO)","heading":"A Useful Data Generation Functions (TODO)","text":"","code":""},{"path":"common-probability-distributions-todo.html","id":"common-probability-distributions-todo","chapter":"B Common Probability Distributions (TODO)","heading":"B Common Probability Distributions (TODO)","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
