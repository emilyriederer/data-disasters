[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Disasters",
    "section": "",
    "text": "Training in data analysis often begins with Statistics 101 course. Students learn the “happy path” of answer data that adheres to specific assumptions (such as “independent and identically distributed with a Normal density”) and answers pre-specified questions (most notably, the infamous null hypothesis significance test). Then, they venture out into the world of real-world data analysis where non-experimental data is rarely so well behaved and the questions asked of it are far more nuanced.\nNo one course should aim to teach students everything they should know about statistics. In fact, one of the best parts about a career in statistics is the responsibility and privilege of life-long learning. However, the flaw of introductory statistics is not that it’s incomplete, but that it’s not obvious how it is not complete. Statistics is a bad salesman. There’s no season finale, no cliff hanger, no teasing and hinting and promising more and better to come. Student may leave their studies believing that answering more complex data analysis questions is trivially easy (by relying on the one-size-fits-all “panacea” that they learned) or intractably difficult (when the assumptions of that method are not met.)\nThis book attempts to add more color to all the dimensions of data analysis while showcasing the nuances throughout the true life cycle of data analysis using two strategies.\nFirst, it attempts to highlight common pitfalls in all the parts of data analysis: from data management and computation to visualization, interpretation, and modeling and even to communication and collaboration. Data analysis is fundamentally a creative task, so there are rarely canonical one-size-fits-all solutions. Curiously, however, there are plenty of canonical issues even if they require different solutions in different settings. Thus, the goal of this book is to highlight common data disasters and, in doing so, help students cultivate an intuition for how to detect common problems before they occur in an important analysis.\nSecond, while exploring these data disasters, we humbly put forth a (woefully incomplete!) literature review of more advanced methods from statistics and other quantitative disciplines (e.g. economics, epidemiology), to help learners build a “mental index” of terms to search and techniques to study should they encounter a relevant problem.\n\nThe content in this book is currently being developed and is all subject to change.\nChapters and sections tagged as WIP (work-in-progress) have substantial content and are suitable for reading.\nChapters and sections tagged as TODO have minimal outlines or code examples (if that).\n\n\nIn particular, we will aim to help you avoid twelve types of data disasters:\n\n\nData Dalliances: Misinterpreting or misuing data based on how it was collected or what it represents\n\nComputational Quandaries: Letting computers do what you said and not what you meant\n\nEgregious Aggregations: Losing critical information when information is condensed\n\nVexing Visualization: Confusing ourselves or others with plotting choices\n\nIncredible Inferences: Drawing incorrect conclusions for analytical results\n\nCavalier Causality: Falling prey to spurious correlations masquerading as causality\n\nMindless Modeling: Failing to get the most value out of models by not tailoring the features, targets, and performance metrics\n\nAlternative Algorithms: Lacking an understanding of alternative methods which may be better suited for the problem at hand\n\nFutile Findings: Asking and answering questions that aren’t useful\n\nComplexifying Code: Making projects unwieldy or more difficult to understand than necessary\n\nRejecting Reproducibility: Working inefficiently instead of an efficient, reproducible, and sharable workflow\n\nMourning Mistakes: Letting the perfect be the enemy of the good\n\nIn each chapter, we will see numerous examples of each disaster and consider strategies to help us mitigate. Along the way, we’ll emphasize:\n\nThe importance of domain knowledge and the data-generating process to decide what it is you want to do\nThe utility of simulation as a tool to explore if, in fact, you are doing it\nThe exploration of counterexamples to build intuition for common patterns of problems even where common solutions don’t exist\n\nAs we go, we will notice how three common themes that challenge the focus of introductory statistics:\n\nSummary statistics mask interesting stories that we see when focusing on the variation\n\nSimilarly, observations and variables are rarely independent; the story is in the covariance\n\nAssumptions of Normality, or more broadly symmetry, are often in appropriate in wonky, highly skewed world\n\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important."
  },
  {
    "objectID": "comments.html",
    "href": "comments.html",
    "title": "Comments? Discuss here!",
    "section": "",
    "text": "This book is an active work-in-progress. I welcome any questions or suggestions.\nFeel free to open an issue or PR about specific parts of the book or to leave general comments below."
  },
  {
    "objectID": "author.html",
    "href": "author.html",
    "title": "About the Author",
    "section": "",
    "text": "Find me on my website and on Twitter"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction (TODO)",
    "section": "",
    "text": "Statistics is not synonymous with data analysis; rigor vs practicality\n“Evaluating the Success of a Data Analysis” (Hicks and Peng 2019)\n“Data Alone is not Ground Truth” (Bassa 2017)"
  },
  {
    "objectID": "intro.html#case-study",
    "href": "intro.html#case-study",
    "title": "1  Introduction (TODO)",
    "section": "1.1 Case Study",
    "text": "1.1 Case Study"
  },
  {
    "objectID": "intro.html#what-is-data",
    "href": "intro.html#what-is-data",
    "title": "1  Introduction (TODO)",
    "section": "1.2 What is data?",
    "text": "1.2 What is data?\nData is…"
  },
  {
    "objectID": "intro.html#what-is-analysis",
    "href": "intro.html#what-is-analysis",
    "title": "1  Introduction (TODO)",
    "section": "1.3 What is analysis?",
    "text": "1.3 What is analysis?\nAnalysis is the process of turning information into insight…S"
  },
  {
    "objectID": "intro.html#what-is-workflow",
    "href": "intro.html#what-is-workflow",
    "title": "1  Introduction (TODO)",
    "section": "1.4 What is workflow?",
    "text": "1.4 What is workflow?\nWorkflows are an intentional process for accomplishing a complex goal…"
  },
  {
    "objectID": "intro.html#what-is-data-analysis",
    "href": "intro.html#what-is-data-analysis",
    "title": "1  Introduction (TODO)",
    "section": "1.5 What is data analysis?",
    "text": "1.5 What is data analysis?\nData analysis altogether is…"
  },
  {
    "objectID": "intro.html#what-are-data-disasters",
    "href": "intro.html#what-are-data-disasters",
    "title": "1  Introduction (TODO)",
    "section": "1.6 What are data disasters?",
    "text": "1.6 What are data disasters?\nData disasters occur when…\n\n\n\n\n\nBassa, Angela. 2017. “Data Alone Isn’t Ground Truth.” Blog. Medium. https://medium.com/@angebassa/data-alone-isnt-ground-truth-9e733079dfd4.\n\n\nHicks, Stephanie C., and Roger D. Peng. 2019. “Evaluating the Success of a Data Analysis.” https://arxiv.org/abs/1904.11907."
  },
  {
    "objectID": "data-dall.html",
    "href": "data-dall.html",
    "title": "2  Data Dalliances (WIP)",
    "section": "",
    "text": "The first step to data analysis is, in fact, data. While this may seem obvious, statistics textbooks often dodge this detail. Discussions of regression analysis often begin with a statement like:\nbut in practice this statement is as absurd as writing a book about how to win a basketball game, assuming your team already has a 20 point lead with 1 minute left to play.\nIt’s very convenient but typically incorrect to assume that the data we happen to have is the ideal (or, more humbly, sufficient) data for the questions we wish to analyze. The specific vagaries of data vary greatly by domain, but a commonality across many fields (such as political science, economics, epidemiology, and market research) is that we are often called to work with found data (or, more formally, “observational data”) from administrative sources or production systems. In contrast to artisanally crafted data experimental data (like the carefully controlled agricultural experiments which motivated many early methods developments in statistics), this data was generated neither by us nor for us. To quote Angela Bassa, the head of data science at an e-commerce company: “Data isn’t ground truth. Data are artifacts of systems” (Bassa 2017).\nThe analytical implications of observational versus experimental data are well explored in the field of causal inference (which we will discuss some in Section 6 and Section 7). However, this distinction has implications far earlier in the data analysis process, as well. To name a few:\nIn this chapter, we will explore data structures and the full data generating process to better understand how different types of data challenges emerge. In doing so, we will hone sharper intuition for how our data can deceive us and what to watch out for when beginning an analysis."
  },
  {
    "objectID": "data-dall.html#preliminaries",
    "href": "data-dall.html#preliminaries",
    "title": "2  Data Dalliances (WIP)",
    "section": "\n2.1 Preliminaries",
    "text": "2.1 Preliminaries\nBefore we begin our exploration of data dalliances, we must first establish a baseline understanding of data structure, data production, and data quality.\n\n2.1.1 Data Structure Basics\n\n2.1.1.1 Relational Data Structure\nUnderstanding the content and structure of the data you are using is a critical prerequisite to analysis. In this book, we focus on tabular, structured data like one might find in an Excel spreadsheet or relational database.1\nIn particular, many tools work best with what R developer Hadley Wickham describes as “tidy data” (Wickham 2014). Namely:\n\nEach variable forms a column\nEach observation forms a row\nEach type of observational unit forms a table\n\nThis is analogous to how one generally finds data arranged in a database and how statisticians are used to conceptualizing it. For example, the design matrix of a linear model consists of one column of data for each independent variable to be included in the model and one row for each observation.2 As Wickham points out, this is also similar to what is called “3rd normal form” in the world of relational database management systems.\nUsing this data structure is valuable not only because it is similar to what many modern data tools expect, but also because it provides us a framework to think critically about what defined each observation and each variable in our dataset.\n\n2.1.1.2 Schemas\nThe first part of the definition of tidy data references variables in columns. These variables in turn consist of three main parts:\n\nA column name used to access and manipulate that data\nA data type used to store that data (e.g. and integer format or data format)\nA definition used to denote what that field represents in the real-world\n\nColumn names are always explicit. In a database or a language like R and python, the data type is also explicit.3 Ideally, the definition is explicitly defined in metadata or a data dictionary, but sometimes this may be implicit.\n\n2.1.2 Data Production Processes\nIn statistical modeling we discuss the data generating process: we can build models that describe the mechanisms that create our observations. We can broaden this notion to think about the generating process of each of these steps of data production.\nRegardless of the type of data (experimental, observational, survey, etc.), there are generally four main steps to production: collection, extraction, loading, and transformation.4\n\n\nCollect: The way in which signals from the real world are captured as data. This could include logging (e.g. for web traffic or system monitoring), sensors (e.g. temperature collection), surveys, and more\n\nExtract: The process of removing data from the place in which it was originally captured in preparation of moving it somewhere in which analysis can be done\n\nLoad: The process of loading the extracted data to its final destination\n\nTransform: The process of modeling and transforming data so that its structure is useful for analysis and its variables are interpretable\n\nTo better theorize about data quality issues, it’s useful to think of four DGPs (as shown in Figure 2.1): the real-world DGP, the data collection/extraction DGP5, the data loading DGP, and the data transformation DGP.\n\n\n\n\nFigure 2.1: A schematic of the data production process\n\n\n\n\n\n2.1.2.1 E-Commerce Data Example\nFor example, consider the role of each of these four DGPs for e-commerce data:\n\n\nReal-world DGP: Supply, demand, marketing, and a range of factors motivate a consumer to visit a website and make a purchase\n\nData collection DGP: Parts of the website are instrumented to log certain customer actions. This log is then extracted from the different operational system (login platforms, payment platforms, account records) to be used for analysis\n\nData loading DGP: Data recorded by different systems is moved to a data warehouse for further processing through some sort of manual, scheduled, or orchestrated job. These different systems may make data available at different frequencies.\n\nData transformation DGP: To arrive at that final data presentation requires creating a data model to describe domain-specific attributes with key variables crafted with data transformations\n\n2.1.2.2 Subway Data Example\nOr, consider the role of each of these four DGPs for subway ridership data6:\n\n\nReal-world DGP: Riders are motivated to use public transportation to commute, run errands, or visit friends. Different motivating factors may cause different weekly and annual seasonality\n\nData collection DGP: To ride the subway, riders go to a station and enter and exit through turnstiles. The mechanical rotation of the turnstile caused by a rider passing through is recorded\n\nData loading DGP: Data recorded at each turnstile is collected through a centralized computer system at the station. Once a week, each station uploads a flat file of this data to a data lake owned by the city’s Department of Transportation\n\nData transformation DGP: Turnstiles from different companies may have different data formats. Transformation may include harmonizing disparate sources, coding system-generated codes (e.g. Station XYZ) to semantically meaningful names (e.g. Main Street Station), and publishing a final unified representation across stations and across time\n\nThroughout this chapter, we’ll explore how understanding key concepts about each of these DGPs can help guide our intuition on where to look for problems.\n\n2.1.3 Data Quality Dimension\nTo guide our discussion of how data production can affect aspects of data quality, we need a guiding definition of data quality. This is challenging because data quality is subjective and task-specific. It matters much more if data is “fit for purpose” and operates in a way that is transparent to its users moreso than meeting some preordained quality standard.\nRegardless, it’s useful for our discussion to think about general dimensions of data quality. Here, we will rely on six dimensions of data quality outlined by Data Management Association (“Six Dimensions of Data Quality Assessment” 2020). Their official definitions are:\n\n\nCompleteness: The proportion of stored data against the potential of “100% complete”\n\nUniqueness: Nothing will be recorded more than once based upon how that thing is identified. It is the inverse of an assessment of the level of duplication\n\nTimeliness: The degree to which data represent reality from the required point in time\n\nValidity: Data are valid if it conforms to the syntax (format, type, range) of its definition\n\nAccuracy: The degree to which data correctly describes the “real world” object or event being described.\n\nConsistency: The absence of difference, when comparing two or more representations of a thing against a definition\n\n2.1.4 Questions to Ask (TODO)\nOur goal of understanding data is to ensure can assess its data quality and fit for our purpose. Understanding both its structure and its production process helps to accomplish this."
  },
  {
    "objectID": "data-dall.html#data-collection",
    "href": "data-dall.html#data-collection",
    "title": "2  Data Dalliances (WIP)",
    "section": "\n2.2 Data Collection",
    "text": "2.2 Data Collection\nOne of the tricky nuances of data collection is understanding what precisely is getting captured and logged in the first place. No matter how robust the sensors, loggers, or other mechanisms are that record our dataset, that data is still unfit for its purpose so long as the analyst does not fully understand what it represents. In the next section, we will see how what data gets collected (and our understanding of it) can alter our notions of data completion and how we must handle it in our computations.\n\n2.2.1 What Makes a Record (Row)\nThe first priority when starting to work with a dataset is understanding what a single record (row) represents and what causes it to be generated.\nConsider something as simple as a login system where users must enter their credentials, endure a Captcha-like verification process to prove that they are not a robot, and enter a multi-factor authentication code. Figure 2.2 depicts such a process.\n\n\n\n\nFigure 2.2: A diagram illustrating a multi-step process for a user to login to a website or app\n\n\n\n\nWhich of these events gets collected and recorded has a significant impact on subsequent data processing. In a technical sense, no inclusion/exclusion decision here is incorrect, per say, but if the producers’ choices don’t match the consumers’ understandings, it can lead to misleading results.\nFor example, an analyst might seek out a logins table in order to calculate the rate of successful website logins. Reasonably enough, they might compute this rate as the sum of successful events over the total. Now, suppose two users attempt to login to their account, and ultimately, one succeeds in accessing their private information and the other doesn’t. The analyst would probably hope to compute and report a 50% login success rate. However, depending on how the data is represented, they could quite easily compute nearly any value from 0% to 100%.\nAs a thought experiment, we can consider what types of events might be logged:\n\n\nPer Attempt: If data is logged once per overall login attempt, successful attempts only trigger one event, but a user who forgot their password may try (and fail) to login multiple times. In the case illustrated above, that deflates the successful login rate to 25%.\n\nPer Event: If the logins table contains a row for every login-related event, each ‘success’ will trigger a large number of positive events and each ‘failure’ will trigger a negative event preceded by zero or more positive events. In the case illustrated below, this inflates our successful login rate to 86%.\n\nPer Conditional: If the collector decided to only look at downstream events, perhaps to circumvent record duplication, they might decide to create a record only to denote the success or failure of the final step in the login process (MFA). However, login attempts that failed an upstream step would not generate any record for this stage because they’ve already fallen out of the funnel. In this case, the computed rate could reach 100%\n\n\nPer Intermediate: Similarly, if the login was defined specifically as successful password verification, the computed rate could his 100% even if some users subsequently fail MFA\n\nThese different situations are further illustrated in Figure 2.3, and their calculations are shown in Table 2.1.\n\n\n\n\nFigure 2.3: Login events recorded under different data collection paradigms\n\n\n\n\n\n\nTable 2.1: ?(caption)\n\n\n\n\n(a) Success rate naively computed under different data collection schemes\n\n\nSession\nAttempt\nEvent\nOutcome\nIntermediate\n\n\n\nSuccess\n1\n1\n6\n1\n2\n\n\nTotal\n2\n4\n7\n1\n2\n\n\nRate (%)\n50\n25\n86\n100\n100\n\n\n\n\n\n\n(Note that we did not explicitly “simulate” data here, but the workflow is largely the same. We imagine a real-world process, how that might translate into a digital representation, and created a numerical example to understand the implications. Not all simulations require fancy code; sometimes paper and a pencil or a thought experiment works just fine.)\nWhile humans have a shared intuition of what concepts like a user, session, or login are, the act of collecting data forces us to map that intuition onto an atomic event. Any misunderstanding in precisely what that definition is can have massive impact on the perceived data quality; “per event” data will appear heavily duplicated if it is assumed to be “per session” data.\nIn some cases, this could be obvious to detect. If the system outputs fields that are incredibly specific (e.g. with some hyperbole, imagine a step_in_the_login_process field with values taking any of the human-readable descriptions of the fifteen processes listed in the image above), but depending how this source is organized (e.g. in contrast to above, if we only have fields like sourceid and processid with unintuitive alphanumeric encoded values) and defined, it could be nearly impossible to understand the nuances without uncovering quality metadata or talking to a data producer.\n\n2.2.2 What Doesn’t Make a Record (Row)\nAlong with thinking about what does count (or gets logged), it’s equally important to understand what systemically does not generate a record. Consider users who have the intent or desire to login (motivated by a real-world DGP) but cannot find the login page, or users who load the login page but never click a button because they know that they’ve forgotten their password and see no way to request it. Often, some of these corner cases may be some of the most critical and informative (e.g. here, demonstrating some major flaws in our UI). It’s hard to computationally validate what data doesn’t exist, so conceptual data validation is critical.\n\n2.2.3 Records versus Keys\nThe preceding discussion on what types of real-world observations will or will not generate records in our resulting dataset is related to but distinct from another important concept from the world of relational databases: primary keys.\nPrimary keys are a minimal subset of variables in a dataset than define a unique record. For example, in the previous discussion of customer logins this might consist of natural keys7 such as the combination of a session_id and a timestamp or surrogate keys8 such as a global event_id that is generated every time the system logs any event.\nUnderstanding a table’s primary keys can be useful for many reasons. To name a few reasons, these fields are often useful for linking data from one table to another and for identifying data errors (if the uniqueness of these fields are not upheld). They also can be suggestive of the true granularity of the table.\nHowever, simply knowing a table’s primary keys does not resolve the issues we discussed in the prior two sections. Any of the many different data collection strategies we considered are unique by session and timestamp; however, as we’ve seen, that is no guarantee that they must contain every session and timestamp in the universe of events.\n\n2.2.4 What Defines a Variable (Column)\nJust as critical as understanding what constitutes a record (row) in a dataset is understanding the precise definition of each variable (column). Superficially, this task seems easier: after all, each variable has a name which hopefully includes some semantic information. However, quite often this information can provide a false sense of security. Just because you identify a variable with a promising sounding name, that does not mean that it is the most relevant data for your analysis.\nFor example, consider wanting to analyze patterns in customer spend amounts across orders on an e-commerce website. You might find a table of orders with a field called amt_spend. But what might this mean?\n\nIf the dataset is sourced from a payment processor, it likely includes the total amount billed to a customers’ credit card: including item prices less any discounts, shipping costs, taxes, etc. Alternatively, if this order was split across a gift card and a credit card, this field might only reflect the amount charged to the credit card\nIf the dataset is created for Finance, it might perhaps include only the total of item prices less discounts if this best corresponded to the data the Finance team needs for revenue reporting\nSomeone, somewhere, at some point might have assigned amt_spend to the name of the variable containing gross spend (before accounting for any discounts) and there might be a different variable amt_spend_net which accounts for discounts applied\n\nIt’s critical to understand what each variable actually means. The upside of this is that it forces analysts to think more crisply about their research questions and what the ideal variables for their analysis would be. As we’ve seen, concepts like “spend” may seem deceptively simple, but are not unambiguous."
  },
  {
    "objectID": "data-dall.html#data-extraction-loading",
    "href": "data-dall.html#data-extraction-loading",
    "title": "2  Data Dalliances (WIP)",
    "section": "\n2.3 Data Extraction & Loading",
    "text": "2.3 Data Extraction & Loading\nChecking that data contains expected and only expected records (that is, completeness, uniqueness, and timeliness) is one of the most common first steps in data validation. However, the superficially simple act of loading data into a data warehouse or updating data between tables can introduce a variety of risks to data completeness which require different strategies to detect. Data loading errors can result in data that is stale, missing, duplicate, inconsistently up-to-date across sources, or complete but for only a subset of the range you think.\nWhile the data quality principles of completeness, uniqueness, and timeliness would suggest that records should exist once and only once, the reality of many haphazard data loading process means data may appear sometime between zero and a handful of times. Data loads can occur in many different ways. For example, they might be:\n\nmanually executed\nscheduled (like a cron job)\norchestrated (with a tool like Airflow or Prefect)\n\nNo approach is free from challenges. For example, scheduled jobs risk executing before an upstream process has completed (resulting in stale or missing data); poorly orchestrated jobs may be prevented from working due to one missing dependency or might allow multiple stream to get out of sync (resulting in multisource missing data). Regardless of the method, all approaches must be carefully configured to handle failures gracefully to avoid creating duplicates, and the frequency at which they are executed may cause partial loading issues if it is incompatible with the granularity of the source data.\n\n2.3.0.1 Data Load Failure Modes\nTo develop our understanding of the true data generating process and to formulate theories on how our data could be broken (and what we should validate), it is useful to understand the different ways data extraction and loading can fail.\nFigure 2.4 illustrates a number of examples. Suppose that each row of boxes in the diagram represents one day of records in a table.\n\n\n\n\nFigure 2.4: Different modes of data loading failure\n\n\n\n\nOur dataset might be susceptible to:\n\n\nStale data occurs when the data is not as up-to-date as would be expected from is regular refresh cadence. This could happen if a manual step was skipped, a scheduled job was executed before the upstream source was available, or orchestrated data checks found errors and quarantined new records\n\nMissing data occurs when one data load fails but subsequent loads have succeeded\n\nDuplicate data occurs when one data load is executed multiple times\n\nMultisource missing data occurs when a table is loaded from multiple sources, and some have continued to update as expected while others have not\n\nPartial data occurs when a table is loaded correctly as intended by the producer but contains less data than expected by the consumer (e.g. a table loads ever 12 hours but because there is some data for a given date, the user assumes that all relevant records for that date have been loaded)\n\nThe differences in these failure modes become important when an analyst attempts to assess data completeness. One of the first approaches an analyst might consider is simply to check the min() and max() event dates in their table. However, this can only help detect stale data. To catch missing data, an analyst might instead attempt to count the number of distinct days represented in the data; to detect duplicate data, that analyst might need to count records by day and examine the pattern.\n\n\n\n\n\n\n\n\n\n\nMetric\nStale\nMissing\nDuplicate\nMulti\nPartial\n\n\n\n\nmin(date)max(date)\n\n13\n14\n14\n14\n14\n\n\ncount(distinct date)\n3\n3\n4\n4\n4\n\n\ncount(1) by date\n1001001000\n1001000100\n100100200100\n1001006666\n10010010050\n\n\n\ncount(1)count(distinct PKs)\n\n300300\n300300\n400300\n332332\n350350\n\n\n\nIn a case like the toy example above where the correct number of rows per date is highly predictable and the number of dates is small, such eyeballing is feasible; however when the expected number of records varies day-to-day or time series are long, this approach becomes subjective, error-prone, and intractable. Additionally, it still might be hard to catch errors in mutli-source data or partial loads if the lower number of records was still within the bounds of reasonable deviation for a series. These last two types deserve further exploration.\n\n2.3.0.2 Multi-Source\nA more effective strategy for assessing data completeness requires a better understanding of how data is being collected and loaded. In the case of multi-source data, one single source stopping loading may not be a big enough change to disrupt aggregate counts but could still jeopardize meaningful analysis. It would be more useful to conduct completeness checks by subgroup to identify these discrepancies.\nBut not any subgroup will do; the subgroup must correspond to the various data sources. For example, suppose we run an e-commerce store and wish to look at sales from the past month by category. Naturally, we might think to check the completeness of the data by category. But what if sales data is sourced from three separate locations: our Shopify site (80%), our Amazon Storefront (15%), and phone sales (5%). Unless we explicitly check completeness by channel (a dimension we don’t particularly care about for our analysis), it would be easy to miss if our data source for phone sales has stopped working or loads at a different frequency.\nAnother interesting aspect of multi-source data, is multiple sources can contribute either to different rows/records or different columns/variables. Table-level frequency counts won’t help us in the latter case since other sources might create the right total number of records but result in some specific fields in those records being missing or inaccurate.\n\n2.3.0.3 Partial Loads\nPartial loads really are not data errors at all, but are still important to detect since they can jeopardize an analysis. A common scenario might occur if a job loads new data every 12 hours (say, data from the morning and afternoon of day n-1 loads on day n at 12AM and 12PM, respectively). An analyst retrieving data at 11AM may be concerned to see an approximate ~50% drop in sales in the past day, despite confirming that their data looks to be “complete” since the maximum record date is, in fact, day n-1. Of course, this concern could be somewhat easily allayed if they then checked a timestamp field, but such a field might not exists or might not have been used for validation since its harder to anticipate the appropriate maximum timestamp than it is the maximum date.\n\n2.3.0.4 Delayed or Transient Records\nThe interaction between choices made in the data collection and data loading phases can introduce their own sets of problems.\nConsider an orders table for an e-commerce company that analysts may use to track customer orders. It might contain one record per order_id x event (placement, processing, shipment), one record per order placed, one record per order shipping, or one record per order with a status field that changes over time to denote the order’s current stage of life. Some of these options are illustrated in Figure 2.5.\n\n\n\n\nFigure 2.5: Illustration of alternative data collection and extraction strategies for order data\n\n\n\n\nAny of these modeling choices seem reasonable and the difference between them might appear immaterial. But consider the collection choice to record and report shipped events. Perhaps this might be operationally easier if shipment come from one source system whereas orders could come from many. However, an interesting thing about shipments is that they are often lagged in a variable way from the order date.\nSuppose the e-commerce company in question offers three shipping speeds at checkout. Figure 2.6 shows the range of possible shipment dates based on the order dates for the three different speeds (shown in different bars/colors).\n\n\n\n\nFigure 2.6: A conceptual chart of when different classes of real-world events might materialize as records in our dataset\n\n\n\n\nHow might this effect our perceived data quality?\n\nOrder data could appear stale or not timely since orders with a given order_date would only load days later once shipped\nSimilar to missing or multisource data, the data range in the table could lead to deceptive and incomplete data validation because some orders from a later order date might ship (and thus be logged) before all orders from a previous order date\nPut another way, we could have multiple order dates demonstrating partial data loads\nThese features of the data might behave inconsistently across time due to seasonality (e.g. no shipping on weekends or federal holidays), so heuristics developed to clean the data based on a small number of observations could fail\nFrom an analytical perspective, orders with faster shipping would be disproportionately overrepresented in the “tail” (most recent) data. If shipping category correlated with other characteristics like total order spend, this could create an artificial trend in the data\n\nOnce again, understanding that data is collected at point of shipment and reasoning how shipment timing varies and impacts loading is necessary for successful validation.\nIf this thought experiment seems to vague, we can make it more concrete by mocking up a dataset with which to experiment.\nIn the simplest version, we will simply suppose one order is submited on each of 10 days with dates (represented for convenience as integers and not calendar dates) given by the dt_subm vector. Suppose shipping always takes three days, so we can easily calculate the shipment date (dt_ship) based on the submission date. The shipment date is the same as the date the data will be logged and loaded (dt_load).\n\n# data simulation: single orders + deterministic ship dates ----\ndt_subm <- 1:10\ndays_to_ship <- 3\ndt_ship <- dt_subm + days_to_ship\ndt_load <- dt_ship\ndf <- data.frame(dt_subm, dt_ship, dt_load)\nhead(df)\n\n  dt_subm dt_ship dt_load\n1       1       4       4\n2       2       5       5\n3       3       6       6\n4       4       7       7\n5       5       8       8\n6       6       9       9\n\n\nSuppose we are an analyst living in day 5 and wonder how many orders were submitted on day 3. We can observe all shipments loaded before day 5 so we filter our data accordingly. However, when we count how many records exist for day 3 we find none. Instead, when we move ahead to an analysis date of day 7, we are able to observe the orders submitted on day 3.\n\nlibrary(dplyr)\n\n# how many day-3 orders do we observe as of day-5? ----\ndf %>% \n  filter(dt_load <= 5) %>% \n  filter(dt_subm == 3) %>% \n  nrow()\n\n[1] 0\n\n# how many day-3 orders do we observe as of day-7? ----\ndf %>% \n  filter(dt_load <= 7) %>% \n  filter(dt_subm == 3) %>% \n  nrow()\n\n[1] 1\n\n\n(Note that these conditions could be checked much more succinctly with a base R expression such as sum(df$dt_load < 7 & df$dt_subm == 3). However, there is sometimes virtue in option for more readable code even if it is less compact. Here, we prefer the more verbose option for the claritfy of our exposition. Such trade-offs, and general thoughts on coding style, are explored further in Section 11.)\nNow, this may seem to trivial. Clearly, if there were zero records for a day, we would catch this in data validation, right? We can make our synthetic data slightly more realistic to better illustrate the problem. Let’s not imagine that there are 10 orders each day, and each order is shipped sometime between 2 and 4 days after the order with equal probability.\n\n# data simulation: multiple orders + random ship dates ----\ndt_subm <- rep(x = 1:10, each = 10)\ndays_to_ship <- sample(x = 2:4, size = length(dt_subm), replace = TRUE)\ndt_ship <- dt_subm + days_to_ship\ndt_load <- dt_ship\ndf <- data.frame(dt_subm, dt_ship, dt_load)\nhead(df)\n\n  dt_subm dt_ship dt_load\n1       1       3       3\n2       1       5       5\n3       1       3       3\n4       1       3       3\n5       1       4       4\n6       1       4       4\n\n\nWhen we repeat the prior analysis, we now see that we have some records for orders submitted on day 3 by the time we begin analysis on day 5. In this case, we might be more easily tricked to believe this is all orders. However, when we repeat the analysis on day 7, we see the the number of orders on day 3 has increased.\nOf course, you can imagine the real world is yet much more complicated than this example. In reality, we would have a random number of orders each day. Additionally, we might have a mixture of different types of orders. There might be high-priced orders where customers tended to be willing to pay for faster shipping, and low-priced orders where customers tend to chose slower shipping. In a case like this, not only might naive validation miss the lack of data completeness, but the sample of shipments we begin to see on day 5 could be unrepresentative of the population of orders placed on day 3. This is a type of selection bias that we will examine further in Section 6 (Incredible Inferences)."
  },
  {
    "objectID": "data-dall.html#data-encoding-transformation-wip",
    "href": "data-dall.html#data-encoding-transformation-wip",
    "title": "2  Data Dalliances (WIP)",
    "section": "\n2.4 Data Encoding & Transformation (WIP)",
    "text": "2.4 Data Encoding & Transformation (WIP)\nOnce data is loaded into a more suitable location for processing and analysis (such as a data warehouse), it often undergoes numerous transformations to change its shape, structure, and content to be more suited for analytical use.\nFor example, recall the logins table that we discussed above. It might be filtered to a cleaner versions which represents only a subset of events, or event identifiers like '1436' might be recoded to more human-readable names like 'mfa-success'.\nUnfortunately, although these steps attempt to increase the data’s usability, they are also not immune to inserting bugs.\n\n2.4.1 Data Encoding\n\n2.4.1.1 Data Types\nOne critical set of decisions in data encoding is what sort of data types each field of interest should be. Data types such as integers, reals, character strings, logicals, dates, and times determine how data is stored and the types of manipulations that can be done to it.\nWe’ll see more examples of the computational implications for our data types in Section 3 (Computational Quandaries). This chapter specifically explores the unique complexities of string and date types.\n\n2.4.1.2 Indicator Variables (TODO)\nwhat is positive case?\n\n“We had a bunch of zeros that should have been coded ones and the ones should have been coded zeroes.”\n\n(https://retractionwatch.com/2013/01/04/paper-on-evidence-for-environmental-racism-in-epa-polluter-fines-retracted-for-coding-error/)\n\n2.4.1.3 General Representation (TODO)\n\n“These data sets often have multiple files that…have unclear and sometimes duplicative variables. Such complexities are commonplace among many data systems… I would not be surprised if coding errors were fairly common, and that the ones discovered constitute only the “tip of the iceberg.” ”\n\n(https://retractionwatch.com/2015/09/10/divorce-study-felled-by-a-coding-error-gets-a-second-chance/)\n\n2.4.1.4 The Many Meanings of Null\nAnother major encoding decision is how to handle null values. Previously, in the discussion of Data Collection, we considered the presence and absence of full records. However, when preparing data for analysis, both data produces and consumers need to decide how to cope with the presence or absence of individual fields.\nIf records contain some but not all relevant information, they may be published with explicitly missing fields or the full record may not be published at all. The difference between implicit and explicit missingness on the resulting data is illustrated in Figure 2.7.\n\n\n\n\nFigure 2.7: A comparison of explicit versus implicit missingness\n\n\n\n\nUnderstanding what the system implies by each explicitly missing data field is also critical for validation and analysis. Checks for data completeness usually include counting null values, but null data isn’t always incorrect. In fact, null data can be highly informative if we know what it means. Some meanings of null data might include:\n\n\nField is not relevant: Perhaps our logins table reports the mobile phone operating system (iOS or Android) that was used to access the login page to track platform-specific issues. However, there is no valid value for this\n\nRelevant value is not known: Our logins table might also have an account_id field which attempts to match login attempts to known accounts/customers using different metadata like cookies or IP addresses. In theory, almost everyone trying to log in should have an account identifier, but our methods may not be good enough to identify them in all cases\n\nRelevant value is null: Of course, sometimes someone without an account at all might try to log in for some reason. In this case, the correct value for an account_id field truly is null\n\nRelevant value was recorded incorrectly: Sometimes systems have glitches. Without a doubt, every single login attempt should have a timestamp, but such a field could be null if this data was somehow lost or corrupted at the source\n\nSimilarly, different systems might or might not report out these nulls in different ways such as:\n\n\nTrue nulls: Literally the entry in the resulting dataset is null\n\nNull-like non-nulls: Blank values like an empty string ('') that contain a null amount of information but won’t be detected when counting null values\n\nPlaceholder values: Meaningless values like an account_id of 00000000 for all unidentified accounts which preserve data validity (the expected structure) but have no intrinsic meaning\n\nSentinel/shadow values: Abnormal values which attempt to indicate the reasons for null-ness such as an account_id of -1 when no browser cookies were found or -2 when cookies were found but did not help link to any specific customer record\n\nEach of these encoding choices changes the definitions of appropriate completeness and validity for each field and, even more critically, impacts the expectations and assertions we should form for data accuracy. We can’t expect 100% completeness if nulls are a relevant value; we can’t check validity of ranges as easily if sentinel values are used with values that are outside the normal range (hopefully, or we have much bigger problems!) So, understanding how upstream systems should work is essential for assessing if they do work.\nSimilarly, understanding how our null data is collected has significant implications for how we subsequently process it. We will discuss this more in Chapter -Section 3 (Computational Quandaries).\n\n2.4.2 Data Transformation\nFinally, once the data is roughly where we want it, it likely undergoes many transformations to translate all of the system-generated fields we discussed in data collection into semantically-relevant dimensions for analytical consumers. Of course, the types of transformations that could be done are innumerable with far more variation than data loading. So, we’ll just look at a few examples of common failure patterns.\n\n2.4.2.1 Pre-Aggregation\nData transformations may include aggregating data up to higher levels of granularity for easier analysis. For example, a transformation might add up item-level purchase data to make it easier for an analyst to look at spend per order of a specific user.\nData transformations not only transform our data, but they also transform how the dimensions of data quality manifest. If data with some of the completeness or uniqueness issues we discussed with data loading is pre-aggregated, these problems can turn into problems of accuracy. For example, the duplicate or partial data loads that we discussed when aggregated could suggest inaccurately high or low quantities respectively.\n\n\n\n\n\n\n\n\n\n2.4.2.2 Field Encoding\nWhen we assess data consistency across tables,\nCategorical fields in a data set might be created in any number of ways including:\n\nDirectly taken from the source\nCoded in a transformation script\nTransformed with logic in a shared user-defined function (UDFs) or macro\n\nJoined from a shared look-up table\n\nEach approach has different implications on data consistency and usability.\n\n\n\n\n\n\n\n\nUsing fields from the source simply is what it is – there’s no subjectivity or room for manual human error. If multiple tables come from the same source, it’s likely but not guaranteed that they will be encoded in the same way.\nCoding transformations in the ELT process is easy for data producers. There’s no need to coordinate across multiple processes or use cases, and the transformation can be immediately modified when needed. However, that same lack of coordination can lead to different results for fields that should be the same.\nAlternatively, macros, UDFs, and look-up tables provided centralized ways to map source data inputs to desired analytical data outputs in a systemic and consistent way. Of course, centralization has its own challenges. If something in the source data changes, the process of updating a centralized UDF or look-up table may be slowed down by the need to seek consensus and collaborate. So, data is more consistent but potentially less accurate.\nRegardless, such engineered values require scrutiny – paticularly if they are being used as a key to join multiple tables – and the distinct values in them should be carefully examined.\n\n2.4.2.3 Updating Transformations\nOf course, data consistency is not only a problem across different data sources but within one data source. Regardless of the method of field encoding used in the previous step, the intersection of data loading and data transformation strategies can introduce data consistency errors over time.\nOften, for computation efficiency, analytical tables are loaded using an incremental loading strategy. This means that only new records (determined by time period, a set of unique keys, or other criteria) from the upstream source are loaded to the downstream table. This is in contrast to a full refresh where the entire downstream table is recreated on each update.\n\n\n\n\n\n\n\n\nIncremental loads have many advantages. Rebuilding tables in entirety can be very time consuming and computationally expensive. In particular, in non-cloud data warehouses that are not able to scale computing power on demand, this sort of heavy duty processing job can noticeably drain resources from other queries that are trying to run in the database. Additionally, if the upstream staging data is ephemeral, fully rebuilding the table could mean failing to retain history.\nHowever, in the case that our data transformations change, incremental loads may introduce inconsistency in our data overtime as only new records are created and inserted with the new logic.\n\n\n\n\n\n\n\n\nThis is also a problem more broadly if some short-term error is discovered either with data loading or transformation in historical data. Incremental strategies may not always update to include the corrected version of the data.\n\n\n\n\n\n\n\n\nRegardless, this underscores the need to validate entire datasets and to re-validate when repulling data."
  },
  {
    "objectID": "data-dall.html#more-on-missing-data-todo",
    "href": "data-dall.html#more-on-missing-data-todo",
    "title": "2  Data Dalliances (WIP)",
    "section": "\n2.5 More on Missing Data (TODO)",
    "text": "2.5 More on Missing Data (TODO)\nMCAR / MAR / MNAR"
  },
  {
    "objectID": "data-dall.html#other-types-of-data-todo",
    "href": "data-dall.html#other-types-of-data-todo",
    "title": "2  Data Dalliances (WIP)",
    "section": "\n2.6 Other Types of Data (TODO)",
    "text": "2.6 Other Types of Data (TODO)\n\n2.6.1 Survey Data\n\n2.6.2 Human-Generated"
  },
  {
    "objectID": "data-dall.html#strategies-todo",
    "href": "data-dall.html#strategies-todo",
    "title": "2  Data Dalliances (WIP)",
    "section": "\n2.7 Strategies (TODO)",
    "text": "2.7 Strategies (TODO)\n\n2.7.1 Understand the intent\n\nWhy was data originally collected? By whom and for what purpose?\nAre there clear, unique definitions of key concepts (e.g. entites, measures)? What are they?\nDocumentation (metadata, dictionaries)\n\n2.7.2 Understand the execution\n\nlearn about the real-world systems\nunderstand key steps in data production process\ndocumentation (lineage, provenance)\n\n2.7.3 Seek expertise\n\ntalking to experts (upstream and downstream)\n\n2.7.4 Trust but verify\n\nalways on data validation\n\nsummaries\ncontext-informed assertions\n\n\nexploratory analysis"
  },
  {
    "objectID": "data-dall.html#real-world-disasters",
    "href": "data-dall.html#real-world-disasters",
    "title": "2  Data Dalliances (WIP)",
    "section": "\n2.8 Real World Disasters",
    "text": "2.8 Real World Disasters\n\n2.8.1 Data loading artificially spikes COVID cases\nData loading, particularly when a new process is introduced can go astray and lead to spurious results. The city of El Paso discovered this after reporting an exceedingly high number of daily COVID cases (Borunda 2020):\n\nEl Paso city and public health officials on Thursday admitted to a major blunder, saying that the more than 3,100 new cases reported a day earlier was incorrect.\n\n\nThe number was the result of a two-day upload of cases in one day after the public health department went from imputing data manually to an automatic data upload that was intended to increase efficiency, Public Health Director Angela Mora said at news conference.\n\nThis instance demonstrates not only how data goes wrong but also how easy it is to trust discrepancies and find explanations in real-world phenomena. The article goes on to suggest that the extremely high case numbers were not immediately caught given the overall “alarming” levels:\n\nMora said she accepted responsibility and that she should have taken a deeper look after noticing the abnormally high number of new cases.El Paso County is still seeing more than 1,000 new cases on a daily basis, officials said. The correct number of new cases Wednesday was 1,537.\n\n\n“The numbers are still high and they are still alarming but they were not as high as we reported,” Mora said.\n\nSimilar data artifacts also arose from a variety of other factors besides system changes. For example, delays in 9-to-5 office paperwork (the real-world events triggering the creation of some key data elements) over the holidays led to more artificial trends in data reporting around the end-of-year holidays:\n\nWhile 238 deaths is the most reported in Illinois since the start of the pandemic, the IDPH noted in its release that some data was, “delayed from the weekends, including this past holiday weekend.”\n\n- Illinois sees biggest spike in reported COVID-19 deaths to date after holidays delay some data, officials say, WGN9 (TODO CITATION)\n\n2.8.2 Data encoding leads to incorrect BMI calculation\nData that is encoded inconsistently may be mishandled in automated decision processes. For example, data manually entered into electronic health records (EHR) by countless care provides might be compiled into a single source although the meanings could be inconsistent.\nIn the early days of the COVID19 vaccination push, some regions chose to prioritize their vaccine supply by certain individual characteristics including a high body mass index (BMI), which is a function of one’s weight relative to their height.\nOne 6ft 2in tall man’s data was misinterpreted to suggest that he was 6.2 centimeters tall; this caused his to get high priority for a vaccine.\nAs described in the BBC (“Covid: Man Offered Vaccine After Error Lists Him as 6.2cm Tall” 2021):\n\nA man in his 30s with no underlying health conditions was offered a Covid vaccine after an NHS error mistakenly listed him as just 6.2cm in height. Liam Thorp was told he qualified for the jab because his measurements gave him a body mass index of 28,000.\n\n\n\n\n\n\n\nBassa, Angela. 2017. “Data Alone Isn’t Ground Truth.” Blog. Medium. https://medium.com/@angebassa/data-alone-isnt-ground-truth-9e733079dfd4.\n\n\nBorunda, Daniel. 2020. “El Paso Officials Admit Massive COVID-19 Spike of 3,100 New Cases Was Error.” El Paso Times, November. https://www.elpasotimes.com/story/news/health/2020/11/05/coronavirus-update-el-paso-covid-19-restrictions-shutdowns-curfew/6174493002/.\n\n\n“Covid: Man Offered Vaccine After Error Lists Him as 6.2cm Tall.” 2021. BBC North West, February. https://www.bbc.com/news/uk-england-merseyside-56111209.\n\n\n“Six Dimensions of Data Quality Assessment.” 2020. DAMA UK Working Group. http://www.dama-nl.org/wp-content/uploads/2020/09/DDQ-Dimensions-of-Data-Quality-Research-Paper-version-1.2-d.d.-3-Sept-2020.pdf.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software, Articles 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10."
  },
  {
    "objectID": "comp-quan.html",
    "href": "comp-quan.html",
    "title": "3  Computational Quandaries (WIP)",
    "section": "",
    "text": "After gaining confidence in one’s data (or, at least, making peace with it), the next step in a data analysis is often to start cleaning and exploring that data with summary statistics, plots, and models. Generally, this requires a computational tool like SQL, R, or python.\nThe process of computation itself can be fraught with challenges. Computational tools are extremely literal; they are excellent at doing precisely what they were told to do but not often what analysts might have meant or wished that they would do. Additionally, the moment an analyst begins to use a tool, the conversation is no longer between them and the data; suddenly, the mental model of how every single tool developer thought you might want to do analysis affects the tools’ behaviors and the analysts’ results.\nIn this chapter, we will explore common ways that tools may do something technically correct, reasonable, and as-intended but very much not what analysts may expect. Along the way, we will see how computational methods interact with the data encoding choices we discussed in Section 2 (Data Dalliances)."
  },
  {
    "objectID": "comp-quan.html#preliminaries---data-computation",
    "href": "comp-quan.html#preliminaries---data-computation",
    "title": "3  Computational Quandaries (WIP)",
    "section": "\n3.1 Preliminaries - Data Computation",
    "text": "3.1 Preliminaries - Data Computation\nBefore we think about specific tools or failure modes, we can first consider the common types of operations that the analytical tools allow us to do with our data.\n\n3.1.1 Single Table Operations\nGiven a single data table, we may wish to do operations (illustrated in Figure 3.1) such as:\n\n\nFiltering: Extracting a subset of a dataset for analysis based on certain inclusion criteria for each record\n\nAggregation: Grouping our data table by one or more variables and condensing information across records with aggregate functions like counts, sums, and averages\n\nTransformation: Create new columns or modifying existing columns to represent more complex or domain-specific context\n\n\n\n\n\nFigure 3.1: Illustration of basic single-table data wrangling operations\n\n\n\n\n\n3.1.2 Multiple Table Operations\nOften, we can get additional value in an analysis by combining multiple types of information from difference tables. When working with multiple tables, we may be interested in:\n\n\nCombining Row-wise: Taking multiple tables with the same schemas (column names and data types) and creating a single table which contains the union (all records), intersection (only matching), or difference (only in one) of the records in the two tables\n\nCombining Column-wise: Appending additional fields to existing records through joining (also known as merging) multiple tables\n\n3.1.3 Mechanics\nAll of these operations rely on a few core computational tasks:\n\n\nArithmetic: Basic addition, subtraction, multiplication, and division to aggregate and transform data\n\nMapping: Similar to arithmetic, other one-to-one or many-to-one transformations of numerical or categorical data (such as binning into categories)\n\nEquality: Comparing whether or not two values are equal is critical for data filtering, column-wise combination, and certain types of data transformation\n\nCasting: Converting data types of different elements into a comparable format is necessary for row-wise combination and often a prerequisite to certain equality and arithmetic tasks\n\nWhile these operations may seem simple, their behavior within certain tools and when employed for certain data types may sometimes lead to unintuitive or misleading results."
  },
  {
    "objectID": "comp-quan.html#null-values",
    "href": "comp-quan.html#null-values",
    "title": "3  Computational Quandaries (WIP)",
    "section": "\n3.2 Null Values",
    "text": "3.2 Null Values\nIn Section 2 (Data Dalliances), we discuss how null values may represent many different concepts and be encoded in multiple different ways. In addition to those semantic challenges, various representations of null values may cause different computational problems.1 In this section, we will explore these potential failure modes.\n\n3.2.1 Types of Null Values\nNot only can null values represent many different things (as explored in Section 2), they also may be represented in many different ways. Understanding how nulls are encoded in one’s dataset is a critical prerequisite to attempting any of the computations described in the subsequent sections.\n\n3.2.1.1 Language representations\nDifferent programming languages each offer their own versions of null values – and sometimes more than one. For example, the R language includes NA, typed NAs (e.g. NA_integer_, NA_character_), NaN, and NULL; meanwhile, core python has None and the numpy module provides a nan.\nThese different values carry different semantic and functional meanings. For example R’s NA generally means “the presence of an absence” whereas NULL is “the absence of a presence”. This is articulated more clearly if we examine the lengths of these objects and observe that NA has a length 1 whereas NULL has a length 0.\n\nis.finite(NA)\nis.infinite(NA)\n\n[1] FALSE\n[1] FALSE\n\n\n\nlength(NA)\nlength(NULL)\n\n[1] 1\n[1] 0\n\n\nAs further proof that these are not interchangeable, we may use the helper functions is.na() and is.null(). It’s false that NA is NULL and essentially unable to be evaluated if NULL is NA because NULLs are truly nothing.2\n\nis.na(NA)\nis.null(NULL)\nis.na(NULL)\nis.null(NA)\n\n[1] TRUE\n[1] TRUE\nlogical(0)\n[1] FALSE\n\n\nTo further complicate matters, we have NaN (“not a number”), along with -Inf and Inf, which generally arise when we attempt to abuse R’s calculator. Somewhat charmingly, Inf and -Inf may be used in some rudimentary calculations where the limit is returned.3\n\n1/0\n0/0\n1/Inf\n\n[1] Inf\n[1] NaN\n[1] 0\n\n\n\n3.2.1.2 Sentinel value encoding\nBeyond these null types offered natively by different programming languages, there are also many different data management conventions for null values. Because null values can have many meanings, sometimes missing fields are encoded with “out of range” values which intend to suggest a type of missingness.\nFor example, the US Census Bureau’s Medical Expenditure Panel Survey uses the following reserved codes to denote different types of missingness: (TODO: cite p10 https://www.meps.ahrq.gov/data_stats/download_data/pufs/h206a/h206adoc.pdf)\n- -1 INAPPLICABLE Question was not asked due to skip pattern\n- -7 REFUSED Question was asked and respondent refused to answer question\n- -8 DK Question was asked and respondent did not know answer\n- -14 NOT YET TAKEN/USED Respondent answered that the medicine has not yet been used\n- -15 CANNOT BE COMPUTED Value cannot be derived from data\nThis approach preserves a lot of relevant information while, at the same time, being readily apparent that these values are not valid when the data is manually inspect. Unfortunately, manually inspecting every data field is rarely possible, and such sentinel values may go undetected when looking at higher-level summaries.\nConsider a survey of a population of retired adults where age is coded as 999 if not provided. Below, we simulate 100,000 such observations that are uniformly distributed between the age of 65 and 95 (hence, have an expected value of 80). Next, we replace merely half of a percent with our “null” values of 999. Taking the mean with these false values results in a mean of about 85. This number alone might not raise the alarm; after all, we know the dataset’s population is older adults. However, accidentally treating these as valid values biases our results by a somewhat remarkable five years.\n\nset.seed(123)\n\nn <- 100000\np <- 0.01 / 2\nages <- runif(n, 65, 95)\n\nages_nulls <- ages\nages_nulls[1:(n*p)] <- 999\n\nmean(ages)\nmean(ages_nulls)\n\n[1] 79.97897\n[1] 84.57468\n\n\nSo, the first order of business with null values is understanding how they are encoded and translating them to the most computationally appropriate form. However, that is only the beginning of the story.\n\n3.2.1.3 Other types of nulls (TODO)\nmight be blank string which won’t get detected in standard null checks\n\nx <- \"\"\nis.na(x)\n\n[1] FALSE\n\n\nsame thing is true in dataframes\n\nlibrary(dplyr)\ndf <- data.frame(x = c(\"a\", \"b\", \"\", \"d\"))\nsummarize_all(df, ~sum(is.na(.)))\n\n  x\n1 0\n\n\nin such cases, need to explicitly check for such alternative encodings like blank strings\n\nx <- \"\"\nis.na(x) | x == \"\"\n\n[1] TRUE\n\n\nof course, a blank string isn’t the only choice\ncould also have an empty string of any length\n\nx <- \"  \"\nis.na(x) | x == \"\"\nis.na(x) | trimws(x) == \"\"\nis.na(x) | nchar(trimws(x)) == 0\n\n[1] FALSE\n[1] TRUE\n[1] TRUE\n\n\nwe’ll see more about how fluid strings can be in the string section below\n\n3.2.2 Aggregation\nOnce null values are coded as “true” nulls, how these nulls are handled in the simple aggregation of data varies both across different languages and across different functions within a language. To better understand the problems this might cause, we will look at examples in R and SQL.\nTo explore aggregation, let’s build a simple dataset. We will suppose that we are working with a subscription-based e-commerce service and that we are looking at a spend dataset with one record per customer and information about the amount they spent and returned in a given month:\n\nspend <-\n  data.frame(\n    AMT_SPEND = c(10, 20, NA),\n    AMT_RETURN = rep(NA, 3)\n  )\n\nhead(spend)\n\n  AMT_SPEND AMT_RETURN\n1        10         NA\n2        20         NA\n3        NA         NA\n\n\nTo compute the average amount spent (AMT_SPEND) with the dplyr package, an analyst might first reasonably write the following summarize() statement. However, as we can see, due to the presence of null values within the AMT_SPEND column, the result of this aggregation is for the whole quantity of AVG_SPEND to be set to the value NA.\nA glance at the documentation for the mean() function4 reveals that the function has a parameter called na.rm. This parameter’s default value is FALSE, but, when it is set to TRUE, it removes null values from our dataset. Adding this argument to the previous statement allows us to reach a numerical answer.\n\nsummarize(spend, \n          AVG_SPEND = mean(AMT_SPEND),\n          AVG_SPEND_NARM = mean(AMT_SPEND, na.rm = TRUE))\n\n  AVG_SPEND AVG_SPEND_NARM\n1        NA             15\n\n\nHowever, is this the right numerical answer? Recall that what na.rm = TRUE does is drop the null values from the set of numbers being averaged. However, suppose the null values represent that no purchases were made for a given customer in a given month. That is, zero dollars were spent. In effect, we have removed all non-purchasers from the data being averaged.\nMore precisely, we have switched from taking the average\n\\[\n  \\frac{ \\sum_{1}^{n} Spend }{\\sum_{1}^{n} 1}\n\\] over all \\(n\\) customers\nto taking the average\n\\[\n  \\frac{ \\sum_{Spend > 0} Spend }{\\sum_{Spend > 0} 1}\n\\] over only those customers with spend.\nAt face value, we could say that the code above is giving the incorrect answer; by dropping some low (zero) purchase amounts, the average amount spend per customer is inflated. A second perspective, which is someone more philosophically troubling, is that this tiny change to the code which fixed the obvious problem (returning a null value) has introduced a non-obvious problem by fundamentally changing the question that we are asking. By dropping all accounts from our table who made no purchases, we are no longer answering “What is the average amount spent by all of our customer?” but rather “What is the average amount spent by an actively engaged customer?” In the language of probability theory, we might say that we have then changed our estimand from the expected value of spend to expected value of spend conditional on spend being positive. This technical quirk has significant analytical impact.\nTo answer the real question at hand, we have a couple of options. We could manually sum() the amount spent with the option to drop nulls but then divide by the correct denominator (all observations – not just those with spend) or we could explicitly recode null values in AMT_SPEND to zero before taking the average.5 Either of these options lead to the correct conclusion of a lower average spend amount.\n\nsummarize(\n    spend,\n    AVG_SPEND_MANUAL = sum(AMT_SPEND, na.rm = TRUE) / n(),\n    AVG_SPEND_RECODE = mean(coalesce(AMT_SPEND, 0))\n  )\n\n  AVG_SPEND_MANUAL AVG_SPEND_RECODE\n1               10               10\n\n\nThis is all well and good if we could just accept that the behaviors above are simply how nulls work, but further complexity comes as we see that there is no industry standard across tools. For example, as the SQL code below shows, SQL’s avg() function behaves more like R’s mean() with the na.rm = TRUE option set (whereas, you may recall that R’s mean() behaves with na.rm = FALSE by default). That is, the default behavior of SQL is to only operate on the valid and available values. The result of this default may mean that it is less obvious when our dataset has null values. SQL, unlike R, does not ask for “permission” to drop out nulls; instead, it unilaterally makes a decision how to handle these variables.\n\nSELECT \n  avg(amt_spend) as AVG_SPEND\nFROM spend\n\n\n\n  AVG_SPEND\n1        15\n\n\nHowever, this is not to suggest that null values cannot also be “destructive” in SQL (that is, returning null). While aggregation functions (which compute over the rows/records) like sum() and avg() drop nulls, operators like + and - (which compute across columns/variables in the same row/record) do not exhibit the same behavior. Consider, for example, if we wish to calculate the average net purchase amount (purchases minus returns) instead of the gross (total) purchase amount.\n\nSELECT \n  avg(amt_spend - amt_return) as AVG_SPEND_NET\nFROM spend\n\n\n\n  AVG_SPEND_NET\n1            NA\n\n\nDespite what we learned above about SQL’s avg() function, the query above returns only a null value. What has happened? In our spend dataset, the amt_return column is completely null (representing no return purchases). Because the subtraction occurs before the average is taken, subtracting null values in the amt_return variable from valid numbers in amt_spend variable creates a new variable of all null values. This new variable, which is already all null, is passed to the avg() function. This process is shown step-by-step below.\n\nSELECT\n  amt_spend, \n  amt_return, \n  amt_spend-amt_return \nFROM spend\n\n\n\n  AMT_SPEND AMT_RETURN amt_spend-amt_return\n1        10         NA                   NA\n2        20         NA                   NA\n3        NA         NA                   NA\n\n\n\n3.2.3 Comparison\nNull values don’t just introduce complexity when doing arithmetic. Difficulties also arise any time multiple variables are assessed for equality or inequality. Since a null value is unknown, most programming languages generally will not consider nulls to be comparable with other nulls.\nWe can observe simple examples of this in both R and SQL. In neither language can a null value be assessed for equality or inequality versus either another number or another null.\n\nNA == 3 \nNA > 10 \nNA == NA\n\n[1] NA\n[1] NA\n[1] NA\n\n\n\nSELECT\n  (NULL = 3) as NULL_EQ_NUM,\n  (NULL > 10) as NULL_GT_NUM,\n  (NULL = NULL) as NULL_EQ_NULL\n\n\n\n  NULL_EQ_NUM NULL_GT_NUM NULL_EQ_NULL\n1          NA          NA           NA\n\n\nIn these toy examples, such outcomes may seem perfectly logical. However, this same reasoning can arise in sneakier ways and lead to uninteded results when equality evaluations are implicit in the task at hand instead of the singular focus. We’ll now see examples from data filtering, joining, and transformation.\n\nIf you’re familiar with SQL, you may have been surprised to notice that there is no FROM clause in the query above. In fact, SQL queries can treat values just like variables containing only a single record.\nWe will use this trick throughout the chapter for exploring how SQL works when we don’t have an ideal sample dataset to test certain scenarios. Beyond exposition in this book, this trick is also useful in practice.\n\n\n3.2.3.1 Filtering\nSuppose we want to split our dataset into two datasets based on high or low values of spend. We might assume the following two lines of code will create a clear partition (implying that each record would fall into exactly one group.)\n\nspend_lt20 <- filter(spend, AMT_SPEND < 20)\nspend_gte20 <- filter(spend, AMT_SPEND >= 20)\n\nHowever, if we examine the resulting datasets, we see that neither contains the third record which had a null value for the AMT_SPEND variable.\n\nspend_lt20\n\n  AMT_SPEND AMT_RETURN\n1        10         NA\n\nspend_gte20\n\n  AMT_SPEND AMT_RETURN\n1        20         NA\n\n\nThe same situation results in SQL.\n\nSELECT *\nFROM spend\nWHERE AMT_SPEND < 20\n\n\n\n  AMT_SPEND AMT_RETURN\n1        10         NA\n\n\n\nSELECT *\nFROM spend\nWHERE AMT_SPEND >= 20\n\n\n\n  AMT_SPEND AMT_RETURN\n1        20         NA\n\n\nThis is a direct result of the fact that nulls cannot be compared for equality any inequality. We can think of data filtering as a two-step process: first evaluate whether the condition is TRUE or FALSE, then return only the records for which the condition holds true. When we conduct the more manual process of filtering step-by-step, we see that the null value of AMT_SPEND does not get a “truth value” when compared with a number. Thus, it is not contained in either “truth value” subset.\n\nmutate(spend, is_lt20 = (AMT_SPEND < 20))\n\n  AMT_SPEND AMT_RETURN is_lt20\n1        10         NA    TRUE\n2        20         NA   FALSE\n3        NA         NA      NA\n\n\nThus, whenever our data has null values, the very common act of data filtering risks excluding important information.\n\n3.2.3.2 Joining\nThe same phenomenon as described above also happens when joining multiple datasets.\nSuppose we have multiple datasets we wish to merge based on columns denoting a record’s name and date of birthday. For ease of exploration, we will make the simplest possible such dataset and simply try to merge it to itself. (This may seem silly, but often when trying to understand computationally complex things, it is a good idea to make the scenario as simple as possible. In fact, this idea is core to the concept of computational unit tests which we will discuss at the end of this chapter.)\n\nbday <- data.frame(NAME = c('Anne', 'Bob'), BIRTHDAY = c('2000-01-01', NA))\nbday\n\n  NAME   BIRTHDAY\n1 Anne 2000-01-01\n2  Bob       <NA>\n\n\nIn SQL, if we try to join this table, the records in row 1 will match because 'Anne' == 'Anne' and '2000-01-01' == '2000-01-01'. However, poor Bob’s record is eliminated because his birth date is logged as null, and NA == NA is false.\n\nSELECT a.*\nFROM\n  bday as a\n  INNER JOIN\n  bday as b\n  ON\n  a.NAME = b.NAME and\n  a.BIRTHDAY = b.BIRTHDAY\n\n\n\n  NAME   BIRTHDAY\n1 Anne 2000-01-01\n\n\nIn contrast, R’s dplyr::inner_join() function will not do this by default. This function lets us specifically control how nulls are matches with the na_matches argument, with a default option to match on NA values. (You may read more about the argument by typing ?dplyr::inner_join in the R console to pull up the documentation.)\n\ninner_join(bday, bday, by = c('NAME', 'BIRTHDAY'))\n\n  NAME   BIRTHDAY\n1 Anne 2000-01-01\n2  Bob       <NA>\n\n\nThis example then is not only a cautionary tale for how null values may unintentionally corrupt our data transformations but also how “brittle” our knowledge and intuition may be when moving between tools. Neither of these default behaviors is strictly better or worse, but they are definitely different and have real implications on our analysis.\n\n3.2.3.3 Transformation\nA common task in data analysis is to aggregate results by subgroup. For example, we might want to summarize how many customers (rows/records) spent more or less than $10. To discern this, we might create a categorical variable for high versus low purchase amounts, group by this variable and count.\nThe psuedocode would read something like this:\n\ndata %>%\n  mutate(HIGH_LOW = << transform AMT_SPEND >>) %>%\n  group_by(HIGH_LOW) %>%\n  count()\n\nTo define the HIGH_LOW variable, we might use a function like ifelse(), dplyr::if_else(), or dplyr::case_when(). However, once again, we have the issue of how values are partitioned when nulls are included. If we recode any records with AMT_SPEND of less than or equal to 10 to “Low” and default the rest to “High”, we will accidentally count all null values in the “High” group.\n\nspend %>%\n  mutate(HIGH_LOW = case_when(\n    AMT_SPEND <= 10 ~ \"Low\", \n    TRUE ~ \"High\")\n    ) %>%\n  group_by(HIGH_LOW) %>% \n  count()\n\n# A tibble: 2 x 2\n# Groups:   HIGH_LOW [2]\n  HIGH_LOW     n\n  <chr>    <int>\n1 High         2\n2 Low          1\n\n\nInstead, it is more accurate and transparent (unless we know specifically what null values mean and what group they should be part of) to not let one of our “core” categories by the “default” case in our logic. We can explicitly encode any residual values as something like “OTHER” or “ERROR” to help us see that there is a problem requiring extra attention.\n\nspend %>%\n  mutate(HIGH_LOW = case_when(\n    AMT_SPEND <= 10 ~ \"Low\", \n    AMT_SPEND > 10 ~ \"High\",\n    TRUE ~ \"OTHER\")\n    ) %>%\n  group_by(HIGH_LOW) %>% \n  count()\n\n# A tibble: 3 x 2\n# Groups:   HIGH_LOW [3]\n  HIGH_LOW     n\n  <chr>    <int>\n1 High         1\n2 Low          1\n3 OTHER        1\n\n\nWhile nulls contribute to this issue, it’s important to realize that nulls are not the only factor causing this error nor or they the solution. The more substantial issue is the careless use of defaults and implicit encoding versus explicit encoding. In the second form of the SQL query above, we are more specific about exactly what is allowed in each category which ensures any unexpected inputs will not be allowed to “sneak” into ordinary outputs."
  },
  {
    "objectID": "comp-quan.html#logicals-todo",
    "href": "comp-quan.html#logicals-todo",
    "title": "3  Computational Quandaries (WIP)",
    "section": "\n3.3 Logicals (TODO)",
    "text": "3.3 Logicals (TODO)\nMuch like the different versions of nulls that we met in the last section, logical data types use reserved keywords to represent TRUE and FALSE. (The exact formats of logical reserved keywords vary by language. R and SQL use TRUE and FALSE and python uses TRUE and FALSE.) This means that these names function like a number or a letter which intrinsically hold one specific value and cannot take on a different value besides their own.\n\nTRUE = 5\n\nError in TRUE = 5: invalid (do_set) left-hand side to assignment\n\n2 = 5\n\nError in 2 = 5: invalid (do_set) left-hand side to assignment\n\n\nAcross languages, TRUE and FALSE are considered equivalent to the numeric representations of 1 and 0 respectively.\n\nas.numeric(TRUE)\nas.numeric(FALSE)\n\n[1] 1\n[1] 0\n\n\nA consequence of this numerical equivalency is that TRUE and FALSE may be compared to each other or other numbers and be included in mathematical expressions.\n\nTRUE > FALSE\nTRUE < 5\nFALSE > -1\nTRUE + 1\nTRUE * 5\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] 2\n[1] 5\n\n\nsimilar in SQL\n\nselect\n  TRUE > FALSE as a,\n  TRUE < 5 as b,\n  FALSE > -1 as c,\n  TRUE + 1 as d,\n  TRUE * 5 as e\n\n\n1 records\n\na\nb\nc\nd\ne\n\n\n1\n1\n1\n2\n5\n\n\n\n\n\n3.3.1 Language-specific nuances (CUT?)\n\n3.3.1.1 Keyword abbrevations\nR also recognizes the abbreviations of T and F as TRUE and FALSE respectively; however this is not recommended. T and F are not reserved keywords, so they can be overwritten with a different value. This makes code using such abbreviations “brittle” and less reliable.\n\nif (T) {\"Hello\"} else {\"Goodbye\"}\nif (F) {\"Hello\"} else {\"Goodbye\"}\nT = 0\nif (T) {\"Hello\"} else {\"Goodbye\"}\n\n[1] \"Hello\"\n[1] \"Goodbye\"\n[1] \"Goodbye\"\n\n\n\n3.3.1.2 Alternative representations\nWe’ve previously seen that logicals have associated numerical values. Different languages may also treat their string representations differently.\nFor example, R believes that the string \"TRUE\" is equal to the logical value TRUE when directly compared. However, this relationships breaks the transitive property of mathematics6 because TRUE equals both 1 and \"TRUE\", yet \"TRUE\" does not equal 1 so the mathematical operations that can be done with logical TRUE cannot be done with string \"TRUE\".\n\nTRUE == 1\nTRUE == \"TRUE\"\nTRUE == \"True\"\nTRUE * 5\n\"TRUE\" * 5\n\nError in \"TRUE\" * 5: non-numeric argument to binary operator\n\n\n[1] TRUE\n[1] TRUE\n[1] FALSE\n[1] 5\n\n\nIn contrast neither SQL not python honor the string forms of their respective logical reserved keywords at all.\n\nselect \n  (TRUE == 1) as is_int_true,\n  (TRUE == 'TRUE') as is_char_true,\n  TRUE*5 as true_times_five,\n  'TRUE'*5 as true_str_time_five\n\n\n\n  is_int_true is_char_true true_times_five true_str_time_five\n1           1            0               5                  0\n\n\n\nTrue == 1\nTrue == \"True\"\nTrue == \"TRUE\"\nTrue * 5\n\"True\" * 5\n\nTrue\nFalse\nFalse\n5\n'TrueTrueTrueTrueTrue'\n\n\n\n3.3.2 Comparison (TODO)\nThe nuances of logical representation and handling seem straightforward in isolation. However, when encountered in real-world data problems, they are not isolated and are unlikely to be our main focus.\nImagine two datasets which all encode the same information but use boolean, string, and integer representations of a logical respectively.\n\ndf1 <- data.frame(ID = 1:3, IND = rep(TRUE, 3), X = 1:3)\ndf2 <- data.frame(ID = 1:5, IND = rep('TRUE', 5), Y = 11:15)\ndf3 <- data.frame(ID = 1:5, IND = rep(1, 5), Z = 21:25)\n\nBy simple inspection, the logical and string representations in particular look superficially similar and yet they will behave differently.\n\nhead(df1)\n\n  ID  IND X\n1  1 TRUE 1\n2  2 TRUE 2\n3  3 TRUE 3\n\nhead(df2)\n\n  ID  IND  Y\n1  1 TRUE 11\n2  2 TRUE 12\n3  3 TRUE 13\n4  4 TRUE 14\n5  5 TRUE 15\n\n\nIf we use R’s dplyr::filter() or base::subset() function to subset the data, the value of df1 will correctly subset based on the boolean values of IND. However, R will not know how to interpret the string version in df2.\n\nfilter(df1, IND)\nfilter(df2, IND)\n\nError: Problem with `filter()` input `..1`.\ni Input `..1` is `IND`.\nx Input `..1` must be a logical vector, not a character.\n\nsubset(df2, df2$IND)\n\nError in subset.data.frame(df2, df2$IND): 'subset' must be logical\n\n\n  ID  IND X\n1  1 TRUE 1\n2  2 TRUE 2\n3  3 TRUE 3\n\n\n\nfilter(df1, isTRUE(IND))\nfilter(df2, isTRUE(IND))\n\n[1] ID  IND X  \n<0 rows> (or 0-length row.names)\n[1] ID  IND Y  \n<0 rows> (or 0-length row.names)\n\n\n\nfilter(df2, isTRUE(IND))\n\n[1] ID  IND Y  \n<0 rows> (or 0-length row.names)\n\nleft_join(df1, df2, by = c(\"ID\", \"IND\"))\n\nError: Can't join on `x$IND` x `y$IND` because of incompatible types.\ni `x$IND` is of type <logical>>.\ni `y$IND` is of type <character>>.\n\n\n\nmerge(df1, df2, by.x = c(\"ID\", \"IND\"), by.y = c(\"ID\", \"IND\"), all.x = TRUE)\n\n  ID  IND X  Y\n1  1 TRUE 1 11\n2  2 TRUE 2 12\n3  3 TRUE 3 13\n\n\n\n\n\n\nselect df1.*, df2.*\nfrom \n df1 \n left join\n df2\n on\n df1.id = df2.id and\n df1.ind = df2.ind\n\n\n3 records\n\nID\nIND\nX\nID\nIND\nY\n\n\n\n1\n1\n1\nNA\nNA\nNA\n\n\n2\n1\n2\nNA\nNA\nNA\n\n\n3\n1\n3\nNA\nNA\nNA\n\n\n\n\n\n\nimport pandas as pd \n\ndata1 = {'ID': list(range(1, 4)),\n        'IND': [True] * 3,\n        'X': list(range(1,4))\n        }\ndata2 = {'ID': list(range(1, 6)),\n         'IND': ['True'] * 5,\n        'Y': list(range(11, 16))}\n\ndf1 = pd.DataFrame(data1)\ndf2 = pd.DataFrame(data2)\n\ndf1\ndf2\n\n   ID   IND  X\n0   1  True  1\n1   2  True  2\n2   3  True  3\n   ID   IND   Y\n0   1  True  11\n1   2  True  12\n2   3  True  13\n3   4  True  14\n4   5  True  15\n\n\n\npd.merge(left = df1, right = df2, how = \"left\", on = ['ID', 'IND'])\n\n   ID   IND  X   Y\n0   1  True  1 NaN\n1   2  True  2 NaN\n2   3  True  3 NaN"
  },
  {
    "objectID": "comp-quan.html#numbers-todo",
    "href": "comp-quan.html#numbers-todo",
    "title": "3  Computational Quandaries (WIP)",
    "section": "\n3.4 Numbers (TODO)",
    "text": "3.4 Numbers (TODO)\n\n3.4.1 Integer division\nIn R (mostly what we’d expect)\n\n1/2\n\n[1] 0.5\n\n\n\nifelse(3/4 > 0.5, 'High', 'Low')\n\n[1] \"High\"\n\n\nIn some SQL dialect SQL (SQLite shown. “Modern” interfaces like Snowflake and BigQuery don’t do this)\n\nSELECT (1/2) as one_div_two\n\n\n\n  one_div_two\n1           0\n\n\nThis rounding can get masked when we are recoding or doing subsequent calculations\n\nSELECT\n  case\n    when 3/4 > 0.5 then 'High' \n    else 'Low'\n  end as high_low_int,\n  case\n    when 3.0 / 4 > 0.5 then 'High'\n    else 'Low'\n  end as high_low_float\n\n\n\n  high_low_int high_low_float\n1          Low           High\n\n\nThe above works because of implicit casting. We can also do explicit casting, but where we do this matters\n\nSELECT\n  case \n    when cast(3 as float) / 4 > 0.5 then 'High'\n    else 'Low'\n  end as cast_first,\n  case\n    when cast(3/4 as float) > 0.5 then 'High'\n    else 'Low'\n  end as cast_last\n\n\n\n  cast_first cast_last\n1       High       Low\n\n\n\n3.4.2 Inexact storage and comparison\nIn R\n\n(3 - 2.9) <= 0.1\n(2 - 1.9) <= 0.1\n(1 - 0.9) <= 0.1\n\n[1] FALSE\n[1] FALSE\n[1] TRUE\n\n\n\n(3 - 2.9) == 0.1\n(2 - 1.9) == 0.1\n(1 - 0.9) == 0.1\n\n[1] FALSE\n[1] FALSE\n[1] FALSE\n\n\ndespite the fact that we see no difference\n\n3 - 2.9\n2 - 1.9\n1 - 0.9\n\n[1] 0.1\n[1] 0.1\n[1] 0.1\n\n\nIn python - same problem but in slightly different cases\n\n(3 - 2.9) <= 0.1\n(2 - 1.9) <= 0.1\n(1 - 0.9) <= 0.1\n\nFalse\nFalse\nTrue\n\n\nhere we can see the differences\n\n3 - 2.9\n2 - 1.9\n1 - 0.9\n\n0.10000000000000009\n0.10000000000000009\n0.09999999999999998\n\n\nSame thing with SQL where differences are masked\n\nselect\n  3 - 2.9 as sub_three,\n  2 - 1.9 as sub_two,\n  1 - 0.9 as sub_one,\n  (3 - 2.9) <= 0.1 as sub_compare_three,\n  (2 - 1.9) <= 0.1 as sub_compare_two,\n  (1 - 0.9) <= 0.1 as sub_compare_one\n\n\n\n  sub_three sub_two sub_one sub_compare_three sub_compare_two sub_compare_one\n1       0.1     0.1     0.1                 0               0               1\n\n\nInstead, can use either built-in equality checker (python equivalent is math.isclose) or check that difference between two numbers is in very small window\n\nall.equal(1 - 0.9, 0.1)\n\nabs( (1-0.9) - 0.1 ) <= 1e-10\n\n[1] TRUE\n[1] TRUE\n\n\nAnother example\n\n0.6 + 0.3\n\n[1] 0.9\n\n0.6 + 0.3 == 0.9\n\n[1] FALSE\n\nprint(.1 + .2)\n\n[1] 0.3\n\nprint(.1 + .2, digits = 18)\n\n[1] 0.30000000000000004\n\n\n\n3.4.3 Division by zero\nmore of a design issue about the right way to handle\nwe’ve seen before how we have to understand other peoples use of null values\nthis is a case where we he to decide what makes most sense"
  },
  {
    "objectID": "comp-quan.html#strings-wip",
    "href": "comp-quan.html#strings-wip",
    "title": "3  Computational Quandaries (WIP)",
    "section": "\n3.5 Strings (WIP)",
    "text": "3.5 Strings (WIP)\nString data can be inherently appealing. At their best, strings are used to bring more readable and human interpretable values into a dataset. However, string data and the processing thereof comes with its own challenges.\nFirst, unlike numbers, human language strings can be ambiguously defined. 2 is the only number to represent the value of two. However, the incorporation of human language means that many different words, phrases, and formatting choices can represent the same concept. This is confounded by instances where string data was manually entered, as is the case with user-input data.7\nSecondly, string data is one of the most flexible datatypes and can contain any other types of information – from should-be-logical values (\"yes\"/\"no\", \"true\"/\"false\"), should-be-numeric values (\"27\"), should-be-date values (\"2020-01-01\"), and even complex data encodings like JSON blobs (\"{\"name\":{\"first\":\"emily\",\"last\":\"riederer\"},\"social\":{\"twitter\":\"emilyriederer\",\"github\":\"emilyriederer\",\"linkedin\":\"emilyriederer\"}}\" with hideous formatting for emphasis.) For a data publisher, this may be a convenience, but as we will see it can turn into a frustration or a liability when functions and comparison operations are attempted with strings that semantically represent a different type of value.\n\n3.5.1 Dirty Strings (TODO)\nwhitespace\n\n\"a\" == \"a\"\n\"a b\" == \"a b\"\n\"a b\" == \"a  b\"\n\"a b\" == \"a b \"\n\n[1] TRUE\n[1] TRUE\n[1] FALSE\n[1] FALSE\n\n\n“fancy” characters (alternate encodings like ms word)\n\n' \" ' == ' \" '\n' “ ' ===' \" ' '\n\n[1] TRUE\n[1] FALSE\n\n\nspecial characters and display versus values\n\nx <- \"a\\tb\"\ncat(x) # what you see...\nx == \"a    b\" # ...is not what you get\n\na   b[1] FALSE\n\n\n\n3.5.2 Regular Expressions (TODO)\nwe promised not to be solution oriented, but\nnot knowing regex is a disaster when trying to work with string data…\n\n3.5.3 Comparison\nTODO\n\n3.5.3.1 String ordering\nStrings are ranked based on alphabetical order just like a dictionary. Some properties of this ordering include that:\n\nnumbers are smaller than letters (1 < \"a\")\nlower-case is smaller than upper case (\"a\" < \"A\")\nfewer characters are smaller than more characters (\"a\" < \"aa\")\n\nSuch rules make perfect sense for true characters. However, when strings are used as a “catch all” to represent other structures, typical comparison operators can produce odd results. For example, it is generally uncontroverisal that ninety-one is less than one hundred twenty. However, the string \"91\" is greater than \"120\" because only the character \"9\" is compared to the character \"1\".8\n\n91 < 120\n\"91\" < \"120\"\n\n[1] TRUE\n[1] FALSE\n\n\nWhen strings are used to represent dates and times, comparison operators may or may not work depending on the precise formatting conversions. Below, we see that “YYYYQQ”-formats sort correctly because the information is hierarchically nested; millenia are compared before centuries, centuries before decades, decades before years, and years before quarters. However, many other string representations of dates, like “QQ-YYYY” will not order correctly. Related topics will be discussed in the “Dates and Times” section.\n\n\"20190Q4\" < \"2020Q3\" # string (alphabetic) ordering same as semantic ordering\n\"Q4-2019\" < \"Q3-2020\" # string and semantic orderings are different\n\n[1] TRUE\n[1] FALSE\n\n\nThese examples demonstrate that we shouldn’t rely on sorting schemes that follow different rules. Before doing comparisons on such types, its a safer bet to cast them to the format most truly representative of their types. If, for some reason, you do wish to keep them as strings, the second example shows that its is wise to format them in the most conducive format possible so things just work.\n\n3.5.3.2 Type coercion\nWe discussed string comparison before when looking at “dirty” strings. More unexpected behavior arises when strings are compared across different data types. Many computing programs will attempt to coerce the objects to a similar and comparable type. Sometimes, this can be convenient as operations “just work”, but as always there is a cost for convenience. As we’ll see, delegating important decisions to our computing engine may not always capture the semantic relationships that we are most interested in.\nFor example, consider compare a string and a number. To make them more comparable, R will convert them both to strings before checking for equality. Thus, the number 2020 is equivalent to the string \"2020\" but not the string \"02020\".\n\n\"2020\" == 2020\n\"02020\" == 2020\n\n[1] TRUE\n[1] FALSE\n\n\nIn contrast, SQLite9 thinks that the string '2020' is greater than the number 2020 and that these two quantities are not equal.\n\nSELECT\n  case when     '2020' =  2020 then 1 else 0 end as is_eq,\n  case when not '2020' == 2020 then 1 else 0 end as not_eq,\n  case when     '2020' <  2020 then 1 else 0 end as is_lt,\n  case when     '2020' >  2020 then 1 else 0 end as is_gt\n\n\n\n  is_eq not_eq is_lt is_gt\n1     0      1     0     1\n\n\n^TODO: where this could cause problems (FIPS example?)\n\n3.5.4 Transformation (TODO)\nbasic things like addition differ by language\nin R, returns error:\n\n'a' + 'b'\n\nError in \"a\" + \"b\": non-numeric argument to binary operator\n\n'a' * 5\n\nError in \"a\" * 5: non-numeric argument to binary operator\n\n\nin SQLite, goes to zero:\n\nSELECT\n  'a' + 'b' as string_add,\n  'a'*5 as string_mult\n\n\n\n  string_add string_mult\n1          0           0\n\n\nin python, does concatenation for + and analogous (concatenation of repeat) for *:\n\n'a' + 'b'\n\n'ab'\n\n'a' * 5\n\n'aaaaa'"
  },
  {
    "objectID": "comp-quan.html#dates-and-times-wip",
    "href": "comp-quan.html#dates-and-times-wip",
    "title": "3  Computational Quandaries (WIP)",
    "section": "\n3.6 Dates and Times (WIP)",
    "text": "3.6 Dates and Times (WIP)\nUnlike character strings, dates and times seem like they should be well defined with distinct, quantifiable components like years, months, and days. However, many different conventions for date formatting and underlying storage formats exist. This leads to similar challenges with dates and times as we saw with strings before.\nSome common formats in the wild are:\n\nYYYYMMDD\nYYYYMM\nMMDDYYYY\nDDMMYYYY\nMM/DD/YYYY\nMM/DD/YY\nDD/MM/YYYY\nYYYY-MM-DD (ISO8601)\n\nIn addition to how dates are formatted, they may be stored in a variety of different ways “under the hood” such as Unix time (seconds since 1970-01-01 00:00:00 UTC) and Julian time (days since noon in Greenwich on November 24, 4714 B.C) (TODO).\nTo complicate matters further, many of these formats may be represented either by native date types in various programs or by more basic data types (such as integers for the first four and strings for the last four). In addition, analogous formats exist for timestamps which encode both calendar date and time of day (hour, minute, and second information).\nTODO: why ISO8601?\n\n3.6.1 Comparison\nAutomatic conversion of data types Dates versus timestamps\n\ndf_dt <-\ndata.frame(\n  DT_ENROLL = as.Date(\"2020-01-01\"),\n  DT_PURCH  = 20200101,\n  DT_LOGIN  = as.POSIXlt(\"2020-01-01T12:00:00\") \n  )\n\n\n\n\nnone of these are equal so nothing returns on filtering\n\nfilter(df_dt, DT_ENROLL == DT_PURCH) %>% nrow()\n\n[1] 0\n\nfilter(df_dt, DT_ENROLL == DT_LOGIN) %>% nrow()\n\nWarning in mask$eval_all_filter(dots, env_filter): Incompatible methods\n(\"Ops.Date\", \"Ops.POSIXt\") for \"==\"\n\n\n[1] 0\n\n\nthe same thing happens in sql\n\nselect * from df_dt where DT_ENROLL = DT_PURCH\n\n\n\n[1] DT_ENROLL DT_PURCH  DT_LOGIN \n<0 rows> (or 0-length row.names)\n\n\n\nselect * from df_dt where DT_ENROLL = DT_LOGIN\n\n\n\n[1] DT_ENROLL DT_PURCH  DT_LOGIN \n<0 rows> (or 0-length row.names)\n\n\nin what way aren’t they equal? to understand this its helpful to know how the computer encodes these dates\nwith as.numeric() in R we can see the numeric representation of the date\n\nas.numeric(df_dt$DT_ENROLL)\n\n[1] 18262\n\n\nthis works the same way in SQL\n\nselect cast(DT_ENROLL as integer), cast(DT_PURCH as integer) from df_dt\n\n\n\n  cast(DT_ENROLL as integer) cast(DT_PURCH as integer)\n1                      18262                  20200101\n\n\nthis has the implication that things that are on the same date have an inequality relationship in both languages\n\nfilter(df_dt, DT_ENROLL < DT_PURCH) %>% nrow()\n\n[1] 1\n\n\n\nselect \n  cast(DT_ENROLL as integer), \n  case when DT_ENROLL < 18000 then 1 else 0 end as lt_18000,\n  case when DT_ENROLL < 19000 then 1 else 0 end as lt_19000,\n  case when DT_ENROLL < DT_PURCH then 1 else 0 end as lt_purch\nfrom df_dt\n\n\n\n  cast(DT_ENROLL as integer) lt_18000 lt_19000 lt_purch\n1                      18262        0        1        1\n\n\nNote this this can affect both filters and joins\nand this similarly causes a more general problem when comparing a date to a date-as-an-integer\n\nas.Date(\"2020-01-01\") > 20160501\n\n[1] FALSE\n\n\n\nselect cast('2020-01-01' as date) > 20160501\n\n\n\n  cast('2020-01-01' as date) > 20160501\n1                                     0"
  },
  {
    "objectID": "comp-quan.html#changing-data-types-todo",
    "href": "comp-quan.html#changing-data-types-todo",
    "title": "3  Computational Quandaries (WIP)",
    "section": "\n3.7 Changing Data Types (TODO)",
    "text": "3.7 Changing Data Types (TODO)\ndata types can incidentally change between programs\n\ndf <- data.frame(DT = as.Date(\"2020-10-01\"))\nprint(\"-- At creation --\")\nsprintf(\"Class of DT variable: %s\", class(df$DT))\ntmp <- tempfile()\nwrite.csv(df, file = tmp, row.names = FALSE)\ndf <- read.csv(file = tmp)\nprint(\"-- After saving and reloading with read.csv --\")\nsprintf(\"Class of DT variable: %s\", class(df$DT))\n\n[1] \"-- At creation --\"\n[1] \"Class of DT variable: Date\"\n[1] \"-- After saving and reloading with read.csv --\"\n[1] \"Class of DT variable: character\"\n\n\nsome tools try to make “smart” guesses based on format\n\nlibrary(readr)\n\ndf <- readr::read_csv(file = tmp)\nprint(\"-- After saving and reloading with readr --\")\nsprintf(\"Class of DT variable: %s\", class(df$DT))\n\n[1] \"-- After saving and reloading with readr --\"\n[1] \"Class of DT variable: Date\"\n\n\nhowever, this can make things slower or unideal (one example: https://github.com/tidyverse/readr/issues/1094#issuecomment-628612430). you can also specify your own types manually"
  },
  {
    "objectID": "comp-quan.html#factors-in-r-todo",
    "href": "comp-quan.html#factors-in-r-todo",
    "title": "3  Computational Quandaries (WIP)",
    "section": "\n3.8 Factors in R (TODO)",
    "text": "3.8 Factors in R (TODO)\n\nfactor(c(0,1,0,1)) == 1\nas.integer(factor(c(0,1,0,1))) == 1\nas.integer(factor(c(0,1,0,1)))\n\n[1] FALSE  TRUE FALSE  TRUE\n[1]  TRUE FALSE  TRUE FALSE\n[1] 1 2 1 2"
  },
  {
    "objectID": "comp-quan.html#programming-errors-todo",
    "href": "comp-quan.html#programming-errors-todo",
    "title": "3  Computational Quandaries (WIP)",
    "section": "\n3.9 Programming Errors (TODO)",
    "text": "3.9 Programming Errors (TODO)\n\n3.9.1 Default Cases (WIP)\nsee case-when example in nulls section\n\n3.9.2 Order of Operations (WIP)\nPEMDAS but sometimes still ambiguous\n\n1 + 1  * 2 / 3 - 1\n(1 + 1) * 2 / 3 - 1\n1 + 1 * 2 / (3 - 1)\n\n[1] 0.6666667\n[1] 0.3333333\n[1] 2\n\n\nSQL clause order of evaluations\n\n3.9.3 Object References (WIP)\nCopying and modifying object overview\n\n\n\n\nDifferent relationships between named variables and their values\n\n\n\n\nWhen might each be preferred?\nWhat risks are there if we don’t understand which we are doing?\nIn Python\n\nx = [1,2,3]\ny = x\ny.append(4)\nprint(y)\nprint(x)\n\n[1, 2, 3, 4]\n[1, 2, 3, 4]\n\n\n\nz = x.copy()\nz.append(5)\nprint(z)\nprint(x)\n\n[1, 2, 3, 4, 5]\n[1, 2, 3, 4]\n\n\npandas DataFrame methods with inplace arg (False is default)\nIn R\n\nlibrary(data.table)\n\nDT <- data.table(a=c(1,2), b=c(11,12))\nprint(DT)\n\nnewDT <- DT        # reference, not copy\nnewDT[1, a := 100] # modify new DT\n\nprint(DT)          # DT is modified too.\n\nDT = data.table(a=c(1,2), b=c(11,12))\nnewDT <- DT        \nnewDT$b[2] <- 200  # new operation\nnewDT[1, a := 100]\n\nprint(DT)\n\n   a  b\n1: 1 11\n2: 2 12\n     a  b\n1: 100 11\n2:   2 12\n   a  b\n1: 1 11\n2: 2 12\n\n\nFrom https://stackoverflow.com/questions/10225098/understanding-exactly-when-a-data-table-is-a-reference-to-vs-a-copy-of-another\n\nimport pandas as pd\n\n# set-up sample data ----\ndata = {'a': [1, 2], \n        'b': [11, 12]}\ndf = pd.DataFrame(data = data)\n\n# rename columns without replacing ----\ndf.rename(columns = {'a':'x'})\n\n   x   b\n0  1  11\n1  2  12\n\ndf\n\n# rename columns with replacing ----\n\n   a   b\n0  1  11\n1  2  12\n\ndf.rename(columns = {'a':'x'}, inplace = True)\ndf\n\n   x   b\n0  1  11\n1  2  12\n\n\n\nadd_ones <- function(data) {\n  \n  data$x0 <- rep(0, nrow(data))\n  \n}\n\ndf <- data.frame(x1 = 1:5)\ndf\n\n  x1\n1  1\n2  2\n3  3\n4  4\n5  5\n\nadd_ones(df)\ndf\n\n  x1\n1  1\n2  2\n3  3\n4  4\n5  5\n\n\n\nadd_ones <- function(data) {\n  \n  data$x0 <- rep(0, nrow(data))\n  return(data)\n  \n}\n\ndf <- data.frame(x1 = 1:5)\ndf\n\n  x1\n1  1\n2  2\n3  3\n4  4\n5  5\n\nadd_ones(df)\n\n  x1 x0\n1  1  0\n2  2  0\n3  3  0\n4  4  0\n5  5  0\n\ndf\n\n  x1\n1  1\n2  2\n3  3\n4  4\n5  5\n\ndf2 <- add_ones(df)\ndf2 \n\n  x1 x0\n1  1  0\n2  2  0\n3  3  0\n4  4  0\n5  5  0"
  },
  {
    "objectID": "comp-quan.html#trusting-tools",
    "href": "comp-quan.html#trusting-tools",
    "title": "3  Computational Quandaries (WIP)",
    "section": "\n3.10 Trusting Tools",
    "text": "3.10 Trusting Tools\n\n3.10.1 Delegating decisions\nA theme throughout this book is the fundamentally social nature of data analysis. Data analysis is fraught without understanding the countless decisions made along the way by those who generated it (whose data is reflected), those who collected it, those who migrated it, and those who have posed questions of it. On one hand, this is a beautiful aspect of analysis; on the other hand, it means that analysts and their analyses are subject to all of the cognitive and social psychological biases of everyday humans.\nOne such bias is “social proof”: assuming that if a tool behaves a certain way, it must be because it is correct.\nAssuming that our tools know best is admittedly an attractive proposition. It appeals to a desire to think that someone, somewhere is “in charge” and, perhaps more critically, helps us avoid a domino effect of distrust (If we don’t trust our tools how can we trust our results? And if we can’t trust our results, how can we trust anything at all?) Unfortunately, there are many reasons are tools might not know best. For example, the tool’s developer might have:\n\nMade a mistake\nHad a different analysis problem in mind with a different optimal approach\nBeen optimizing for a different constraint (e.g. explainability vs. accuracy, speed vs. theoretical properties)\nCome from a community with different norms\nBeen affording users the flexibility to do things many ways even if they don’t agree\nBuilt a certain feature for a different purpose than how you are using it\nNot thought about it at all\n\nAs a few concrete examples from popular open source tools. We’ll look briefly at the prominent python library scikitlearn for machine learning and Apache Spark, an engine for large-scale distributed data processing.\n\n3.10.1.1 Defaults in scikitlearn\n\nscikitlearn’s default behavior for logistic regression modeling10 automatically applies L2 regularization. You might or might not know what this means, and you might or might not want to apply it to your problem. That’s fine. The important thing is that it will change your estimates and predictions, and it is not a part of the classical definition of that algorithm (for modelers coming from a statistical background.)\nOf course, there’s nothing inherently wrong about this choice; the library authors just had different goals than a typical statistical. scikitlearn developer Olivier Grisel explains on Twitter that this choice (and others in the library) is explained because “Scikit-learn was always designed to make it easy to get good predictive accuracy (eg as measured by CV) rather than as statistical inference library.” Additionally, this choice is documented in bold in the function documentation.\nHowever, an analyst could easily miss this nuance if they do not read the documentation. Or, if they misinterpret this choice as social proof that regularization is always the right approach, they might not make the best choice for their own analysis.\n\n3.10.1.2 Algorithms in Spark\n\nAs a second example, according to a 2015 Jira ticket, developers of Spark considered multiple methodologies they could use when adding the functionality to compute feature importance for a random forest. Ultimately, a core contributor advised against permutation importance due to its computational cost.\n\n\n\n\nJIRA ticket for Spark with a discussion of which random forest variable importance algorithm to implement\n\n\n\n\nClearly, no one wants a workflow that is too costly or timely to run. So, once again, there is no right or wrong. However, since every approach to feature importance has its own biases, pitfalls, and challenges in interpretation, it’s a mistake for an end-user to not carefully understand which algorithm is used and why.\n\n3.10.1.3 Null handling in ggplot2 (TODO)\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.0.5\n\ndf <- data.frame(x1 = 1:5, \n                 x2 = c(1, 2, NA, NA, 5),\n                 y1 = 1:5,\n                 y2 = c(1, 2, NA, NA, 5))\nggplot(df, aes(x1, y1)) + geom_line()\n\n\n\nggplot(df, aes(x1, y2)) + geom_line()\n\n\n\nggplot(df, aes(x2, y1)) + geom_line()\n\nWarning: Removed 2 row(s) containing missing values (geom_path).\n\n\n\n\nggplot(df, aes(x2, y2)) + geom_line()\n\nWarning: Removed 2 row(s) containing missing values (geom_path).\n\n\n\n\n\n\n3.10.1.4 Boxplots in ggplot2 (TODO)\ndepending on how you change the scale it also changes the calculations\nhttps://stackoverflow.com/questions/5677885/ignore-outliers-in-ggplot2-boxplot\n\n3.10.2 “Off-Label” Use (TODO)\ncoined in https://www.rstudio.com/resources/rstudioglobal-2021/maintaining-the-house-the-tidyverse-built/\n\n3.10.3 Security (TODO)\nnamespace squatting\nexecutable code"
  },
  {
    "objectID": "comp-quan.html#inefficient-processing-todo",
    "href": "comp-quan.html#inefficient-processing-todo",
    "title": "3  Computational Quandaries (WIP)",
    "section": "\n3.11 Inefficient Processing (TODO)",
    "text": "3.11 Inefficient Processing (TODO)"
  },
  {
    "objectID": "comp-quan.html#strategies-wip",
    "href": "comp-quan.html#strategies-wip",
    "title": "3  Computational Quandaries (WIP)",
    "section": "\n3.12 Strategies (WIP)",
    "text": "3.12 Strategies (WIP)\nParagraph 1 TODO\nSome computational quandaries are inherent to our tools themselves, but often they are a function both of the tools and the ways we chose to use them. More strategies related to writing robust and resilient code will be discussed in Section 11 (Complexify Code).\n\n3.12.1 Understand the intent\n\nread the docs\nlook at examples\ndon’t carry default knowledge between languages\n\n3.12.2 Understand the execution\n\ntest out simple examples (like we’ve been doing)\nspecificlly try out corner cases\n\n3.12.3 Be explicit not implicit\n\ndefault arguments\nexamples above with casting, coalescingW"
  },
  {
    "objectID": "comp-quan.html#real-world-disasters-wip",
    "href": "comp-quan.html#real-world-disasters-wip",
    "title": "3  Computational Quandaries (WIP)",
    "section": "\n3.13 Real World Disasters (WIP)",
    "text": "3.13 Real World Disasters (WIP)\nhttps://www.theguardian.com/politics/2020/oct/05/how-excel-may-have-caused-loss-of-16000-covid-tests-in-england\n\nThe data error, which led to 15,841 positive tests being left off the official daily figures, means than 50,000 potentially infectious people may have been missed by contact tracers and not told to self-isolate.\n\n\n\n\n\n\n\n\n\n\nBarrett, Brian. 2019. “How a ’NULL’ License Plate Landed One Hacker in Ticket Hell.” WIRED, August. https://www.wired.com/story/null-license-plate-landed-one-hacker-ticket-hell/."
  },
  {
    "objectID": "egre-aggr.html",
    "href": "egre-aggr.html",
    "title": "4  Egregious Aggregations (WIP)",
    "section": "",
    "text": "Once armed with an understanding of the data and tools available for analysis, a common start to analysis is exploring data with aggregation. At its heart, any sort of data analysis is the process of condensing raw data into something more manageable and useful while giving up as little of the information as possible. From linear regressions and hypothesis testing to random forests and beyond, much of data analysis could truly be called “applied sums an averages”.\nMany elementary tools for this task are much better at the comprehension task than the preservation one. We learn rigorous assumptions to consider and validate when studying linear regression, but basic arithmetic aggregation presents itself as agnostic and welcome to any type of data. However, the underlying distributions of our variables and the relationships between them have a significant impact on the how informative and interpretable various summarizations are.\nIn this chapter, we will explore different ways that univariate and multivariate aggregations can be naive or uninformative."
  },
  {
    "objectID": "egre-aggr.html#motivating-example-similar-in-summary",
    "href": "egre-aggr.html#motivating-example-similar-in-summary",
    "title": "4  Egregious Aggregations (WIP)",
    "section": "\n4.1 Motivating Example: Similar in Summary",
    "text": "4.1 Motivating Example: Similar in Summary\nTo begin, we will look at a whimsical toy example. This may feel trite or manufactured, but the subsequent sections will aim to convince you that these issues are not just esoteric. Consider the “datasaurus dozen” dataset (Matejka and Fitzmaurice 2017) which is available within the datasauRus R package ?r-datasaurus.\n\nlibrary(datasauRus)\n\nThis dataset contains 12 sets of data stacked on top of one another and identified by the dataset column.1. Besides the identifier column, the data is fairly small and contains only two variables x and y.\n\ndf <- datasauRus::datasaurus_dozen\nhead(df)\n\n  dataset       x       y\n1    dino 55.3846 97.1795\n2    dino 51.5385 96.0256\n3    dino 46.1538 94.4872\n4    dino 42.8205 91.4103\n5    dino 40.7692 88.3333\n6    dino 38.7179 84.8718\n\n\nA quick analysis of summary statistics reveals that each of the 12 datasets is very consistent in its summary statistics. The means and variances of x and y and even their correlations are nearly identifcal.\n\n\n\nSummary statistics for Datasaurus Dozen datasets\n\ndataset\nmean(x)\nmean(y)\nvar(x)\nvar(y)\ncor(x, y)\n\n\n\naway\n54.266\n47.835\n281.227\n725.750\n-0.064\n\n\nbullseye\n54.269\n47.831\n281.207\n725.533\n-0.069\n\n\ncircle\n54.267\n47.838\n280.898\n725.227\n-0.068\n\n\ndino\n54.263\n47.832\n281.070\n725.516\n-0.064\n\n\ndots\n54.260\n47.840\n281.157\n725.235\n-0.060\n\n\nh_lines\n54.261\n47.830\n281.095\n725.757\n-0.062\n\n\nhigh_lines\n54.269\n47.835\n281.122\n725.763\n-0.069\n\n\nslant_down\n54.268\n47.836\n281.124\n725.554\n-0.069\n\n\nslant_up\n54.266\n47.831\n281.194\n725.689\n-0.069\n\n\nstar\n54.267\n47.840\n281.198\n725.240\n-0.063\n\n\nv_lines\n54.270\n47.837\n281.232\n725.639\n-0.069\n\n\nwide_lines\n54.267\n47.832\n281.233\n725.651\n-0.067\n\n\nx_shape\n54.260\n47.840\n281.231\n725.225\n-0.066\n\n\n\n\n\nHowever, as shown in Figure 4.1, when we visualize this data, we find that the 12 datasets reveal remarkably different patterns.\n\n\nWarning: package 'ggplot2' was built under R version 4.0.5\n\n\n\n\nFigure 4.1: Scatterplots for Datasaurus Dozen datasets\n\n\n\n\nThis dataset is a more elaborate version of Anscombe’s Quartet, a well-known set of four datasets which exhibit similar properties. Examining a simiilar plot for Anscombe’s Quartet (with data from the anscombe dataset which ships in R’s datasets package), we can get better intuition for how the phenomenon is manufactured. Figure 4.2 shows as similar plot to Figure 4.1. Comparing datasets 1 and 3, for example, we can see a trade-off between a semi-strong trend with a consistent-seeming amount of noise and an nearly perfect linear trend with a single outlier.\n\n\nWarning: package 'tidyr' was built under R version 4.0.5\n\n\nWarning: Values are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = length` to identify where the duplicates arise\n* Use `values_fn = {summary_fun}` to summarise duplicates\n\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(x, y)`\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\nFigure 4.2: Scatterplots for Anscombe’s Quartet\n\n\n\n\nFigure 4.2 also plots the simple linear regression line for each dataset. Similar to the summary statistics, these are also identical. We know this because the regression coefficient for a simple linear regression is given by cov(x,y)/sd(x)sd(y). You’ll notice I do not write “we can see that…” because, in fact, we can only see similarity not equality. The message of this section may seem to be “don’t summarize your data without plotting it”, but conducting “visual analytics” without looking at the numbers is also problematic. We’ll explore the latter topic more in Chapter -Section 5 (Vexing Visualiztions).\nWhile there are clearly a contrived example (and, if you so chose to check out the “Datasaurus Dozen” paper, a very cleverly contrived example!), its also a cautionary tale. Summary statistics are not just insufficient when they focus on central tendency (e.g. mean) instead of spread. In this example, even an examination of variation and covariation led to an overly simplistic view of the underlying data."
  },
  {
    "objectID": "egre-aggr.html#averages-wip",
    "href": "egre-aggr.html#averages-wip",
    "title": "4  Egregious Aggregations (WIP)",
    "section": "\n4.2 Averages (WIP)",
    "text": "4.2 Averages (WIP)\n\n4.2.1 Implicit assumptions (TODO)\nWhen statistics students study linear regression, they are introduced to a number of canonical assumptions including:\n\nThe true functional form between the dependent and independent variables is linear / additive\nErrors are independent\nErrors have constant variance (that is, they are homoskedastic)\nErrors have a normal distribution\n\nOf course, whether or not these assumptions hold, there’s nothing stopping anyone from mechanically fit at linear regression2. Instead, these assumptions are required to make the output of a linear regression meaningful and, more specifically, for conducting correct inference.\nSimilarly, there are no limitations on mechanically computing an average\n\n4.2.2 Averaging skewed data\nArithmetic average versus colloquial meaning of average as “typical”\nSkewed data\nMultimodal data / mixture models\n\n4.2.3 No “average” observation\nIn the previous section, the average represented a point in the relevant data range even if it was not perhaps the one most representative of a “typical” observation. We discussed how in some situations this quantity may be a reasonable answer to certain types of questions and an aid for certain types of decisions.\nHowever, when we seek an average profile over multiple variables, the problems of averages are further compounded. We may end up with a set of “average” summary statistics that are not representative of any part of our population.\nTo see this, let’s assume we are working with data for a company with a subscription business model. We might be interested in profiling the age of each account (how long they have been a subscriber) and their activity (measured by amount spent on an e-commerce platform, files downloaded on a streaming service, etc.)\nThe following code simulates a set of observations: 80% of accounts are between 0 to 3 years in age and have an average activity level of 100 while 20% of accounts are older than 3 years in age and have an average activity level of 500. (Don’t over-think the specific probability distributions lived here. We are concerned with interrogating the properties of the average and not with simulating a realistic data generating process. Giving ourselves permission to be wrong or “lazy” about unimportant things gives us more energy to focus on what matters.)\n\nset.seed(123)\n\n# define simulation parameters ----\n## n: total observations\n## p: proportion of observations in group 1\nn <- 5000\np <- 0.8\nn1 <- n*p\nn2 <- n*(1-p)\n\n# generate fake dataset with two groups ----\ndf <- \n  data.frame(\n    age = c(runif(n1,   0,  3), runif(n2,   3, 10)),\n    act = c(rnorm(n1, 100, 10), rnorm(n2, 500, 10))\n  )\n\nFigure 4.3 shows a scatterplot of the relationship between account age (x-axis) and activity level (y-axis). Meanwhile, the marginal rug plots shows the univariate distribution of each variable. The sole red dot denotes the coordinates of the average age and average activity. Notably, this dot exists in a region of “zero density”; that is, it is not representative of any customer. Strategic decisions made with this sort of observation in mind as the “typical” might not be destined for success.\n\n\n\n\nFigure 4.3: A scatterplot of two variables and their averages\n\n\n\n\n\n4.2.4 The product of averages\nAs the above example shows, averages of multivariate data can produce poor summaries – particularly when these variables are interrelated3.\nA second implication of this observation is that deriving additional computations based on pre-averaged numbers is likely to obtain inaccurate results.\nFor example, consider that we wish to estimate the average dollar amount of returns per any e-commerce order. Orders may generally be a mixture of low-price orders (around $50 on average) and high-price orders (around $250 on average). Low-price orders may have a 10% probability of being returned while high price orders have a 20% probability. (Again, are these numbers, distributions, or relationships hyper-realistic? Not at all. However, once again we are telling ourselves a story just to reason about numerical properties, so we have to give ourselves permission to not focus on irrelevant details.)\n\nset.seed(123)\n\n# define simulation parameters ----\n## n: observations per group\n## pr[1|2]: mean price per group\nn <- 100\npr1 <- 50\npr2 <- 250\npr_sd <- 5\nre1 <- 0.1\nre2 <- 0.2\n\n# simulate spend amounts and return indicators ----\namt_spend  <- c(rnorm(n, pr1, pr_sd), rnorm(n, pr2, pr_sd))\nind_return <- c(rbinom(n, 1, re1),    rbinom(n, 1, re2))\n\n# compute summary statistics ----\naverage_of_product <- mean(amt_spend * ind_return)\nproduct_of_average <- mean(amt_spend) * mean(ind_return)\n\nThe true average amount returned across all of our orders is 36.0438333 (from the average_of_product variable). However, if instead we already knew an average spend amount and an average return proportion, we might be inclined to compute the product_of_average method which returns a value of 26.9922866. (This is a difference of 9.05 relative to an average purchase amount of 150.)\nAt first, this may seem unintuitive until we write out the formulas and realize that these metrics are, in fact, two very different quantities:\n\\[\n  \\frac{ \\sum_{1}^{n} Spend }{\\sum_{1}^{n} 1} \\ * \\frac{ \\sum_{1}^{n} I(Return) }{\\sum_{1}^{n} 1}\n\\] over all \\(n\\) orders\nversus\n\\[\n  \\frac{\\sum_{1}^{n} Spend * I(Return)}{\\sum_{1}^{n} 1}\n\\]\nIf this still feels counterintuitive, we can see how much of the difference is accounted for by the interrelation between our two variables. In the following code, we break the relationship between the variables by randomly reordering the ind_return variable so it is no longer has any true relationship to the corresponding amt_spend variable.\n\n# randomly reorder one of two variables to break relationships ----\nind_return <- sample(ind_return, size = 200)\n\n# recompute variables ----\naverage_of_product <- mean(amt_spend * ind_return)\nproduct_of_average <- mean(amt_spend) * mean(ind_return)\n\nAfter redoing the calculations, we find that th two values are much closer. average_of_product is now 24.1041313 and product_of_average is now 26.9922866. These are notably still not the same number so that does not mean that these two equations are equivalent if variables are unrelated; however, this second result once again illustrates the extent to which interrelations can defy our naive intuitions.\n\n4.2.5 Average over what? (TODO)\nno such thing as an unweighted average (just sometimes weights are equal)\nformal definition of expected value forces you to pick a probability distribution\neg avg mpg by time vs by mileage?\nnot strictly an error but our language allows an ill-defined problem\n\n4.2.6 Dichotomization and distributions\n\nn <- 1000\n\n# simulate x and y: uniformly distributed x ----\nx1 <- runif(n)\ny1 <- 5 + 3*x1 + rnorm(n)\n\n# simulate x and y: same relationship, more concentrated distribution of x ----\nx2 <- c( runif(n*0.1, 0.00, 0.44), \n         runif(n*0.8, 0.45, 0.55), \n         runif(n*0.1, 0.55, 1.00) \n        )\ny2 <- 5 + 3*x2 + rnorm(n)\n\n# com\ng1 <- ifelse(x1 < 0.5, 0, 1)\nmeans1 <- c(mean(y1[g1 == 0]), mean(y1[g1 == 1]))\nmeans1 \n\n[1] 5.813698 7.254438\n\ng2 <- ifelse(x2 < 0.5, 0, 1)\nmeans2 <- c(mean(y2[g2 == 0]), mean(y2[g2 == 1]))\nmeans2\n\n[1] 6.218347 6.762943\n\nmeans1\n\n[1] 5.813698 7.254438\n\nmeans2\n\n[1] 6.218347 6.762943\n\ncor(x1, y1)\n\n[1] 0.6424276\n\ncor(x2, y2)\n\n[1] 0.3687667\n\n\n\n4.2.7 Small sample sizes"
  },
  {
    "objectID": "egre-aggr.html#proportions-wip",
    "href": "egre-aggr.html#proportions-wip",
    "title": "4  Egregious Aggregations (WIP)",
    "section": "\n4.3 Proportions (WIP)",
    "text": "4.3 Proportions (WIP)\nnote that these are of course just a type of average (average of indicators) but helpful to examine challenges separately\n\n4.3.1 Picking the right denominator\n\n4.3.2 Sample size effects"
  },
  {
    "objectID": "egre-aggr.html#variation-todo",
    "href": "egre-aggr.html#variation-todo",
    "title": "4  Egregious Aggregations (WIP)",
    "section": "\n4.4 Variation (TODO)",
    "text": "4.4 Variation (TODO)\n\nx <- c(-20, -10, -5, 0, 5, 10, 20)\nmean(x)\nvar(x)\n\n[1] 0\n[1] 175\n\n\n\nx <- c(-15, -15, -5, 0, 5, 15, 15)\nmean(x)\nvar(x)\n\n[1] 0\n[1] 158.3333"
  },
  {
    "objectID": "egre-aggr.html#correlation-wip",
    "href": "egre-aggr.html#correlation-wip",
    "title": "4  Egregious Aggregations (WIP)",
    "section": "\n4.5 Correlation (WIP)",
    "text": "4.5 Correlation (WIP)\nAs shown in Figure 4.4,\n\n\n\n\nFigure 4.4: Plots of x from 1 to 10 over a range of common functions\n\n\n\n\n\n4.5.1 Linear relationships only\n\nx <- 1:10\ny <- list(linear = x, quadratic = x**2, cubic = x**3, quartic = x**4)\n\nvapply(y, FUN = function(z) cor(x, z, method = \"pearson\"), FUN.VALUE = numeric(1))\n\n   linear quadratic     cubic   quartic \n1.0000000 0.9745586 0.9283912 0.8816779 \n\n\n\n4.5.2 Multiple forms\nTraditional (Pearson) correlation depends on specific values whereas Spearman and Kendall focus on order statistics\n\n# polynomials ----\nx <- 1:10\ny <- list(linear = x, quadratic = x**2, cubic = x**3, quartic = x**4)\n\nvapply(y, FUN = function(z) cor(x, z, method = \"pearson\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"spearman\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"kendall\"), FUN.VALUE = numeric(1))\n\n   linear quadratic     cubic   quartic \n1.0000000 0.9745586 0.9283912 0.8816779 \n   linear quadratic     cubic   quartic \n        1         1         1         1 \n   linear quadratic     cubic   quartic \n        1         1         1         1 \n\n\nSimilar results with a different set of functions\n\n# other functional forms ----\nx <- 1:10\ny <- list(sin(x), sqrt(x), exp(x), log(x))\n\nvapply(y, FUN = function(z) cor(x, z, method = \"pearson\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"spearman\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"kendall\"), FUN.VALUE = numeric(1))\n\n[1] -0.1705046  0.9891838  0.7168704  0.9516624\n[1] -0.1393939  1.0000000  1.0000000  1.0000000\n[1] -0.1111111  1.0000000  1.0000000  1.0000000\n\n\n\n4.5.3 Sensitivity to domain\nThe “strength of relationship” (completely deterministic) is the same in both cases\nHowever, the summarization of the relationships changes\nHere’s same case as before:\n\n# polynomials ----\nx <- 1:10\ny <- list(linear = x, quadratic = x**2, cubic = x**3, quartic = x**4)\n\nvapply(y, FUN = function(z) cor(x, z, method = \"pearson\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"spearman\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"kendall\"), FUN.VALUE = numeric(1))\n\n   linear quadratic     cubic   quartic \n1.0000000 0.9745586 0.9283912 0.8816779 \n   linear quadratic     cubic   quartic \n        1         1         1         1 \n   linear quadratic     cubic   quartic \n        1         1         1         1 \n\n\nAnd here’s a different range:\n\n# polynomials, diff range ----\nx <- -10:10\ny <- list(linear = x, quadratic = x**2, cubic = x**3, quartic = x**4)\n\nvapply(y, FUN = function(z) cor(x, z, method = \"pearson\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"spearman\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"kendall\"), FUN.VALUE = numeric(1))\n\n   linear quadratic     cubic   quartic \n1.0000000 0.0000000 0.9179069 0.0000000 \n   linear quadratic     cubic   quartic \n        1         0         1         0 \n   linear quadratic     cubic   quartic \n        1         0         1         0 \n\n\n\n4.5.4 Partial correlation\nA lot of EDA starts with some sort of correlation matrix\nThis won’t always account for the fact that some variables can mask correlation between others\nConsider two groups with trends in different directions\n\nn_obsvs <- 10\nn_group <-  2\n\ngroup <- rep(1:n_group, each = n_obsvs)\nvar1  <- rep(1:n_obsvs, times = n_group)\nvar2  <- var1 * rep(c(5, -5), each = n_obsvs)\nvar3  <- var1 * rep(c(1,  5), each = n_obsvs)\n\nAs Figure 4.5 shows\n\n\n\n\nFigure 4.5: Subgroups demonstrating opposing linear relationships\n\n\n\n\nBecause of the opposing trends, their correlation becomes zero\n\ncor(var1, var2, method = \"pearson\")\ncor(var1, var2, method = \"spearman\")\ncor(var1, var2, method = \"kendall\")\n\n[1] 0\n[1] 0\n[1] 0\n\n\nHowever, by group the correlation is 1\n\ncor(var1[group == 1], var2[group == 1])\ncor(var1[group == 2], var2[group == 2])\n\n[1] 1\n[1] -1\n\n\nA similar thing happens when the relationship has the same sign but different slopes\n\ncor(var1, var3, method = \"pearson\")\n\n[1] 0.5703518\n\n\nwhile the correlation is still one within group\n\ncor(var1[group == 1], var3[group == 1])\ncor(var1[group == 2], var3[group == 2])\n\n[1] 1\n[1] 1\n\n\nEven partial correlation doesn’t help in case of opposing signs\n\nlibrary(ppcor)\n\nWarning: package 'ppcor' was built under R version 4.0.5\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\npcor(data.frame(var1, var2, group))$estimate\n\n      var1       var2      group\nvar1     1  0.0000000  0.0000000\nvar2     0  1.0000000 -0.8864053\ngroup    0 -0.8864053  1.0000000\n\n\nIt improves the strength of the estimated correlation in the case of var3 but still failes to estimate it correctly. Partial correlation would be assuming a form like var3 ~ var1 + group and not var3 ~ var1 * group\n\npcor(data.frame(var1, var3, group))$estimate\n\n            var1      var3      group\nvar1   1.0000000 0.8320503 -0.7375338\nvar3   0.8320503 1.0000000  0.8864053\ngroup -0.7375338 0.8864053  1.0000000"
  },
  {
    "objectID": "egre-aggr.html#trends",
    "href": "egre-aggr.html#trends",
    "title": "4  Egregious Aggregations (WIP)",
    "section": "\n4.6 Trends",
    "text": "4.6 Trends\n\n4.6.1 “If trends continue…”\nFigure 4.6 shows that…\n\nlibrary(ggplot2)\n\nn <- 300\nx <- runif(n)\ny <- x + rnorm(n)\n\nggplot(\n  data.frame(x, y),\n  aes(x, y)\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ I(x**1), se = FALSE, fullrange = TRUE) +\n  geom_smooth(method = \"lm\", formula = y ~ I(x**2), se = FALSE, fullrange = TRUE) +\n  geom_smooth(method = \"lm\", formula = y ~ I(x**3), se = FALSE, fullrange = TRUE) +\n  scale_x_continuous(limit = c(0,2))\n\n\n\nFigure 4.6: Extrapolated linear, quadratic, and cubic fits of data\n\n\n\n\n\n4.6.2 Seasonality\n\nseas_factor <- c(0.7, 0.8, 0.8,\n                 0.9, 1.0, 1.1,\n                 1.1, 1.1, 1.2,\n                 1.2, 1.5, 1.8\n                 )\nbase <- 1000\nn_visits <- base * seas_factor\n\nmean(df$n_visits[1:12])\n\nWarning in mean.default(df$n_visits[1:12]): argument is not numeric or logical:\nreturning NA\n\nmean(df$n_visits[8:12])\n\nWarning in mean.default(df$n_visits[8:12]): argument is not numeric or logical:\nreturning NA\n\nmean(df$n_visits[c(8:12, 1:12)])\n\nWarning in mean.default(df$n_visits[c(8:12, 1:12)]): argument is not numeric or\nlogical: returning NA\n\n\n[1] NA\n[1] NA\n[1] NA"
  },
  {
    "objectID": "egre-aggr.html#comparisons-todo",
    "href": "egre-aggr.html#comparisons-todo",
    "title": "4  Egregious Aggregations (WIP)",
    "section": "\n4.7 Comparisons (TODO)",
    "text": "4.7 Comparisons (TODO)\n\n4.7.1 Percents versus percentage points\n\n\n\n\n4.7.2 Changes with small bases\n\n(0.015 - 0.010) / 0.010\n(0.805 - 0.800) / 0.800\n\n[1] 0.5\n[1] 0.00625\n\n\n\n0.015 / 0.010\n0.805 / 0.800\n\n[1] 1.5\n[1] 1.00625"
  },
  {
    "objectID": "egre-aggr.html#outliers-todo",
    "href": "egre-aggr.html#outliers-todo",
    "title": "4  Egregious Aggregations (WIP)",
    "section": "\n4.8 Outliers (TODO)",
    "text": "4.8 Outliers (TODO)"
  },
  {
    "objectID": "egre-aggr.html#strategies-todo",
    "href": "egre-aggr.html#strategies-todo",
    "title": "4  Egregious Aggregations (WIP)",
    "section": "\n4.9 Strategies (TODO)",
    "text": "4.9 Strategies (TODO)"
  },
  {
    "objectID": "egre-aggr.html#real-world-disasters-todo",
    "href": "egre-aggr.html#real-world-disasters-todo",
    "title": "4  Egregious Aggregations (WIP)",
    "section": "\n4.10 Real World Disasters (TODO)",
    "text": "4.10 Real World Disasters (TODO)\nStraight vs weighted averages in COVID positivity rates (May 2020)\n\nThe changes could result in real-world differences for Hoosiers, because the state uses a county’s positivity rate as one of the numbers to determine which restrictions that county will face. Those restrictions determine how many people may gather, among other items.\n\n\nSome Hoosiers may see loosened restrictions because of the changes. While Box said the county-level impact will be mixed, she predicted some smaller counties will see a decline in positivity rate after the changes.\n\n\n“The change to the methodology is how we calculate the seven-day positivity rate for counties. In the past, similar to many states, we’ve added each day’s positivity rate for seven days and divided by seven to obtain the week’s positivity rate. Now we will add all of the positive tests for the week and divide by the total tests done that week to determine the week’s positivity rate. This will help to minimize the effect that a high variability in the number of tests done each day can have on the week’s overall positivity, especially for our smaller counties.”\n\nthree issues here\nfirst straight versus weighted averages\n\navg_of_ratios <- (10/100 + 90/100) / 2\n\nratio_of_sums <- (10 + 90) / (100 + 100)\n\navg_of_ratios == ratio_of_sums\n\navg_of_ratios_uneq <- (10/100 + 180 / 200) / 2\n\nratio_of_sums_uneq <- (10 + 180) / (100 + 200)\n\navg_of_ratios_uneq == ratio_of_sums_uneq\n\nweightavg_of_ratios_uneq <- (100/300)*(10/100) + (200/300)*(180/200)\n\nratio_of_sums_uneq == weightavg_of_ratios_uneq\n\n[1] TRUE\n[1] FALSE\n[1] TRUE\n\n\nthen back to the data for why it matters.\nif data is from same distribution, this could increase variance but shouldn’t effect mean\nRecall that the standard deviation of sample proportion is \\(\\sqrt(p*(1-p)/n)\\)\nlink to discussions of sample size and different types of averages\n\nset.seed(123)\n\n# define simulation parameters ----\n## n: total draws from binomial distribution\n## p: proportion of successes\np <- 0.5\nn <- 1000\n\n# sample from binomials of different size ----\ns010 <- rbinom(n,  10, p) /  10\ns100 <- rbinom(n, 100, p) / 100\ns500 <- rbinom(n, 500, p) / 500\n\n# set results as dataframe for inspection ----\ndf <- data.frame(\n  s = rep(c(10, 100, 500), each = n),\n  x = c(s010, s100, s500)\n)\n\n\nlibrary(ggplot2)\n\nggplot(data = df) +\n  aes(x = x, col = as.character(s)) +\n  geom_density() +\n  geom_vline(xintercept = p, col = 'darkgrey', linetype = 2) +\n  labs(\n    title = \"Sampling Distribution for p = 0.5\",\n    col = \"Sample Size\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = \"bottom\"\n  )\n\n\n\n\nbut low sample days based on real world are probably also a sign of a different distribution (only very urgent cases get tested?)\n\n\n\n\n\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics Through Simulated Annealing.” In. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3025453.3025912.\n\n\nMay, Ethan. 2020. “How an Error in the Calculation of Indiana’s Positivity Rate May Affect You.” Indianapolis Star, December. https://www.indystar.com/story/news/health/2020/12/23/covid-indiana-positivity-rate-error-corrected-dec-30/4013741001/."
  },
  {
    "objectID": "incr-infe.html",
    "href": "incr-infe.html",
    "title": "6  Incredible Inferences (TODO)",
    "section": "",
    "text": "Previously, we have seen how different inputs like data, tools, and methods can add risks to our data analysis. However, the battle is not won simply when we get our first set of outputs. In this chapter, we will explore common errors in interpreting the results of our analysis by exploring aspects of bias, missingness, and confounding."
  },
  {
    "objectID": "incr-infe.html#common-biases",
    "href": "incr-infe.html#common-biases",
    "title": "6  Incredible Inferences (TODO)",
    "section": "\n6.1 Common Biases",
    "text": "6.1 Common Biases"
  },
  {
    "objectID": "incr-infe.html#policy-induced-relationships",
    "href": "incr-infe.html#policy-induced-relationships",
    "title": "6  Incredible Inferences (TODO)",
    "section": "\n6.2 Policy-induced relationships",
    "text": "6.2 Policy-induced relationships\n\nset.seed(123)\n\nn <- 1000\nx1 <- runif(n)\nx2 <- runif(n)\ny <- x1 + x2 > 1\ndf <- data.frame(x1, x2, y)\n\nwith(df, cor(x1, x2))\nwith(df[df$y,], cor(x1, x2))\n\n[1] -0.05927849\n[1] -0.5003105\n\n\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.0.5\n\nggplot(df) +\n  aes(x = x1, y = x2, col = y) +\n  geom_point()"
  },
  {
    "objectID": "incr-infe.html#feature-leakage",
    "href": "incr-infe.html#feature-leakage",
    "title": "6  Incredible Inferences (TODO)",
    "section": "\n6.3 Feature leakage",
    "text": "6.3 Feature leakage\n\nn <- 1000\nminutes_month1 <- runif(n, 60, 1200)\nminutes_month2 <- runif(n, 60, 1200) \nminutes_tot <- minutes_month1 + minutes_month2\ndf <- data.frame(minutes_month1, minutes_month2, minutes_tot)\n\n\n\n\nFigure 6.1 shows…\n\n\n\n\nFigure 6.1: Correlation of independent versus cumulative quantities"
  },
  {
    "objectID": "incr-infe.html#diligent-data-dredging",
    "href": "incr-infe.html#diligent-data-dredging",
    "title": "6  Incredible Inferences (TODO)",
    "section": "\n6.4 “Diligent” data dredging",
    "text": "6.4 “Diligent” data dredging\n\nset.seed(123)\n\nn <- 1000\nx <- rnorm(n)\n\nrandom_test <- function(x) {\n  \n  indices <- sample(1:length(x), length(x)/2, replace = FALSE)\n  group1 <- x[indices]\n  group2 <- x[-indices]\n  tt <- t.test(group1, group2)\n  return(tt$p.value)\n  \n}\n\np <- vapply(1:10000, FUN = function(...) {random_test(x)}, FUN.VALUE = numeric(1))\nsum(p < 0.05)\n\n[1] 500\n\n\n\nn_obsv <- 1000\nn_vars <- 100\nmat_cat <- matrix(\n  data = rbinom(n_obsv * n_vars, 1, 0.5),\n  nrow = n_obsv,\n  ncol = n_vars\n  )\nmat_all <- cbind(x, mat_cat)\ndf <- as.data.frame(mat_all)\nnames(df) <- c(\"x\", paste0(\"v\", 1:n_vars))\nhead(df)\n\n            x v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 v11 v12 v13 v14 v15 v16 v17 v18\n1 -0.56047565  1  1  0  1  1  0  1  0  0   1   1   0   1   1   0   1   0   0\n2 -0.23017749  1  1  0  0  0  0  0  1  1   1   1   0   0   1   1   1   1   1\n3  1.55870831  1  0  0  0  1  0  1  1  0   1   0   0   0   1   1   1   1   1\n4  0.07050839  1  0  1  1  0  0  1  0  1   1   1   0   0   0   0   0   1   0\n5  0.12928774  1  1  1  0  0  0  0  0  0   1   1   1   1   0   1   1   0   0\n6  1.71506499  1  0  0  1  1  0  1  1  1   1   1   0   1   1   0   0   0   0\n  v19 v20 v21 v22 v23 v24 v25 v26 v27 v28 v29 v30 v31 v32 v33 v34 v35 v36 v37\n1   0   1   0   0   1   1   1   0   0   0   0   1   0   0   1   0   0   0   1\n2   0   0   1   0   1   0   1   0   1   0   0   1   1   0   1   0   0   0   1\n3   0   0   1   0   1   1   0   1   0   0   0   1   0   1   0   0   1   1   1\n4   0   0   1   1   0   1   1   0   0   0   0   1   1   1   1   0   0   1   0\n5   0   1   1   0   0   0   1   0   1   0   0   1   0   0   0   0   1   0   1\n6   1   1   0   0   0   1   1   0   0   1   1   0   1   1   0   0   0   1   1\n  v38 v39 v40 v41 v42 v43 v44 v45 v46 v47 v48 v49 v50 v51 v52 v53 v54 v55 v56\n1   1   1   0   0   0   1   1   1   0   1   0   0   1   1   0   0   0   0   1\n2   0   0   0   0   0   0   0   0   0   1   0   1   0   1   1   0   0   1   0\n3   1   0   0   0   1   0   1   1   0   0   1   0   1   1   1   1   1   1   0\n4   0   0   0   1   0   1   0   0   0   1   0   0   0   0   1   1   1   0   0\n5   0   1   0   0   1   0   1   0   1   1   1   1   0   1   0   0   0   1   1\n6   0   1   1   1   1   1   0   0   0   1   0   0   1   0   0   1   1   0   0\n  v57 v58 v59 v60 v61 v62 v63 v64 v65 v66 v67 v68 v69 v70 v71 v72 v73 v74 v75\n1   0   1   1   0   0   1   0   0   0   1   1   1   0   0   0   0   1   0   0\n2   1   0   1   1   0   1   0   1   1   1   1   0   1   1   0   1   1   1   1\n3   0   1   0   1   0   1   1   1   1   1   0   1   0   1   1   0   1   1   0\n4   0   0   1   0   1   1   1   1   1   0   1   0   0   0   1   0   1   0   0\n5   1   0   0   0   0   0   1   0   1   0   0   0   1   0   1   1   1   0   1\n6   1   0   1   1   0   1   1   0   1   0   1   0   1   0   0   0   1   1   0\n  v76 v77 v78 v79 v80 v81 v82 v83 v84 v85 v86 v87 v88 v89 v90 v91 v92 v93 v94\n1   1   0   0   1   0   1   0   0   0   0   1   1   1   0   0   1   1   1   0\n2   1   1   1   1   1   0   1   1   1   0   0   1   1   0   0   0   0   1   0\n3   0   0   0   1   0   0   1   0   0   0   0   1   0   1   0   0   1   0   0\n4   0   0   0   0   0   0   0   1   0   1   1   0   0   0   0   0   0   0   1\n5   0   0   0   1   1   1   0   0   1   1   0   0   1   1   0   1   0   1   1\n6   1   0   0   1   0   1   1   0   0   0   0   1   1   0   1   0   0   0   0\n  v95 v96 v97 v98 v99 v100\n1   0   0   1   1   0    0\n2   1   1   0   1   0    1\n3   0   1   0   1   0    1\n4   1   0   0   1   1    1\n5   1   1   1   0   1    0\n6   0   1   1   0   0    1\n\n\n\nt.test(x ~ v1, data = df)$p.value\nt.test(x ~ v2, data = df)$p.value\nt.test(x ~ v3, data = df)$p.value\nt.test(x ~ v4, data = df)$p.value\n# etc.\n\n[1] 0.09770958\n[1] 0.8733535\n[1] 0.02182194\n[1] 0.1525164\n\n\nSuccess! ..Or success?\nsample splitting with “train”\n(obviously a very ugly way to do this, but that’s the point)\n\nt.test(x ~ v1, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v2, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v3, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v4, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v5, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v6, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v7, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v8, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v9, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v10, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v11, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v12, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v13, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v14, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v15, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v16, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v17, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v18, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v19, data = df[1:(n_obsv/2),])$p.value\n\n[1] 0.6022476\n[1] 0.4946592\n[1] 0.1959636\n[1] 0.368161\n[1] 0.2115401\n[1] 0.7112932\n[1] 0.3126737\n[1] 0.8141703\n[1] 0.9032519\n[1] 0.8211763\n[1] 0.4415702\n[1] 0.2564228\n[1] 0.5292002\n[1] 0.1714863\n[1] 0.08549674\n[1] 0.2284842\n[1] 0.6277407\n[1] 0.01318489\n[1] 0.2556232\n\n\nand “test”\n\nt.test(x ~ v18, data = df[(n_obsv/2 + 1):n_obsv,])$p.value\n\n[1] 0.1691076"
  },
  {
    "objectID": "incr-infe.html#superficial-stories",
    "href": "incr-infe.html#superficial-stories",
    "title": "6  Incredible Inferences (TODO)",
    "section": "\n6.5 Superficial stories",
    "text": "6.5 Superficial stories\n\n6.5.1 Regression to the mean\nsimulate truly independent spend amounts across two periods\n\nset.seed(123)\n\nn  <- 1000\nmu <- 100\nsd <- 10\nspend1 <- rnorm(n, mu, sd)\nspend2 <- rnorm(n, mu, sd)\n\ndf <- data.frame(spend1, spend2)\n\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.0.5\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndf %>% \n  group_by(spend1 > mu) %>%\n  summarize_at(vars(starts_with(\"spend\")), mean) %>%\n  mutate(pct_change = round((spend2 - spend1) / spend1, 3))\n\n# A tibble: 2 x 4\n  `spend1 > mu` spend1 spend2 pct_change\n  <lgl>          <dbl>  <dbl>      <dbl>\n1 FALSE           92.2   99.7      0.081\n2 TRUE           108.   101.      -0.063\n\n\n\ndf %>%\n  mutate(spend1_bin = cut(spend1, 5)) %>%\n  group_by(spend1_bin) %>%\n  summarize_at(vars(starts_with(\"spend\")), mean) %>%\n  mutate(pct_change = round((spend2 - spend1) / spend1, 3))\n\n# A tibble: 5 x 4\n  spend1_bin spend1 spend2 pct_change\n  <fct>       <dbl>  <dbl>      <dbl>\n1 (71.8,84]    80.5   97.8      0.215\n2 (84,96.1]    91.1  100.       0.098\n3 (96.1,108]  102.   101.      -0.012\n4 (108,120]   113.   101.      -0.101\n5 (120,132]   124.   103.      -0.167\n\n\n\ndf %>%\n  mutate(spend1_bin = cut(spend1, 5)) %>%\n  group_by(spend1_bin) %>%\n  summarize(corr = cor(spend1, spend2))\n\n# A tibble: 5 x 2\n  spend1_bin    corr\n  <fct>        <dbl>\n1 (71.8,84]   0.281 \n2 (84,96.1]  -0.0149\n3 (96.1,108]  0.0438\n4 (108,120]   0.101 \n5 (120,132]  -0.165 \n\n\n\nmean(spend1 > spend2)\nmean(spend1 < spend2)\n\n[1] 0.49\n[1] 0.51\n\n\n\nsum((spend1 > mu) * (spend1 > spend2)) / sum(spend1 > mu)\nsum((spend1 < mu) * (spend1 < spend2)) / sum(spend1 > mu)\n\n[1] 0.7168317\n[1] 0.7267327\n\n\n\nlibrary(ggplot2)\n\nggplot(df) +\n  aes(x = spend1, y = spend2) + \n  geom_point()\n\n\n\n\n\n6.5.2 Distribution shifts\n\n\n\nFigure 6.2 shows that…\n\n\n\n\nFigure 6.2: Trends within and between customer behavioral groups\n\n\n\n\nFigure 6.3 shows that…\n\n\nWarning: Removed 1 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 3 row(s) containing missing values (geom_path).\n\n\n\n\nFigure 6.3: Possible subgroup trends contributing to aggregate trend\n\n\n\n\nThe code used to generate this mock dataset is shown below.\n\nhi_engagement <- 10\nlo_engagement <- 2\npr_engagement <- 0.85^(0:24)\navg_engagement <- 10*pr_engagement + 2*(1-pr_engagement)\n\ndf <- \n  data.frame(\n    t = 1:length(avg_engagement), \n    avg_engagement, \n    hi_engagement, \n    lo_engagement\n    )"
  },
  {
    "objectID": "incr-infe.html#tricky-timing-issues-wip",
    "href": "incr-infe.html#tricky-timing-issues-wip",
    "title": "6  Incredible Inferences (TODO)",
    "section": "\n6.6 Tricky timing issues (WIP)",
    "text": "6.6 Tricky timing issues (WIP)\n\n6.6.1 Censored data\nSuppose we are wondering how long our subscription customers will stay put\nmean lifetime of customers in 24 and uses exponential distrib (see appendix on distribs)\nwe are analyzing a cohort of customers 18 months after they first subscribed\n\n# time-to-event censored ----\nset.seed(123)\nn <- 1000\ncurr_time <- 18\nmean_lifetime <- 24\n\nlifetime <- rexp(n, rate = 1 / mean_lifetime)\nmean(lifetime)\n\n[1] 24.7195\n\n\nBecause we are only 18 months in, we cannot observe the lifetimes of all customers\nfor those that left before 18 months we have complete data\nbut for those who left after 18 months we only know their lifetime exceeds 18 months.\nThus, if we look at the mean only where we can observe it, it’s biased towards lower lifetimes. (Recall that we know what the correct value is)\n\n#> observed ----\nlifetime_observed <- lifetime\nlifetime_observed[lifetime > curr_time] <- NA\nmean(lifetime_observed, na.rm = TRUE)\n\n[1] 7.918621\n\n\nOf course, we do know more than nothing (null) about the “surviving customers”. We know that their lifetime is at least as large as the current time. So alternatively, we could use the current time in our calculations. This makes for a slightly less biased estimate, but it is still wrong and guaranteed to underestimate the actual average.\n\n#> max ----\nlifetime_max <- pmin(lifetime, curr_time)\nmean(lifetime_max)\n\n[1] 12.87866\n\n\nThis scenario illustrates the concept of censored data. Figure 6.4 illustrates the fundamental problem more clearly.\n\n\n\n\nFigure 6.4: A sample of observations of customer lifetimes showing observed and censored data\n\n\n\n\nSo what can we do instead? A common approach is to examine quantiles (such as the median) which can make more full use of the data we have. Since we know that rank of our observations (that is, that the censored observations are all larger than the observed datapoints), we can reliable calculate the p-th quantile so long as p percent of the data is not censored.\n\n#> quantile ----\nsum(!is.na(lifetime_observed)) / n\n\n[1] 0.508\n\nlifetime_quantile <- lifetime_observed\nlifetime_observed[is.na(lifetime_observed)] <- 100*curr_time\nquantile(lifetime_observed, p = c(0.5))\n\n   50% \n17.548 \n\n\n\n6.6.2 Immortal time bias\n\nrollout_time <- 12\nused_feature <- (lifetime > rollout_time) * rbinom(n, size = 1, prob = 0.5)\naggregate(lifetime, by = list(used_feature), FUN = mean)\n\n  Group.1        x\n1       0 19.51319\n2       1 36.80991"
  },
  {
    "objectID": "incr-infe.html#section",
    "href": "incr-infe.html#section",
    "title": "6  Incredible Inferences (TODO)",
    "section": "\n6.7 ",
    "text": "6.7"
  },
  {
    "objectID": "cava-caus.html",
    "href": "cava-caus.html",
    "title": "7  Cavalier Causality (TODO)",
    "section": "",
    "text": "In Section 6 (Incredible Inferences), we began to see that we can be tricked by biases when we lack causal thinking and an underlying theory for the data generating process. In this chapter, we will revisit some of these same disasters and introduce some specific frameworks to help us more rigorously explore our analysis for errors and biases and, even better, strategize the best ways to fix them."
  },
  {
    "objectID": "mind-mode.html",
    "href": "mind-mode.html",
    "title": "8  Mindless Modeling (TODO)",
    "section": "",
    "text": "Excellent reproducibility paper: https://reproducible.cs.princeton.edu/ feature leakage, model sheets"
  },
  {
    "objectID": "mind-mode.html#features",
    "href": "mind-mode.html#features",
    "title": "8  Mindless Modeling (TODO)",
    "section": "\n8.1 Features",
    "text": "8.1 Features\nRatio Variables: https://journals.sagepub.com/doi/abs/10.1177/1094428118773455?journalCode=orma\n\n8.1.1 Engineering the wrong features"
  },
  {
    "objectID": "mind-mode.html#targets",
    "href": "mind-mode.html#targets",
    "title": "8  Mindless Modeling (TODO)",
    "section": "\n8.2 Targets",
    "text": "8.2 Targets"
  },
  {
    "objectID": "mind-mode.html#evaluation-metrics",
    "href": "mind-mode.html#evaluation-metrics",
    "title": "8  Mindless Modeling (TODO)",
    "section": "\n8.3 Evaluation Metrics",
    "text": "8.3 Evaluation Metrics"
  },
  {
    "objectID": "mind-mode.html#unsupervised-learning",
    "href": "mind-mode.html#unsupervised-learning",
    "title": "8  Mindless Modeling (TODO)",
    "section": "\n8.4 Unsupervised Learning",
    "text": "8.4 Unsupervised Learning"
  },
  {
    "objectID": "mind-mode.html#lifecycle-management",
    "href": "mind-mode.html#lifecycle-management",
    "title": "8  Mindless Modeling (TODO)",
    "section": "\n8.5 Lifecycle Management",
    "text": "8.5 Lifecycle Management"
  },
  {
    "objectID": "mind-mode.html#fair-and-ethical-modeling",
    "href": "mind-mode.html#fair-and-ethical-modeling",
    "title": "8  Mindless Modeling (TODO)",
    "section": "\n8.6 Fair and Ethical Modeling",
    "text": "8.6 Fair and Ethical Modeling"
  },
  {
    "objectID": "alte-algo.html",
    "href": "alte-algo.html",
    "title": "9  Alternative Algorithms (TODO)",
    "section": "",
    "text": "As the consummate showman, P.T. Barnum is often quoted as saying “Leave them wanting more”. Unfortunately, statistics professors have less of a flare for drama. Introductory statistics courses will typically introduce a few types of models (for example, linear and perhaps logistic regression), and that’s a wrap. It’s often until students start taking the subsequent courses that they are exposed to the true limitations of previous techniques and taught to demand more.\nThis chapter attempts to flip that paradigm by briefly surveying a broad number of modeling techniques. The goal is not to go into all of the rigorous deals that one should understand to use these models. Instead, we hope to build a “mental toolbox” of techniques so that you know where to focus your study when you encounter a problem in the real world."
  },
  {
    "objectID": "alte-algo.html#not-modeling",
    "href": "alte-algo.html#not-modeling",
    "title": "9  Alternative Algorithms (TODO)",
    "section": "9.1 Not Modeling",
    "text": "9.1 Not Modeling\n\n9.1.1 First Principles\n\n\n9.1.2 Simple Analyses"
  },
  {
    "objectID": "alte-algo.html#extending-linear-regression",
    "href": "alte-algo.html#extending-linear-regression",
    "title": "9  Alternative Algorithms (TODO)",
    "section": "9.2 Extending Linear Regression",
    "text": "9.2 Extending Linear Regression\n\n9.2.1 Modeling Binary Outcomes\n\n\n9.2.2 Modeling Counts\n\n\n9.2.3 Modeling Time Until an Event\n\n\n9.2.4 Modeling Repeated Measures on a Population\n\n\n9.2.5 Modeling Observations in a Nested Hierarchy"
  },
  {
    "objectID": "alte-algo.html#causal-analysis-patterns",
    "href": "alte-algo.html#causal-analysis-patterns",
    "title": "9  Alternative Algorithms (TODO)",
    "section": "9.3 Causal Analysis Patterns",
    "text": "9.3 Causal Analysis Patterns\nSimilar to https://emilyriederer.netlify.app/post/causal-design-patterns/"
  },
  {
    "objectID": "alte-algo.html#special-data-types",
    "href": "alte-algo.html#special-data-types",
    "title": "9  Alternative Algorithms (TODO)",
    "section": "9.4 Special Data Types",
    "text": "9.4 Special Data Types\n\n9.4.1 Duration Analysis\n\n\n9.4.2 Time & Space Data"
  },
  {
    "objectID": "alte-algo.html#bayesian-methods",
    "href": "alte-algo.html#bayesian-methods",
    "title": "9  Alternative Algorithms (TODO)",
    "section": "9.5 Bayesian Methods",
    "text": "9.5 Bayesian Methods"
  },
  {
    "objectID": "alte-algo.html#simulation-methods",
    "href": "alte-algo.html#simulation-methods",
    "title": "9  Alternative Algorithms (TODO)",
    "section": "9.6 Simulation Methods",
    "text": "9.6 Simulation Methods\n\n9.6.1 Agent-Based\n\n\n9.6.2 Discrete Event"
  },
  {
    "objectID": "alte-algo.html#clustering-beyond-k-means",
    "href": "alte-algo.html#clustering-beyond-k-means",
    "title": "9  Alternative Algorithms (TODO)",
    "section": "9.7 Clustering (beyond K-Means)",
    "text": "9.7 Clustering (beyond K-Means)\n\n9.7.1 Density-Based\n\n\n9.7.2 Mixture Models"
  },
  {
    "objectID": "futi-find.html",
    "href": "futi-find.html",
    "title": "10  Futile Findings (TODO)",
    "section": "",
    "text": "Dirty Dozen p hacking https://psyarxiv.com/xy2dk/"
  },
  {
    "objectID": "comp-code.html",
    "href": "comp-code.html",
    "title": "11  Complexifying Code (TODO)",
    "section": "",
    "text": "structure for sorting, ordering, clear semantics\n\n\n\nlinters / stylers"
  },
  {
    "objectID": "comp-code.html#making-a-monolith",
    "href": "comp-code.html#making-a-monolith",
    "title": "11  Complexifying Code (TODO)",
    "section": "11.2 Making a monolith",
    "text": "11.2 Making a monolith\nnot using functions / files / templates / variables\n\n11.2.1 Making code inflexible (variables)\n\n\n11.2.2 Making useful code chunks hard to reuse (functions)"
  },
  {
    "objectID": "comp-code.html#project-organization",
    "href": "comp-code.html#project-organization",
    "title": "11  Complexifying Code (TODO)",
    "section": "11.3 Project organization",
    "text": "11.3 Project organization"
  },
  {
    "objectID": "comp-code.html#decoding",
    "href": "comp-code.html#decoding",
    "title": "11  Complexifying Code (TODO)",
    "section": "11.4 Decoding",
    "text": "11.4 Decoding\nrubber duck decoding"
  },
  {
    "objectID": "reje-repr.html",
    "href": "reje-repr.html",
    "title": "12  Rejecting Reproducibility (TODO)",
    "section": "",
    "text": "Package managment ?r-renv\nenvironment management\n\n\n\n\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6). https://doi.org/10.1371/journal.pcbi.1005510."
  },
  {
    "objectID": "mour-mist.html",
    "href": "mour-mist.html",
    "title": "13  Mourning Mistakes (TODO)",
    "section": "",
    "text": "Throughout this book, we have explored countless ways that data analysis can go wrong. You may also have noticed that potential disasters are numerous, but truly bullet-proof solutions are relatively scarce. The wrong thing to take away from all this discussion is that analysis itself is doomed."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "14  References",
    "section": "",
    "text": "Barrett, Brian. 2019. “How a ’NULL’ License Plate Landed One\nHacker in Ticket Hell.” WIRED, August. https://www.wired.com/story/null-license-plate-landed-one-hacker-ticket-hell/.\n\n\nBassa, Angela. 2017. “Data Alone Isn’t Ground Truth.”\nBlog. Medium. https://medium.com/@angebassa/data-alone-isnt-ground-truth-9e733079dfd4.\n\n\nBorunda, Daniel. 2020. “El Paso Officials Admit Massive COVID-19\nSpike of 3,100 New Cases Was Error.” El Paso Times,\nNovember. https://www.elpasotimes.com/story/news/health/2020/11/05/coronavirus-update-el-paso-covid-19-restrictions-shutdowns-curfew/6174493002/.\n\n\n“Covid: Man Offered Vaccine After Error Lists Him as 6.2cm\nTall.” 2021. BBC North West, February. https://www.bbc.com/news/uk-england-merseyside-56111209.\n\n\nHicks, Stephanie C., and Roger D. Peng. 2019. “Evaluating the\nSuccess of a Data Analysis.” https://arxiv.org/abs/1904.11907.\n\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats,\nDifferent Graphs: Generating Datasets with Varied Appearance and\nIdentical Statistics Through Simulated Annealing.” In. New York,\nNY, USA: Association for Computing Machinery. https://doi.org/10.1145/3025453.3025912.\n\n\nMay, Ethan. 2020. “How an Error in the Calculation of Indiana’s\nPositivity Rate May Affect You.” Indianapolis Star,\nDecember. https://www.indystar.com/story/news/health/2020/12/23/covid-indiana-positivity-rate-error-corrected-dec-30/4013741001/.\n\n\n“Six Dimensions of Data Quality Assessment.” 2020. DAMA\nUK Working Group. http://www.dama-nl.org/wp-content/uploads/2020/09/DDQ-Dimensions-of-Data-Quality-Research-Paper-version-1.2-d.d.-3-Sept-2020.pdf.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software, Articles 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex\nNederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in\nScientific Computing.” PLOS Computational Biology 13\n(6). https://doi.org/10.1371/journal.pcbi.1005510."
  },
  {
    "objectID": "more.html",
    "href": "more.html",
    "title": "(APPENDIX) Appendix",
    "section": "",
    "text": "Common Probability Distributions (TODO)"
  }
]