[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Training data analysis often begins Statistics 101 course.\nStudents learn “happy path” answer data adheres specific assumptions (“independent identically distributed Normal density”) answers pre-specified questions (notably, infamous null hypothesis significance test).\n, venture world real-world data analysis non-experimental data rarely well behaved questions asked far nuanced.one course aim teach students everything know statistics.\nfact, one best parts career statistics responsibility privilege life-long learning.\nHowever, flaw introductory statistics ’s incomplete, ’s obvious complete.\nStatistics bad salesman.\n’s season finale, cliff hanger, teasing hinting promising better come.\nStudent may leave studies believing answering complex data analysis questions trivially easy (relying one-size-fits-“panacea” learned) intractably difficult (assumptions method met.)book attempts add color dimensions data analysis showcasing nuances throughout true life cycle data analysis using two strategies.First, attempts highlight common pitfalls parts data analysis: data management computation visualization, interpretation, modeling even communication collaboration.\nData analysis fundamentally creative task, rarely canonical one-size-fits-solutions.\nCuriously, however, plenty canonical issues even require different solutions different settings.\nThus, goal book highlight common data disasters , , help students cultivate intuition detect common problems occur important analysis.Second, exploring data disasters, humbly put forth (woefully incomplete!) literature review advanced methods statistics quantitative disciplines (e.g. economics, epidemiology), help learners build “mental index” terms search techniques study encounter relevant problem.","code":""},{"path":"index.html","id":"main-topics","chapter":"Preface","heading":"0.1 Main Topics","text":"particular, aim help avoid ten types data disasters:Data Dalliances: Misinterpreting misuing data based collected representsComputational Quandaries: Letting computers said meantEgregious Aggregations: Losing critical information information condensedVexing Visualization: Confusing others plotting choicesIncredible Inferences: Drawing incorrect conclusions analytical resultsCavalier Causality: Falling prey spurious correlations masquerading causalityMindless Modeling: Failing get value models tailoring features, targets, performance metricsAlternative Algorithms: Lacking understanding alternative methods may better suited problem handFutile Findings: Asking answering questions aren’t usefulComplexifying Code: Making projects unwieldy difficult understand necessaryRejecting Reproducibility: Working inefficiently instead efficient, reproducible, sharable workflow","code":""},{"path":"index.html","id":"common-themes","chapter":"Preface","heading":"0.2 Common Themes","text":"chapter, see numerous examples disaster consider strategies help us mitigate.\nAlong way, ’ll emphasize:importance domain knowledge data-generating process decide want doThe utility simulation tool explore , fact, itThe exploration counterexamples build intuition common patterns problems even common solutions don’t existAs go, notice three common themes challenge focus introductory statistics:Summary statistics mask interesting stories see focusing variationSimilarly, observations variables rarely independent; story covarianceAssumptions Normality, broadly symmetry, often appropriate wonky, highly skewed world","code":""},{"path":"index.html","id":"software-information-and-conventions","chapter":"Preface","heading":"Software information and conventions","text":"book created excellent bookdown package.","code":""},{"path":"about-the-author.html","id":"about-the-author","chapter":"About the Author","heading":"About the Author","text":"Emily Riederer …Find website Twitter","code":""},{"path":"introduction-todo.html","id":"introduction-todo","chapter":"1 Introduction (TODO)","heading":"1 Introduction (TODO)","text":"Statistics synonymous data analysis; rigor vs practicality“Evaluating Success Data Analysis” (Hicks Peng 2019)“Data Alone Ground Truth” (Bassa 2017)","code":""},{"path":"introduction-todo.html","id":"what-is-data","chapter":"1 Introduction (TODO)","heading":"1.1 What is data?","text":"Data …","code":""},{"path":"introduction-todo.html","id":"what-is-analysis","chapter":"1 Introduction (TODO)","heading":"1.2 What is analysis?","text":"Analysis process turning information insight…S","code":""},{"path":"introduction-todo.html","id":"what-is-workflow","chapter":"1 Introduction (TODO)","heading":"1.3 What is workflow?","text":"Workflows intentional process accomplishing complex goal…","code":""},{"path":"introduction-todo.html","id":"what-is-data-analysis","chapter":"1 Introduction (TODO)","heading":"1.4 What is data analysis?","text":"Data analysis altogether …","code":""},{"path":"introduction-todo.html","id":"what-are-data-disasters","chapter":"1 Introduction (TODO)","heading":"1.5 What are data disasters?","text":"Data disasters occur …","code":""},{"path":"data-dall.html","id":"data-dall","chapter":"2 Data Dalliances","heading":"2 Data Dalliances","text":"first step data analysis , fact, data. may seem obvious, statistics textbooks often dodge detail.\nDiscussions regression analysis often begin statement like:“Let \\(X\\) \\(n x p\\) design matrix independent variables…”practice statement absurd writing book win basketball game, assuming team already 20 point lead 1 minute left play.’s convenient typically incorrect assume data happen ideal (, humbly, sufficient) data questions wish analyze.\nspecific vagaries data vary greatly domain, commonality across many fields (political science, economics, epidemiology, market research) often called work found data (, formally, “observational data”) administrative sources production systems.\ncontrast artisanally crafted data experimental data (like carefully controlled agricultural experiments motivated many early methods developments statistics), data generated neither us us.\nquote Angela Bassa, head data science e-commerce company: “Data isn’t ground truth. Data artifacts systems” (Bassa 2017).analytical implications observational versus experimental data well explored field causal inference (discuss Chapters 6 7).\nHowever, distinction implications far earlier data analysis process, well.\nname :Records fields many represent entities measures conducive analysisData collection methods may capture different subset events different frequency expected, leading systemic biasesData movement systems can insert errors (, minimum, challenges intuition)Data transformations may fragile transient, reflecting primary purpose system unrelated analytical useIn chapter, explore data structures full data generating process better understand different types data challenges emerge.\n, hone sharper intuition data can deceive us watch beginning analysis.","code":""},{"path":"data-dall.html","id":"preliminaries","chapter":"2 Data Dalliances","heading":"2.1 Preliminaries","text":"begin exploration data dalliances, must first establish baseline understanding data structure, data production, data quality.","code":""},{"path":"data-dall.html","id":"data-structure-basics","chapter":"2 Data Dalliances","heading":"2.1.1 Data Structure Basics","text":"Understanding content structure data using critical prerequisite analysis.\nbook, focus tabular, structured data like one might find Excel spreadsheet relational database.1In particular, many tools work best R developer Hadley Wickham describes “tidy data” (Wickham 2014). Namely:variable forms columnEach observation forms rowEach type observational unit forms tableThis analogous one generally finds data arranged database statisticians used conceptualizing .\nexample, design matrix linear model consists one column data independent variable included model one row observation.2\nWickham points , also similar called “3rd normal form” world relational database management systems.Using data structure valuable similar many modern data tools expect, also provides us framework think critically defined observation variable dataset.","code":""},{"path":"data-dall.html","id":"data-production-processes","chapter":"2 Data Dalliances","heading":"2.1.2 Data Production Processes","text":"statistical modeling discuss data generating process: can build models describe mechanisms create observations.\ncan broaden notion think generating process steps data production.Regardless type data (experimental, observational, survey, etc.), generally four main steps production: collection, extraction, loading, transformation.3Collect: way signals real world captured data. include logging (e.g. web traffic system monitoring), sensors (e.g. temperature collection), surveys, moreExtract: process removing data place originally captured preparation moving somewhere analysis can doneLoad: process loading extracted data final destinationTransform: process modeling transforming data structure useful analysis variables interpretableTo better theorize data quality issues, ’s useful think four DGPs: real-world DGP, data collection/extraction DGP4, data loading DGP, data transformation DGP.\nFIGURE 2.1: schematic data production process\nexample, consider role four DGPs e-commerce data:Real-world DGP: Supply, demand, marketing, range factors motivate consumer visit website make purchaseData collection DGP: Parts website instrumented log certain customer actions. log extracted different operational system (login platforms, payment platforms, account records) used analysisData loading DGP: Data recorded different systems moved data warehouse processing sort manual, scheduled, orchestrated job. different systems may make data available different frequencies.Data transformation DGP: arrive final data presentation requires creating data model describe domain-specific attributes key variables crafted data transformationsOr, consider role four DGPs subway ridership data5:Real-world DGP: Riders motivated use public transportation commute, run errands, visit friends. Different motivating factors may cause different weekly annual seasonalityData collection DGP: ride subway, riders go station enter exit turnstiles. mechanical rotation turnstile caused rider passing recordedData loading DGP: Data recorded turnstile collected centralized computer system station. week, station uploads flat file data data lake owned city’s Department TransportationData transformation DGP: Turnstiles different companies may different data formats. Transformation may include harmonizing disparate sources, coding system-generated codes (e.g. Station XYZ) semantically meaningful names (e.g. Main Street Station), publishing final unified representation across stations across timeThroughout chapter, ’ll explore understanding key concepts DGPs can help guide intuition look problems.","code":""},{"path":"data-dall.html","id":"data-quality-dimension","chapter":"2 Data Dalliances","heading":"2.1.3 Data Quality Dimension","text":"guide discussion data production can affect aspects data quality, need guiding definition data quality.\nchallenging data quality subjective task-specific.\nmatters much data “fit purpose” operates way transparent users moreso meeting preordained quality standard.Regardless, ’s useful discussion think general dimensions data quality. , rely six dimensions data quality outlined Data Management Association.\nSTheir official definitions :Completeness: proportion stored data potential “100% complete”Uniqueness: Nothing recorded based upon thing identified. inverse assessment level duplicationTimeliness: degree data represent reality required point timeValidity: Data valid conforms syntax (format, type, range) definitionAccuracy: degree data correctly describes “real world” object event\ndescribed.Consistency: absence difference, comparing two representations \nthing definition","code":""},{"path":"data-dall.html","id":"data-collection","chapter":"2 Data Dalliances","heading":"2.2 Data Collection","text":"One tricky nuances data collection understanding precisely getting captured logged first place.\nmatter robust sensors, loggers, mechanisms record dataset, data still unfit purpose long analyst fully understand represents.\nnext section, see data gets collected (understanding ) can alter notions data completion must handle computations.","code":""},{"path":"data-dall.html","id":"what-makes-a-record-row","chapter":"2 Data Dalliances","heading":"2.2.1 What Makes a Record (Row)","text":"first priority starting work dataset understanding single record (row) represents causes generated.Consider something simple login system users must enter credentials, endure Captcha-like verification process prove robot, enter multi-factor authentication code. Figure 2.2 depicts process.\nFIGURE 2.2: diagram illustrating multi-step process user login website app\nevents gets collected recorded significant impact subsequent data processing.\ntechnical sense, inclusion/exclusion decision incorrect, per say, producers’ choices don’t match consumers’ understandings, can lead misleading results.example, analyst might seek logins table order calculate rate successful website logins.\nReasonably enough, might compute rate sum successful events total.\nNow, suppose two users attempt login account, ultimately, one succeeds accessing private information doesn’t.\nanalyst probably hope compute report 50% login success rate. However, depending data represented, quite easily compute nearly value 0% 100%.Figure 2.3 depicts different realistic cases:\nFIGURE 2.3: Login events recorded different data collection paradigms\nPer Attempt: data logged per overall login attempt, successful attempts trigger one event, user forgot password may try (fail) login multiple times. case illustrated , deflates successful login rate 25%.Per Event: logins table contains row every login-related event, ‘success’ trigger large number positive events ‘failure’ trigger negative event preceded zero positive events. case illustrated , inflates successful login rate 86%.Per Conditional: collector decided look downstream events, perhaps circumvent record duplication, might decide create record denote success failure final step login process (MFA). However, login attempts failed upstream step generate record stage ’ve already fallen funnel. case, computed rate reach 100%Per Intermediate: Similarly, login defined specifically successful password verification, computed rate 100% even users subsequently fail MFAWhile humans shared intuition concepts like user, session, login , act collecting data forces us map intuition onto atomic event .\nmisunderstanding precisely definition can massive impact perceived data quality; “per event” data appear heavily duplicated assumed “per session” data.cases, obvious detect.\nsystem outputs fields incredibly specific (e.g. hyperbole, imagine step_in_the_login_process field values taking human-readable descriptions fifteen processes listed image ), depending source organized (e.g. contrast , fields like sourceid processid unintuitive alphanumeric encoded values) defined, nearly impossible understand nuances without uncovering quality metadata talking data producer.","code":""},{"path":"data-dall.html","id":"what-doesnt-make-a-record-row","chapter":"2 Data Dalliances","heading":"2.2.2 What Doesn’t Make a Record (Row)","text":"Along thinking count (gets logged), ’s equally important understand systemically generate record. Consider users intent desire login (motivated real-world DGP) find login page, users load login page never click button know ’ve forgotten password see way request .\nOften, corner cases may critical informative (e.g. , demonstrating major flaws UI).\n’s hard computationally validate data doesn’t exist, conceptual data validation critical.","code":""},{"path":"data-dall.html","id":"records-versus-keys","chapter":"2 Data Dalliances","heading":"2.2.3 Records versus Keys","text":"preceding discussion types real-world observations generate records resulting dataset related distinct another important concept world relational databases: primary keys.Primary keys minimal subset variables dataset define unique record.\nexample, previous discussion customer logins might consist natural keys6 combination session_id timestamp surrogate keys7 global event_id generated every time system logs event.Understanding table’s primary keys can useful many reasons.\nname reasons, fields often useful linking data one table another identifying data errors (uniqueness fields upheld).\nalso can suggestive true granularity table.However, simply knowing table’s primary keys resolve issues discussed prior two sections.\nmany different data collection strategies considered unique session timestamp;\nhowever, ’ve seen, guarantee must contain every session timestamp universe events.","code":""},{"path":"data-dall.html","id":"what-defines-a-variable-column","chapter":"2 Data Dalliances","heading":"2.2.4 What Defines a Variable (Column)","text":"Just critical understanding constitutes record (row) dataset understanding precise definition variable (column).\nSuperficially, task seems easier: , variable name hopefully includes semantic information. However, quite often information can provide false sense security.\nJust identify variable promising sounding name, mean relevant data analysis.example, consider wanting analyze patterns customer spend amounts across orders e-commerce website.\nmight find table orders field called amt_spend. might mean?dataset sourced payment processor, likely includes total amount billed customers’ credit card: including item prices less discounts, shipping costs, taxes, etc. Alternatively, order split across gift card credit card, field might reflect amount charged credit cardIf dataset created Finance, might perhaps include total item prices less discounts best corresponded data Finance team needs revenue reportingSomeone, somewhere, point might assigned amt_spend name variable containing gross spend (accounting discounts) might different variable amt_spend_net accounts discounts appliedIt’s critical understand variable actually means.\nupside forces analysts crisply think research questions ideal variables analysis .\n’ve seen, concepts like “spend” may seem deceptively simple, unambiguous.","code":""},{"path":"data-dall.html","id":"the-many-meanings-of-null","chapter":"2 Data Dalliances","heading":"2.2.5 The Many Meanings of Null","text":"Related presence absence full records presence absence individual fields.\nrecords contain relevant information, may published explicitly missing fields full record may published . difference implicit explicit missingness resulting data illustrated Figure 2.4.\nFIGURE 2.4: comparison explicit versus implicit missingness\nUnderstanding system implies explicitly missing data field also critical validation analysis.\nChecks data completeness usually include counting null values, null data isn’t always incorrect.\nfact, null data can highly informative know means. meanings null data might include:Field relevant: Perhaps logins table reports mobile phone operating system (iOS Android) used access login page track platform-specific issues. However, valid value thisRelevant value known: logins table might also account_id field attempts match login attempts known accounts/customers using different metadata like cookies IP addresses. theory, almost everyone trying log account identifier, methods may good enough identify casesRelevant value null: course, sometimes someone without account might try log reason. case, correct value account_id field truly nullRelevant value recorded incorrectly: Sometimes systems glitches. Without doubt, every single login attempt timestamp, field null data somehow lost corrupted sourceSimilarly, different systems might might report nulls different ways :True nulls: Literally entry resulting dataset nullNull-like non-nulls: Blank values like empty string ('') contain null amount information won’t detected counting null valuesPlaceholder values: Meaningless values like account_id 00000000 unidentified accounts preserve data validity (expected structure) intrinsic meaningSentinel/shadow values: Abnormal values attempt indicate reasons null-ness account_id -1 browser cookies found -2 cookies found help link specific customer recordEach encoding choices changes definitions appropriate completeness validity field , even critically, impacts expectations assertions form data accuracy.\ncan’t expect 100% completeness nulls relevant value; can’t check validity ranges easily sentinel values used values outside normal range (hopefully, much bigger problems!)\n, understanding upstream systems work essential assessing work.Similarly, understanding null data collected significant implications subsequently process . discuss Chapter 3 (Computational Quandaries).","code":""},{"path":"data-dall.html","id":"data-extraction-loading","chapter":"2 Data Dalliances","heading":"2.3 Data Extraction & Loading","text":"Checking data contains expected expected records (, completeness, uniqueness, timeliness) one common first steps data validation.\nHowever, superficially simple act loading data data warehouse updating data tables can introduce variety risks data completeness require different strategies detect.\nData loading errors can result data stale, missing, duplicate, inconsistently --date across sources, complete subset range think.data quality principles completeness, uniqueness, timeliness suggest records exist , reality many haphazard data loading process means data may appear sometime zero handful times. Data loads can occur many different ways.\nexample, might :manually executedscheduled (like cron job)orchestrated (tool like Airflow Prefect)approach free challenges.\nexample, scheduled jobs risk executing upstream process completed (resulting stale missing data);\npoorly orchestrated jobs may prevented working due one missing dependency might allow multiple stream get sync (resulting multisource missing data).\nRegardless method, approaches must carefully configured handle failures gracefully avoid creating duplicates, frequency executed may cause partial loading issues incompatible granularity source data.","code":""},{"path":"data-dall.html","id":"data-load-failure-modes","chapter":"2 Data Dalliances","heading":"2.3.0.1 Data Load Failure Modes","text":"develop understanding true data generating process formulate theories data broken (validate), useful understand different ways data extraction loading can fail.Figure 2.5 illustrates number examples. Suppose row boxes diagram represents one day records table.\nFIGURE 2.5: Different modes data loading failure\ndataset might susceptible :Stale data occurs data --date expected regular refresh cadence. happen manual step skipped, scheduled job executed upstream source available, orchestrated data checks found errors quarantined new recordsMissing data occurs one data load fails subsequent loads succeededDuplicate data occurs one data load executed multiple timesMultisource missing data occurs table loaded multiple sources, continued update expected others notPartial data occurs table loaded correctly intended producer contains less data expected consumer (e.g. table loads ever 12 hours data given date, user assumes relevant records date loaded)differences failure modes become important analyst attempts assess data completeness.\nOne first approaches analyst might consider simply check min() max() event dates table.\nHowever, can help detect stale data.\ncatch missing data, analyst might instead attempt count number distinct days represented data; detect duplicate data, analyst might need count records day examine pattern.case like toy example correct number rows per date highly predictable number dates small, eyeballing feasible;\nhowever expected number records varies day--day time series long, approach becomes subjective, error-prone, intractable.\nAdditionally, still might hard catch errors mutli-source data partial loads lower number records still within bounds reasonable deviation series.\nlast two types deserve exploration.","code":""},{"path":"data-dall.html","id":"multi-source","chapter":"2 Data Dalliances","heading":"2.3.0.2 Multi-Source","text":"effective strategy assessing data completeness requires better understanding data collected loaded.\ncase multi-source data, one single source stopping loading may big enough change disrupt aggregate counts still jeopardize meaningful analysis.\nuseful conduct completeness checks subgroup identify discrepancies.subgroup ;\nsubgroup must correspond various data sources.\nexample, suppose run e-commerce store wish look sales past month category.\nNaturally, might think check completeness data category.\nsales data sourced three separate locations: Shopify site (80%), Amazon Storefront (15%), phone sales (5%).\nUnless explicitly check completeness channel (dimension don’t particularly care analysis), easy miss data source phone sales stopped working loads different frequency.Another interesting aspect multi-source data, multiple sources can contribute either different rows/records different columns/variables.\nTable-level frequency counts won’t help us latter case since sources might create right total number records result specific fields records missing inaccurate.","code":""},{"path":"data-dall.html","id":"partial-loads","chapter":"2 Data Dalliances","heading":"2.3.0.3 Partial Loads","text":"Partial loads really data errors , still important detect since can jeopardize analysis.\ncommon scenario might occur job loads new data every 12 hours (say, data morning afternoon day n-1 loads day n 12AM 12PM, respectively).\nanalyst retrieving data 11AM may concerned see approximate ~50% drop sales past day, despite confirming data looks “complete” since maximum record date , fact, day n-1.\ncourse, concern somewhat easily allayed checked timestamp field, field might exists might used validation since harder anticipate appropriate maximum timestamp maximum date.","code":""},{"path":"data-dall.html","id":"delayed-or-transient-records","chapter":"2 Data Dalliances","heading":"2.3.0.4 Delayed or Transient Records","text":"interaction choices made data collection data loading phases can introduce sets problems.Consider orders table e-commerce company analysts may use track customer orders.\nmight contain one record per order_id x event (placement, processing, shipment), one record per order placed, one record per order shipping, one record per order status field changes time denote order’s current stage life. options illustrated Figure 2.6.\nFIGURE 2.6: Illustration alternative data collection extraction strategies order data\nmodeling choices seem reasonable difference might appear immaterial.\nconsider collection choice record report shipped events.\nPerhaps might operationally easier shipment come one source system whereas orders come many.\nHowever, interesting thing shipments often lagged variable way order date.Suppose e-commerce company question offers three shipping speeds checkout. Figure 2.7 shows range possible shipment dates based order dates three different speeds (shown different bars/colors).\nFIGURE 2.7: conceptual chart different classes real-world events might materialize records dataset\nmight effect perceived data quality?Order data appear stale timely since orders given order_date load days later shippedSimilar missing multisource data, data range table lead deceptive incomplete data validation orders later order date might ship (thus logged) orders previous order datePut another way, multiple order dates demonstrating partial data loadsThese features data might behave inconsistently across time due seasonality (e.g. shipping weekends federal holidays), heuristics developed clean data based small number observations failFrom analytical perspective, orders faster shipping disproportionately overrepresented “tail” (recent) data. shipping category correlated characteristics like total order spend, create artificial trend dataOnce , understanding data collected point shipment reasoning shipment timing varies impacts loading necessary successful validation.thought experiment seems vague, can make concrete mocking dataset experiment.simplest version, simply suppose one order submited 10 days dates (represented convenience integers calendar dates) given dt_subm vector.\nSuppose shipping always takes three days, can easily calculate shipment date (dt_ship) based submission date.\nshipment date date data logged loaded (dt_load).Suppose analyst living day 5 wonder many orders submitted day 3.\ncan observe shipments loaded day 5 filter data accordingly.\nHowever, count many records exist day 3 find none.\nInstead, move ahead analysis date day 7, able observe orders submitted day 3.(Note conditions checked much succinctly base R expression sum(df$dt_load < 7 & df$dt_subm == 3).\nHowever, sometimes virtue option readable code even less compact.\n, prefer verbose option claritfy exposition.\ntrade-offs, general thoughts coding style, explored Chapter 11.)Now, may seem trivial. Clearly, zero records day, catch data validation, right?\ncan make synthetic data slightly realistic better illustrate problem.\nLet’s imagine 10 orders day, order shipped sometime 2 4 days order equal probability.repeat prior analysis, now see records orders submitted day 3 time begin analysis day 5.\ncase, might easily tricked believe orders.\nHowever, repeat analysis day 7, see number orders day 3 increased.course, can imagine real world yet much complicated example.\nreality, random number orders day.\nAdditionally, might mixture different types orders.\nmight high-priced orders customers tended willing pay faster shipping,\nlow-priced orders customers tend chose slower shipping.\ncase like , might naive validation miss lack data completeness, sample shipments begin see day 5 unrepresentative population orders placed day 3.\ntype selection bias examine Chapter 6 (Incredible Inferences).","code":"\n# data simulation: single orders + deterministic ship dates ----\ndt_subm <- 1:10\ndays_to_ship <- 3\ndt_ship <- dt_subm + days_to_ship\ndt_load <- dt_ship\ndf <- data.frame(dt_subm, dt_ship, dt_load)\nhead(df)##   dt_subm dt_ship dt_load\n## 1       1       4       4\n## 2       2       5       5\n## 3       3       6       6\n## 4       4       7       7\n## 5       5       8       8\n## 6       6       9       9\nlibrary(dplyr)\n\n# how many day-3 orders do we observe as of day-5? ----\ndf %>% \n  filter(dt_load <= 5) %>% \n  filter(dt_subm == 3) %>% \n  nrow()## [1] 0\n# how many day-3 orders do we observe as of day-7? ----\ndf %>% \n  filter(dt_load <= 7) %>% \n  filter(dt_subm == 3) %>% \n  nrow()## [1] 1\n# data simulation: multiple orders + random ship dates ----\ndt_subm <- rep(x = 1:10, each = 10)\ndays_to_ship <- sample(x = 2:4, size = length(dt_subm), replace = TRUE)\ndt_ship <- dt_subm + days_to_ship\ndt_load <- dt_ship\ndf <- data.frame(dt_subm, dt_ship, dt_load)\nhead(df)##   dt_subm dt_ship dt_load\n## 1       1       5       5\n## 2       1       4       4\n## 3       1       3       3\n## 4       1       3       3\n## 5       1       3       3\n## 6       1       4       4"},{"path":"data-dall.html","id":"data-encoding-modeling-transformation-todo","chapter":"2 Data Dalliances","heading":"2.4 Data Encoding, Modeling, & Transformation (TODO)","text":"","code":""},{"path":"data-dall.html","id":"strategies-todo","chapter":"2 Data Dalliances","heading":"2.5 Strategies (TODO)","text":"","code":""},{"path":"comp-quan.html","id":"comp-quan","chapter":"3 Computational Quandaries","heading":"3 Computational Quandaries","text":"gaining confidence one’s data (, least, making peace ), next step data analysis often start cleaning exploring data summary statistics, plots, models.\nGenerally, requires computational tool like SQL, R, python.process computation can fraught challenges.\nComputational tools extremely literal; excellent precisely told often analysts might meant wished .\nAdditionally, moment analyst begins use tool, conversation longer data;\nsuddenly, mental model every single tool developer thought might want analysis affects tools’ behaviors analysts’ results.chapter, explore common ways tools may something technically correct, reasonable, -intended much analysts may expect.\nAlong way, see computational methods interact data encoding choices discussed Chapter 2 (Data Dalliances).","code":""},{"path":"comp-quan.html","id":"preliminaries---data-computation","chapter":"3 Computational Quandaries","heading":"3.1 Preliminaries - Data Computation","text":"think specific tools failure modes, can first consider common types operations analytical tools allow us data.","code":""},{"path":"comp-quan.html","id":"single-table-operations","chapter":"3 Computational Quandaries","heading":"3.1.1 Single Table Operations","text":"Given single data table, may wish operations (illustrated Figure 3.1) :Filtering: Extracting subset dataset analysis based certain inclusion criteria recordAggregation: Grouping data table one variables condensing information across records aggregate functions like counts, sums, averagesTransformation: Create new columns modifying existing columns represent complex domain-specific context\nFIGURE 3.1: Illustration basic single-table data wrangling operations\n","code":""},{"path":"comp-quan.html","id":"multiple-table-operations","chapter":"3 Computational Quandaries","heading":"3.1.2 Multiple Table Operations","text":"Often, can get additional value analysis combining multiple types information difference tables.\nworking multiple tables, may interested :Combining Row-wise: Taking multiple tables schemas (column names data types) creating single table contains union (records), intersection (matching), difference (one) records two tablesCombining Column-wise: Appending additional fields existing records joining (also known merging) multiple tables","code":""},{"path":"comp-quan.html","id":"mechanics","chapter":"3 Computational Quandaries","heading":"3.1.3 Mechanics","text":"operations rely core computational tasks:Arithmetic: Basic addition, subtraction, multiplication, division aggregate transform dataEquality: Comparing whether two values equal critical data filtering, column-wise combination, certain types data transformationCasting: Converting data types different elements comparable format necessary row-wise combination often prerequisite certain equality arithmetic tasksWhile operations may seem simple, behavior within certain tools employed certain data types may sometimes lead unintuitive misleading results.","code":""},{"path":"comp-quan.html","id":"null-values","chapter":"3 Computational Quandaries","heading":"3.2 Null Values","text":"Chapter 2 (Data Dalliances), discuss null values may represent many different concepts encoded multiple different ways.\naddition semantic challenges, various representations null values may cause different computational problems.8\nsection, explore potential failure modes.","code":""},{"path":"comp-quan.html","id":"types-of-null-values","chapter":"3 Computational Quandaries","heading":"3.2.1 Types of Null Values","text":"can null values represent many different things (explored Chapter 2), also may represented many different ways. Understanding nulls encoded one’s dataset critical prerequisite attempting computations described subsequent sections.","code":""},{"path":"comp-quan.html","id":"language-representations","chapter":"3 Computational Quandaries","heading":"3.2.1.1 Language representations","text":"Different programming languages offer versions null values – sometimes one. example, R language includes NA, typed NAs (e.g. NA_integer, NA_character), NaN, NULL; meanwhile, core python None numpy module provides nan.different values carry different semantic functional meanings. example R’s NA generally means “presence absence” whereas NULL “absence presence.” articulated clearly examine lengths objects observe NA length 1 whereas NULL length 0.proof interchangeable, may use helper functions .na() .null(). ’s false NA NULL essentially unable evaluated NULL NA NULLs truly nothing.9To complicate matters, NaN (“number”), along -Inf Inf, generally arise attempt abuse R’s calculator. Somewhat charmingly, Inf -Inf may used rudimentary calculations limit returned.10","code":"\nc(length(NA), length(NULL))## [1] 1 0\nc(\n  is.na(NA),\n  is.null(NULL),\n  is.na(NULL),\n  is.null(NA)\n)## [1]  TRUE  TRUE FALSE\nc(\n  1/0,   # returns Inf\n  0/0,   # returns NaN\n  1/Inf  # returns 0\n)## [1] Inf NaN   0"},{"path":"comp-quan.html","id":"data-encoding-choices-todo","chapter":"3 Computational Quandaries","heading":"3.2.1.2 Data encoding choices (TODO)","text":"Beyond null types offered natively different programming languages, also many different data management conventions null values. null values can many meanings, sometimes missing fields encoded “range” values intend suggest type missingness.example, US Census Bureau’s Medical Expenditure Panel Survey uses following reserved codes denote different types missingness: (TODO: cite p10 https://www.meps.ahrq.gov/data_stats/download_data/pufs/h206a/h206adoc.pdf)approach preserves lot relevant information , time, readily apparent values valid data manually inspect. Unfortunately, manually inspecting every data field rarely possible, sentinel values may go undetected looking higher-level summaries.Consider survey population retired adults age coded 999 provided. , simulate 100,000 observations uniformly distributed age 65 95 (hence, expected value 80). Next, replace merely half percent “null” values 999. Taking mean false values results mean 85. number alone might raise alarm; , know dataset’s population older adults. However, accidentally treating valid values biases results somewhat remarkable five years., first order business null values understanding encoded translation computationally appropriate form. However, beginning story.","code":"- -1 INAPPLICABLE Question was not asked due to skip pattern\n- -7 REFUSED Question was asked and respondent refused to answer question\n- -8 DK Question was asked and respondent did not know answer\n- -14 NOT YET TAKEN/USED Respondent answered that the medicine has not yet been used\n- -15 CANNOT BE COMPUTED Value cannot be derived from data\nset.seed(123)\n\nn <- 100000\np <- 0.005\nages <- runif(n, 65, 95)\n\nages_nulls <- ages\nages_nulls[1:(n*p)] <- 999\n\nc(mean(ages), mean(ages_nulls))## [1] 79.98 84.57"},{"path":"comp-quan.html","id":"aggregation","chapter":"3 Computational Quandaries","heading":"3.2.2 Aggregation","text":"null values handled simple aggregation data varies across different languages across different functions within language.\nbetter understand problems might cause, look examples R SQL.explore aggregation, let’s build simple dataset. suppose working subscription-based e-commerce service looking monthly_spend dataset one record per customer information amount spent returned given month:compute average amount spent (AMT_SPEND) dplyr package, analyst might first reasonably write following summarize() statement.\nHowever, can see, due presence null values within AMT_SPEND column, result aggregation whole quantity AVG_SPEND set value NA.glance documentation mean() function reveals na.rm parameter , set true, removes null values dataset.\nAdding argument previous statement allows us reach numerical answer.However, right numerical answer?\nna.rm = TRUE drop null values set numbers averaged.\nHowever, suppose null values represent purchases made.\n, zero dollars spent.\neffect, removed non-purchasers data averaged.precisely, switched taking average\\[ \\frac{ \\sum_{1}^{n} Spend }{\\sum_{1}^{n} 1} \\] \\(n\\) customersto taking average\\[ \\frac{ \\sum_{Spend > 0} Spend }{\\sum_{Spend > 0} 1} \\] customers spendAt face value, say code giving incorrect answer; dropping low (zero) purchase amounts, average amount spend per customer inflated.\nsecond perspective, someone philosophically troubling, tiny change code fixed obvious problem (returning null value) introduced non-obvious problem fundamentally changing question asking.\ndropping accounts table made purchases, longer answering “average amount spent new registrant?” rather “average amount spent actively engaged customer?”\ntechnical quirk significant analytical impact.answer real question hand, instead couple options.\nmanually sum() amount spent option drop nulls divide correct denominator (observations – just spend) explicitly recode null values AMT_SPEND zero taking average.11\nEither options lead correct conclusion lower average spend amount.well good just accept behaviors simply nulls work, complexity comes see industry standard across tools.\nexample, SQL code shows, SQL’s avg() function behaves like R’s mean() na.rm = TRUE option set.\n, default behavior SQL operate valid available values.However, suggest null values also destructive SQL.\naggregation functions (compute rows/records) like sum() avg() drop nulls, operators like + - (compute across columns/variables row/record) exhibit behavior.\nConsider, example, wish calculate average net purchase amount (purchases minus returns) instead gross (total) purchase amount.Despite learned SQL’s avg() function, query returns null value.\nhappened?\nspend dataset, amt_return column completely null (representing returns).\nsubtraction occurs average taken, subtracting real numbers amt_spend null values amt_return creates column null values fed avg() function.\nprocess shown step--step .","code":"\nspend <-\n  data.frame(\n    AMT_SPEND = c(10, 20, NA),\n    AMT_RETURN = rep(NA, 3)\n  )\nsummarize(spend, \n          AVG_SPEND = mean(AMT_SPEND),\n          AVG_SPEND_NARM = mean(AMT_SPEND, na.rm = TRUE))##   AVG_SPEND AVG_SPEND_NARM\n## 1        NA             15\nsummarize(\n    spend,\n    AVG_SPEND_MANUAL = sum(AMT_SPEND, na.rm = TRUE) / n(),\n    AVG_SPEND_RECODE = mean(coalesce(AMT_SPEND, 0))\n  )##   AVG_SPEND_MANUAL AVG_SPEND_RECODE\n## 1               10               10SELECT avg(amt_spend) as AVG_SPEND\nFROM spend##   AVG_SPEND\n## 1        15SELECT avg(amt_spend-amt_return) as AVG_SPEND_NET\nFROM spend##   AVG_SPEND_NET\n## 1            NASELECT\n  amt_spend, \n  amt_return, \n  amt_spend-amt_return \nFROM spend##   AMT_SPEND AMT_RETURN amt_spend-amt_return\n## 1        10         NA                   NA\n## 2        20         NA                   NA\n## 3        NA         NA                   NA"},{"path":"comp-quan.html","id":"comparison","chapter":"3 Computational Quandaries","heading":"3.2.3 Comparison","text":"Null values don’t just introduce complexity arithmetic. Difficulties also arise time multiple variables assessed equality inequality. Since null value unknown, programming languages generally consider nulls comparable nulls.can simple examples R SQL.toy examples, outcomes may seem perfectly logical.\nHowever, reasoning can arise sneakier ways lead uninteded results equality evaluations implicit task hand instead singular focus.\n’ll now see examples data filtering, joining, transformation.","code":"\nc(\n  NA == 3, \n  NA > 10, \n  NA == NA\n  )## [1] NA NA NASELECT\n  (NULL = 3) as NULL_EQ_NUM,\n  (NULL > 10) as NULL_GT_NUM,\n  (NULL = NULL) as NULL_EQ_NULL##   NULL_EQ_NUM NULL_GT_NUM NULL_EQ_NULL\n## 1          NA          NA           NA"},{"path":"comp-quan.html","id":"filtering","chapter":"3 Computational Quandaries","heading":"3.2.3.1 Filtering","text":"Suppose want split dataset two datasets based high low values spend.\nmight assume following two lines code create clear partition12 results.However, examining resulting datasets, see neither contains null records.situation results SQL.Thus, whenever data null values, common act data filtering risks excluding important information.","code":"\nspend_lt20 <- filter(spend, AMT_SPEND < 20)\nspend_gte20 <- filter(spend, AMT_SPEND >= 20)\nspend_lt20##   AMT_SPEND AMT_RETURN\n## 1        10         NA\nspend_gte20##   AMT_SPEND AMT_RETURN\n## 1        20         NASELECT *\nFROM spend\nWHERE AMT_SPEND < 20##   AMT_SPEND AMT_RETURN\n## 1        10         NASELECT *\nFROM spend\nWHERE AMT_SPEND >= 20##   AMT_SPEND AMT_RETURN\n## 1        20         NA"},{"path":"comp-quan.html","id":"joining","chapter":"3 Computational Quandaries","heading":"3.2.3.2 Joining","text":"phenomenon described also happens joining multiple datasets.Suppose multiple datasets wish merge based columns denoting record’s name date birthday.\nease exploration, make simplest possible dataset simply try merge .\n(may seem silly, often trying understand computationally complex things, good idea make scenario simple possible.\nfact, idea core concept computational unit tests discuss end chapter.)SQL, try join table, records row 1 match 'Anne' == 'Anne' '2000-01-01' == '2000-01-01'.\nHowever, poor Bob’s record eliminated birthdate logged null, NA == NA false.contrast, R’s dplyr::inner_join() function default.\nfunction lets us specifically control nulls matches na_matches argument, default option match NA values.\n(may read argument typing ?dplyr::inner_join R console pull documentation.)example cautionary tale null values may unintentionally corrupt data transformations also “brittle” knowledge intuition may moving tools.\nNeither default behaviors strictly better worse, definitely different real implications analysis.","code":"\nbday <- data.frame(NAME = c('Anne', 'Bob'), BIRTHDAY = c('2000-01-01', NA))\nbday##   NAME   BIRTHDAY\n## 1 Anne 2000-01-01\n## 2  Bob       <NA>SELECT a.*\nFROM\n  bday as a\n  INNER JOIN\n  bday as b\n  ON\n  a.NAME = b.NAME and\n  a.BIRTHDAY = b.BIRTHDAY##   NAME   BIRTHDAY\n## 1 Anne 2000-01-01\ninner_join(bday, bday, by = c('NAME', 'BIRTHDAY'))##   NAME   BIRTHDAY\n## 1 Anne 2000-01-01\n## 2  Bob       <NA>"},{"path":"comp-quan.html","id":"transformation","chapter":"3 Computational Quandaries","heading":"3.2.3.3 Transformation","text":"common task data analysis aggregate results subgroup.\nexample, might want summarize many customers (rows/records) spent less $10. discern , might create categorical variable high versus low purchase amounts, group variable count.psuedocode read something like :define HIGH_LOW variable, might use function like ifelse(), dplyr::if_else(), dplyr::case_when().\nHowever, , issue values partitioned nulls included.\nrecode records AMT_SPEND less equal 10 “Low” default rest “High,” accidentally count null values “High” group.Instead, accurate transparent (unless know specifically null values mean group part ) let one “core” categories “default” case logic.\ncan explicitly encode residual values something like “” “ERROR” help us see problem requiring extra attention.","code":"data %>%\n  mutate(HIGH_LOW = << transform AMT_SPEND >>) %>%\n  group_by(HIGH_LOW) %>%\n  count()\nspend %>%\n  mutate(HIGH_LOW = case_when(\n    AMT_SPEND <= 10 ~ \"Low\", \n    TRUE ~ \"High\")\n    ) %>%\n  group_by(HIGH_LOW) %>% \n  count()## # A tibble: 2 x 2\n## # Groups:   HIGH_LOW [2]\n##   HIGH_LOW     n\n##   <chr>    <int>\n## 1 High         2\n## 2 Low          1\nspend %>%\n  mutate(HIGH_LOW = case_when(\n    AMT_SPEND <= 10 ~ \"Low\", \n    AMT_SPEND > 10 ~ \"High\",\n    TRUE ~ \"OTHER\")\n    ) %>%\n  group_by(HIGH_LOW) %>% \n  count()## # A tibble: 3 x 2\n## # Groups:   HIGH_LOW [3]\n##   HIGH_LOW     n\n##   <chr>    <int>\n## 1 High         1\n## 2 Low          1\n## 3 OTHER        1"},{"path":"comp-quan.html","id":"dates-and-times-todo","chapter":"3 Computational Quandaries","heading":"3.3 Dates and Times (TODO)","text":"","code":""},{"path":"comp-quan.html","id":"strings-todo","chapter":"3 Computational Quandaries","heading":"3.4 Strings (TODO)","text":"","code":""},{"path":"comp-quan.html","id":"encoding-choices-todo","chapter":"3 Computational Quandaries","heading":"3.5 Encoding Choices (TODO)","text":"","code":""},{"path":"comp-quan.html","id":"order-of-operations-todo","chapter":"3 Computational Quandaries","heading":"3.6 Order of Operations (TODO)","text":"","code":""},{"path":"comp-quan.html","id":"object-references-todo","chapter":"3 Computational Quandaries","heading":"3.7 Object References (TODO)","text":"","code":""},{"path":"comp-quan.html","id":"trusting-tools","chapter":"3 Computational Quandaries","heading":"3.8 Trusting Tools","text":"theme throughout book fundamentally social nature data analysis. Data analysis fraught without understanding countless decisions made along way generated (whose data reflected), collected , migrated , posed questions . one hand, beautiful aspect analysis; hand, means analysts analyses subject cognitive social psychological biases everyday humans.One bias “social proof”: assuming tool behaves certain way, must correct.Assuming tools know best admittedly attractive proposition. appeals desire think someone, somewhere “charge” , perhaps critically, helps us avoid domino effect distrust (don’t trust tools can trust results? can’t trust results, can trust anything ?) Unfortunately, many reasons tools might know best. example, tool’s developer might :Made mistakeHad different analysis problem mind different optimal approachBeen optimizing different constraint (e.g. explainability vs. accuracy, speed vs. theoretical properties)Come community different normsBeen affording users flexibility things many ways even don’t agreeBuilt certain feature different purpose using itNot thought allAs concrete examples popular open source tools. ’ll look briefly prominent python library scikitlearn machine learning Apache Spark, engine large-scale distributed data processing.","code":""},{"path":"comp-quan.html","id":"defaults-in-scikitlearn","chapter":"3 Computational Quandaries","heading":"3.8.1 Defaults in scikitlearn","text":"scikitlearn’s default behavior logistic regression modeling13 automatically applies L2 regularization. might might know means, might might want apply problem. ’s fine. important thing change estimates predictions, part classical definition algorithm (modelers coming statistical background.)course, ’s nothing inherently wrong choice; library authors just different goals typical statistical. scikitlearn developer Olivier Grisel explains Twitter choice (others library) explained “Scikit-learn always designed make easy get good predictive accuracy (eg measured CV) rather statistical inference library.” Additionally, choice documented bold function documentation.However, analyst easily miss nuance read documentation. , misinterpret choice social proof regularization always right approach, might make best choice analysis.","code":""},{"path":"comp-quan.html","id":"algorithms-in-spark","chapter":"3 Computational Quandaries","heading":"3.8.2 Algorithms in Spark","text":"example, according 2015 Jira ticket, developers Spark considered multiple methodologies use adding functionality compute feature importance random forest. Ultimately, core contributor advised permutation importance due computational cost.\nFIGURE 3.2: JIRA ticket Spark discussion random forest variable importance algorithm implement\nClearly, one wants workflow costly timely run. , , right wrong. However, since every approach feature importance biases, pitfalls, challenges interpretation, ’s mistake end-user carefully understand algorithm used .","code":""},{"path":"comp-quan.html","id":"strategies-todo-1","chapter":"3 Computational Quandaries","heading":"3.9 Strategies (TODO)","text":"","code":""},{"path":"eg-agg.html","id":"eg-agg","chapter":"4 Egregious Aggregations","heading":"4 Egregious Aggregations","text":"armed understanding data tools available analysis, common start analysis exploring data aggregation.\nheart, sort data analysis process condensing raw data something manageable useful giving little information possible.Many elementary tools task much better comprehension task preservation one.\nlearn rigorous assumptions consider validate studying linear regression, basic arithmetic aggregation presents agnostic welcome type data.\nHowever, underlying distributions variables relationships significant impact informative interpretable various summarizations .chapter, explore different ways univariate multivariate aggregations can naive uninformative.","code":""},{"path":"eg-agg.html","id":"averages","chapter":"4 Egregious Aggregations","heading":"4.1 Averages","text":"","code":""},{"path":"eg-agg.html","id":"averaging-skewed-data","chapter":"4 Egregious Aggregations","heading":"4.1.1 Averaging skewed data","text":"Arithmetic average versus colloquial meaning average “typical”Skewed dataMultimodal data / mixture models","code":""},{"path":"eg-agg.html","id":"no-average-observation","chapter":"4 Egregious Aggregations","heading":"4.1.2 No “average” observation","text":"previous section, average represented point relevant data range even perhaps one representative “typical” observation.\ndiscussed situations quantity may reasonable answer certain types questions aid certain types decisions.However, seek average profile multiple variables, problems averages compounded.\nmay end set “average” summary statistics representative part population.see , let’s assume working data company subscription business model.\nmight interested profiling age account (long subscriber) activity (measured amount spent e-commerce platform, files downloaded streaming service, etc.)following code simulates set observations:\n80% accounts 0 3 years age average activity level 100 20% accounts older 3 years age average activity level 500.\n(Don’t -think specific probability distributions lived .\nconcerned interrogating properties average simulating realistic data generating process.\nGiving permission wrong “lazy” unimportant things gives us energy focus matters.)Figure 4.1 shows scatterplot relationship account age (x-axis) activity level (y-axis).\nMeanwhile, marginal rug plots shows univariate distribution variable.\nsole red dot denotes coordinates average age average activity.\nNotably, dot exists region “zero density”;\n, representative customer.\nStrategic decisions made sort observation mind “typical” might destined success.\nFIGURE 4.1: scatterplot two variables averages\n","code":"\nset.seed(123)\n\n# define simulation parameters ----\n## n: total observations\n## p: proportion of observations in group 1\nn <- 5000\np <- 0.8\nn1 <- n*p\nn2 <- n*(1-p)\n\n# generate fake dataset with two groups ----\ndf <- \n  data.frame(\n    age = c(runif(n1,   0,  3), runif(n2,   3, 10)),\n    act = c(rnorm(n1, 100, 10), rnorm(n2, 500, 10))\n  )"},{"path":"eg-agg.html","id":"the-product-of-averages","chapter":"4 Egregious Aggregations","heading":"4.1.3 The product of averages","text":"example shows, averages multivariate data can produce poor summaries – particularly variables interrelated14.second implication observation deriving additional computations based pre-averaged numbers likely obtain inaccurate results.example, consider wish estimate average dollar amount returns per e-commerce order.\nOrders may generally mixture low-price orders (around $50 average) high-price orders (around $250 average).\nLow-price orders may 10% probability returned high price orders 20% probability.\n(, numbers, distributions, relationships hyper-realistic?\n.\nHowever, telling story just reason numerical properties, give permission focus irrelevant details.)true average amount returned across orders 36.0438 (average_of_product variable).\nHowever, instead already knew average spend amount average return proportion, might inclined compute product_of_average method returns value 26.9923.\n(difference 9.05 relative average purchase amount 150.)first, may seem unintuitive write formulas realize metrics , fact, two different quantities:\\[ \\frac{ \\sum_{1}^{n} Spend }{\\sum_{1}^{n} 1} \\ * \\frac{ \\sum_{1}^{n} (Return) }{\\sum_{1}^{n} 1} \\] \\(n\\) ordersversus\\[ \\frac{\\sum_{1}^{n} Spend * (Return)}{\\sum_{1}^{n} 1} \\]still feels counterintuitive, can see much difference accounted interrelation two variables.\nfollowing code, break relationship variables randomly reordering ind_return variable longer true relationship corresponding amt_spend variable.redoing calculations, find th two values much closer.\naverage_of_product now 24.1041 product_of_average now 26.9923.\nnotably still number mean two equations equivalent variables unrelated;\nhowever, second result illustrates extent interrelations can defy naive intuitions.","code":"\nset.seed(123)\n\n# define simulation parameters ----\n## n: observations per group\n## pr[1|2]: mean price per group\nn <- 100\npr1 <- 50\npr2 <- 250\npr_sd <- 5\nre1 <- 0.1\nre2 <- 0.2\n\n# simulate spend amounts and return indicatiors ----\namt_spend  <- c(rnorm(n, pr1, pr_sd), rnorm(n, pr2, pr_sd))\nind_return <- c(rbinom(n, 1, re1),    rbinom(n, 1, re2))\n\n# compute summary statistics ----\naverage_of_product <- mean(amt_spend * ind_return)\nproduct_of_average <- mean(amt_spend) * mean(ind_return)\n# randomly reorder one of two variables to break relationships ----\nind_return <- sample(ind_return, size = 200)\n\n# recompute variables ----\naverage_of_product <- mean(amt_spend * ind_return)\nproduct_of_average <- mean(amt_spend) * mean(ind_return)"},{"path":"eg-agg.html","id":"ratios","chapter":"4 Egregious Aggregations","heading":"4.2 Ratios","text":"","code":""},{"path":"eg-agg.html","id":"picking-the-right-denominator","chapter":"4 Egregious Aggregations","heading":"4.2.1 Picking the right denominator","text":"","code":""},{"path":"eg-agg.html","id":"sample-size-effects","chapter":"4 Egregious Aggregations","heading":"4.2.2 Sample size effects","text":"","code":""},{"path":"eg-agg.html","id":"trends","chapter":"4 Egregious Aggregations","heading":"4.3 Trends","text":"","code":""},{"path":"eg-agg.html","id":"if-trends-continue","chapter":"4 Egregious Aggregations","heading":"4.3.1 “If trends continue…”","text":"","code":""},{"path":"eg-agg.html","id":"seasonality","chapter":"4 Egregious Aggregations","heading":"4.3.2 Seasonality","text":"","code":""},{"path":"vex-viz.html","id":"vex-viz","chapter":"5 Vexing Visualization (TODO)","heading":"5 Vexing Visualization (TODO)","text":"","code":""},{"path":"incr-infe.html","id":"incr-infe","chapter":"6 Incredible Inferences (TODO)","heading":"6 Incredible Inferences (TODO)","text":"Previously, seen different inputs like data, tools, methods can add risks data analysis. However, battle won simply get first set outputs. chapter, explore common errors interpreting results analysis exploring aspects bias, missingness, confounding.","code":""},{"path":"cava-caus.html","id":"cava-caus","chapter":"7 Cavalier Causality (TODO)","heading":"7 Cavalier Causality (TODO)","text":"Chapter 6 (Incredible Inferences), began see can tricked biases lack causal thinking underlying theory data generating process. chapter, revisit disasters introduce specific frameworks help us rigorously explore analysis errors biases , even better, strategize best ways fix .","code":""},{"path":"mind-mod.html","id":"mind-mod","chapter":"8 Mindless Modeling (TODO)","heading":"8 Mindless Modeling (TODO)","text":"","code":""},{"path":"mind-mod.html","id":"features","chapter":"8 Mindless Modeling (TODO)","heading":"8.1 Features","text":"","code":""},{"path":"mind-mod.html","id":"targets","chapter":"8 Mindless Modeling (TODO)","heading":"8.2 Targets","text":"","code":""},{"path":"mind-mod.html","id":"evaluation-metrics","chapter":"8 Mindless Modeling (TODO)","heading":"8.3 Evaluation Metrics","text":"","code":""},{"path":"mind-mod.html","id":"clustering","chapter":"8 Mindless Modeling (TODO)","heading":"8.4 Clustering","text":"","code":""},{"path":"mind-mod.html","id":"lifecycle-management","chapter":"8 Mindless Modeling (TODO)","heading":"8.5 Lifecycle Management","text":"","code":""},{"path":"alt-alg.html","id":"alt-alg","chapter":"9 Alternative Algorithms (TODO)","heading":"9 Alternative Algorithms (TODO)","text":"consummate showman, P.T. Barnum often quoted saying “Leave wanting .” Unfortunately, statistics professors less flare drama. Introductory statistics courses typically introduce types models (example, linear perhaps logistic regression), ’s wrap. ’s often students start taking subsequent courses exposed true limitations previous techniques taught demand .chapter attempts flip paradigm briefly surveying broad number modeling techniques. goal go rigorous deals one understand use models. Instead, hope build “mental toolbox” techniques know focus study encounter problem real world.","code":""},{"path":"alt-alg.html","id":"modeling-binary-outcomes","chapter":"9 Alternative Algorithms (TODO)","heading":"9.1 Modeling Binary Outcomes","text":"","code":""},{"path":"alt-alg.html","id":"modeling-counts","chapter":"9 Alternative Algorithms (TODO)","heading":"9.2 Modeling Counts","text":"","code":""},{"path":"alt-alg.html","id":"modeling-time-until-an-event","chapter":"9 Alternative Algorithms (TODO)","heading":"9.3 Modeling Time Until an Event","text":"","code":""},{"path":"alt-alg.html","id":"modeling-repeated-measures-on-a-population","chapter":"9 Alternative Algorithms (TODO)","heading":"9.4 Modeling Repeated Measures on a Population","text":"","code":""},{"path":"alt-alg.html","id":"modeling-observations-in-a-nested-hierarchy","chapter":"9 Alternative Algorithms (TODO)","heading":"9.5 Modeling Observations in a Nested Hierarchy","text":"","code":""},{"path":"alt-alg.html","id":"modeling-time-space-data","chapter":"9 Alternative Algorithms (TODO)","heading":"9.6 Modeling Time & Space Data","text":"","code":""},{"path":"futi-find.html","id":"futi-find","chapter":"10 Futile Findings (TODO)","heading":"10 Futile Findings (TODO)","text":"Previously, seen different inputs like data, tools, methods can add risks data analysis. However, battle won simply get first set outputs. chapter, explore common errors interpreting results analysis exploring aspects bias, missingness, confounding.","code":""},{"path":"comp-code.html","id":"comp-code","chapter":"11 Complexifying Code (TODO)","heading":"11 Complexifying Code (TODO)","text":"","code":""},{"path":"reje-repr.html","id":"reje-repr","chapter":"12 Rejecting Reproducibility (TODO)","heading":"12 Rejecting Reproducibility (TODO)","text":"","code":""},{"path":"useful-data-generation-functions-todo.html","id":"useful-data-generation-functions-todo","chapter":"A Useful Data Generation Functions (TODO)","heading":"A Useful Data Generation Functions (TODO)","text":"","code":""},{"path":"common-probability-distributions-todo.html","id":"common-probability-distributions-todo","chapter":"B Common Probability Distributions (TODO)","heading":"B Common Probability Distributions (TODO)","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
