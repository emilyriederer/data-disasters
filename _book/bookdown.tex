% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Data Disasters},
  pdfauthor={Emily Riederer},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=Blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter

\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Data Disasters}
\author{Emily Riederer}
\date{2021-08-28}

\begin{document}
\maketitle

% you may need to leave a few empty pages before the dedication page

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}

\begin{center}
To all the mistakes I've made (data, and otherwise) and those who tolerated my making them.
%\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


\begin{center}\includegraphics[width=0.9\linewidth]{figures/cover} \end{center}

Training in data analysis often begins with Statistics 101 course.
Students learn the ``happy path'' of answer data that adheres to specific assumptions (such as ``independent and identically distributed with a Normal density'') and answers pre-specified questions (most notably, the infamous null hypothesis significance test).
Then, they venture out into the world of real-world data analysis where non-experimental data is rarely so well behaved and the questions asked of it are far more nuanced.

No one course should aim to teach students everything they should know about statistics.
In fact, one of the best parts about a career in statistics is the responsibility and privilege of life-long learning.
However, \textbf{the flaw of introductory statistics is not that it's incomplete, but that it's not obvious how it is \emph{not} complete}.
Statistics is a bad salesman.
There's no season finale, no cliff hanger, no teasing and hinting and promising more and better to come.
Student may leave their studies believing that answering more complex data analysis questions is trivially easy (by relying on the one-size-fits-all ``panacea'' that they learned) or intractably difficult (when the assumptions of that method are not met.)

This book attempts to add more color to all the dimensions of data analysis while showcasing the nuances throughout the true \emph{life cycle} of data analysis using two strategies.

First, it attempts to highlight common pitfalls in all the parts of data analysis: from data management and computation to visualization, interpretation, and modeling and even to communication and collaboration.
Data analysis is fundamentally a \emph{creative} task, so there are rarely canonical one-size-fits-all solutions.
Curiously, however, there are plenty of canonical \emph{issues} even if they require different solutions in different settings.
Thus, the goal of this book is to highlight common \emph{data disasters} and, in doing so, help students cultivate an intuition for how to detect common problems before they occur in an important analysis.

Second, while exploring these \emph{data disasters}, we humbly put forth a (woefully incomplete!) literature review of more advanced methods from statistics and other quantitative disciplines (e.g.~economics, epidemiology), to help learners build a ``mental index'' of terms to search and techniques to study should they encounter a relevant problem.

The content in this book is currently being developed and is all subject to change.

Chapters and sections tagged as \textbf{WIP} (work-in-progress) have substantial content and are suitable for reading.

Chapters and sections tagged as \textbf{TODO} have minimal outlines or code examples (if that).

\hypertarget{main-topics}{%
\section{Main Topics}\label{main-topics}}

In particular, we will aim to help you avoid eleven types of data disasters:

\begin{itemize}
\tightlist
\item
  \textbf{Data Dalliances}: Misinterpreting or misuing data based on how it was collected or what it represents
\item
  \textbf{Computational Quandaries}: Letting computers do what you said and not what you meant
\item
  \textbf{Egregious Aggregations}: Losing critical information when information is condensed
\item
  \textbf{Vexing Visualization}: Confusing ourselves or others with plotting choices
\item
  \textbf{Incredible Inferences}: Drawing incorrect conclusions for analytical results
\item
  \textbf{Cavalier Causality}: Falling prey to spurious correlations masquerading as causality
\item
  \textbf{Mindless Modeling}: Failing to get the most value out of models by not tailoring the features, targets, and performance metrics
\item
  \textbf{Alternative Algorithms}: Lacking an understanding of alternative methods which may be better suited for the problem at hand
\item
  \textbf{Futile Findings}: Asking and answering questions that aren't useful
\item
  \textbf{Complexifying Code}: Making projects unwieldy or more difficult to understand than necessary
\item
  \textbf{Rejecting Reproducibility}: Working inefficiently instead of an efficient, reproducible, and sharable workflow
\end{itemize}

\hypertarget{common-themes}{%
\section{Common Themes}\label{common-themes}}

In each chapter, we will see numerous examples of each disaster and consider strategies to help us mitigate.
Along the way, we'll emphasize:

\begin{itemize}
\tightlist
\item
  The importance of \textbf{domain knowledge} and the \textbf{data-generating process} to decide what it is you want to do
\item
  The utility of \textbf{simulation} as a tool to explore if, in fact, you are doing it
\item
  The exploration of \textbf{counterexamples} to build \textbf{intuition for common patterns} of problems even where common solutions don't exist
\end{itemize}

As we go, we will notice how three common themes that challenge the focus of introductory statistics:

\begin{itemize}
\tightlist
\item
  Summary statistics mask interesting stories that we see when focusing on the \textbf{variation}
\item
  Similarly, observations and variables are rarely independent; the story is in the \textbf{covariance}
\item
  Assumptions of Normality, or more broadly symmetry, are often in appropriate in wonky, \textbf{highly skewed} world
\end{itemize}

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

This book is built by the excelent \textbf{bookdown} R package with styling tips heavily inspired by the \href{https://github.com/tidymodels/TMwR}{Tidy Maching Leaning with R} repo.

Sidebar icons used throughout the book are credited to \href{https://www.flaticon.com/authors/vectors-market}{Vector Markets} (lightbulb) and \href{https://www.freepik.com\%22}{Freepik} (notepad and caution sign) on \href{https://www.flaticon.com/}{www.flaticon.com}. All other images are my own and made with either \href{https://excalidraw.com/}{Excalidraw} or \href{https://inkscape.org/}{Inkscape}.

\hypertarget{about-the-author}{%
\chapter*{About the Author}\label{about-the-author}}


\textbf{Emily Riederer} is\ldots{}

Find me on my \href{https://emilyriederer.netlify.com?utm_source=datadisasters}{website} and on \href{https://twitter.com/EmilyRiederer}{Twitter}

\mainmatter

\hypertarget{introduction-todo}{%
\chapter{Introduction (TODO)}\label{introduction-todo}}

Statistics is not synonymous with data analysis; rigor vs practicality

``Evaluating the Success of a Data Analysis'' \citep{hicks2019evaluating}

``Data Alone is not Ground Truth'' \citep{bassa_2017}

\hypertarget{case-study}{%
\section{Case Study}\label{case-study}}

\hypertarget{what-is-data}{%
\section{What is data?}\label{what-is-data}}

Data is\ldots{}

\hypertarget{what-is-analysis}{%
\section{What is analysis?}\label{what-is-analysis}}

Analysis is the process of turning information into insight\ldots S

\hypertarget{what-is-workflow}{%
\section{What is workflow?}\label{what-is-workflow}}

Workflows are an intentional process for accomplishing a complex goal\ldots{}

\hypertarget{what-is-data-analysis}{%
\section{What is data analysis?}\label{what-is-data-analysis}}

Data analysis altogether is\ldots{}

\hypertarget{what-are-data-disasters}{%
\section{What are data disasters?}\label{what-are-data-disasters}}

Data disasters occur when\ldots{}

\hypertarget{part-data}{%
\part*{Data}\label{part-data}}


\hypertarget{data-dall}{%
\chapter{Data Dalliances (WIP)}\label{data-dall}}

The first step to data analysis is, in fact, data. While this may seem obvious, statistics textbooks often dodge this detail.
Discussions of regression analysis often begin with a statement like:

\begin{quote}
``Let \(X\) be the \(n x p\) design matrix of independent variables\ldots{}''
\end{quote}

but in practice this statement is as absurd as writing a book about how to win a basketball game, assuming your team already has a 20 point lead with 1 minute left to play.

It's very convenient but typically incorrect to assume that the data we happen to have is the ideal (or, more humbly, sufficient) data for the questions we wish to analyze.
The specific vagaries of data vary greatly by domain, but a commonality across many fields (such as political science, economics, epidemiology, and market research) is that we are often called to work with \emph{found data} (or, more formally, ``observational data'') from administrative sources or production systems.
In contrast to artisanally crafted data experimental data (like the carefully controlled agricultural experiments which motivated many early methods developments in statistics), this data was generated neither by us nor for us.
To quote Angela Bassa, the head of data science at an e-commerce company: ``Data isn't ground truth. Data are artifacts of systems'' \citep{bassa_2017}.

The analytical implications of observational versus experimental data are well explored in the field of causal inference (which we will discuss some in Chapters \ref{incr-infe} and \ref{cava-caus}).
However, this distinction has implications far earlier in the data analysis process, as well.
To name a few:

\begin{itemize}
\tightlist
\item
  Records and fields many not represent the entities or measures most conducive to analysis
\item
  Data collection methods may capture a different subset of events or do so at a different frequency than we expected, leading to systemic biases
\item
  Data movement between systems can insert errors (or, at minimum, challenges to our intuition)
\item
  Data transformations may be fragile or transient, reflecting the primary purpose of the system not our unrelated analytical use
\end{itemize}

In this chapter, we will explore data structures and the full data generating process to better understand how different types of data challenges emerge.
In doing so, we will hone sharper intuition for how our data can deceive us and what to watch out for when beginning an analysis.

\hypertarget{preliminaries}{%
\section{Preliminaries}\label{preliminaries}}

Before we begin our exploration of data dalliances, we must first establish a baseline understanding of data structure, data production, and data quality.

\hypertarget{data-structure-basics}{%
\subsection{Data Structure Basics}\label{data-structure-basics}}

\hypertarget{relational-data-structure}{%
\subsubsection{Relational Data Structure}\label{relational-data-structure}}

Understanding the content and structure of the data you are using is a critical prerequisite to analysis.
In this book, we focus on tabular, structured data like one might find in an Excel spreadsheet or relational database.\footnote{Other types of data that one might encounter in the wild include free text, JSON, spatial data, and more. However, many of these require either more advanced analysis techniques or structuring that converts them into tabular data, so they are out of the scope of this discussion.}

In particular, many tools work best with what R developer Hadley Wickham describes as ``tidy data'' \citep{wickham_tidy}. Namely:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable forms a column
\item
  Each observation forms a row
\item
  Each type of observational unit forms a table
\end{enumerate}

This is analogous to how one generally finds data arranged in a database and how statisticians are used to conceptualizing it.
For example, the design matrix of a linear model consists of one column of data for each independent variable to be included in the model and one row for each observation.\footnote{When data is arrange this way in a matrix \(X\), linear regression coefficients can be computed as \(\beta = (\mathbf{X}^\intercal\mathbf{X})^{-1}\mathbf{X}^\intercal \mathbf{y}\)}
As Wickham points out, this is also similar to what is called ``3rd normal form'' in the world of relational database management systems.

Using this data structure is valuable not only because it is similar to what many modern data tools expect, but also because it provides us a framework to think critically about what defined each observation and each variable in our dataset.

\hypertarget{schemas-todo}{%
\subsubsection{Schemas (TODO)}\label{schemas-todo}}

\hypertarget{data-production-processes}{%
\subsection{Data Production Processes}\label{data-production-processes}}

In statistical modeling we discuss the \textbf{data generating process}: we can build models that describe the mechanisms that create our observations.
We can broaden this notion to think about the generating process of each of these steps of data production.

Regardless of the type of data (experimental, observational, survey, etc.), there are generally four main steps to production: collection, extraction, loading, and transformation.\footnote{You may hear the last three referred to as ELT or ETL}

\begin{itemize}
\tightlist
\item
  \textbf{Collect}: The way in which signals from the real world are captured as data. This could include logging (e.g.~for web traffic or system monitoring), sensors (e.g.~temperature collection), surveys, and more
\item
  \textbf{Extract}: The process of removing data from the place in which it was originally captured in preparation of moving it somewhere in which analysis can be done
\item
  \textbf{Load}: The process of loading the extracted data to its final destination
\item
  \textbf{Transform}: The process of modeling and transforming data so that its structure is useful for analysis and its variables are interpretable
\end{itemize}

To better theorize about data quality issues, it's useful to think of four DGPs (as shown in Figure \ref{fig:dgp}): the real-world DGP, the data collection/extraction DGP\footnote{I don't mean to imply statisticians do not regularly think about the data collection DGP! The rich literature on missing data imputation, censored data in survival analysis, and non-response bias is survey data collection are just a few examples of how carefully statisticians think about how data collection impacts analysis. I chose to break it out here to discuss the more technical aspects of collection}, the data loading DGP, and the data transformation DGP.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/data-dall/dgp} 

}

\caption{A schematic of the data production process}\label{fig:dgp}
\end{figure}

\hypertarget{e-commerce-data-example}{%
\subsubsection{E-Commerce Data Example}\label{e-commerce-data-example}}

For example, consider the role of each of these four DGPs for e-commerce data:

\begin{itemize}
\tightlist
\item
  \textbf{Real-world DGP}: Supply, demand, marketing, and a range of factors motivate a consumer to visit a website and make a purchase
\item
  \textbf{Data collection DGP}: Parts of the website are instrumented to log certain customer actions. This log is then extracted from the different operational system (login platforms, payment platforms, account records) to be used for analysis
\item
  \textbf{Data loading DGP}: Data recorded by different systems is moved to a data warehouse for further processing through some sort of manual, scheduled, or orchestrated job. These different systems may make data available at different frequencies.
\item
  \textbf{Data transformation DGP}: To arrive at that final data presentation requires creating a \href{https://en.wikipedia.org/wiki/Data_model}{data model} to describe domain-specific attributes with key variables crafted with data transformations
\end{itemize}

\hypertarget{subway-data-example}{%
\subsubsection{Subway Data Example}\label{subway-data-example}}

Or, consider the role of each of these four DGPs for subway ridership data\footnote{Like NYC's infamously messy \href{http://web.mta.info/developers/turnstile.html}{turnstile data}. I don't claim to know precisely how this dataset is created, but many of the specific challenges it contains are highly relevant.}:

\begin{itemize}
\tightlist
\item
  \textbf{Real-world DGP}: Riders are motivated to use public transportation to commute, run errands, or visit friends. Different motivating factors may cause different weekly and annual seasonality
\item
  \textbf{Data collection DGP}: To ride the subway, riders go to a station and enter and exit through turnstiles. The mechanical rotation of the turnstile caused by a rider passing through is recorded
\item
  \textbf{Data loading DGP}: Data recorded at each turnstile is collected through a centralized computer system at the station. Once a week, each station uploads a flat file of this data to a data lake owned by the city's Department of Transportation
\item
  \textbf{Data transformation DGP}: Turnstiles from different companies may have different data formats. Transformation may include harmonizing disparate sources, coding system-generated codes (e.g.~Station XYZ) to semantically meaningful names (e.g.~Main Street Station), and publishing a final unified representation across stations and across time
\end{itemize}

Throughout this chapter, we'll explore how understanding key concepts about each of these DGPs can help guide our intuition on where to look for problems.

\hypertarget{data-quality-dimension}{%
\subsection{Data Quality Dimension}\label{data-quality-dimension}}

To guide our discussion of how data production can affect aspects of data quality, we need a guiding definition of data quality.
This is challenging because data quality is \emph{subjective} and \emph{task-specific}.
It matters much more if data is ``fit for purpose'' and operates in a way that is \emph{transparent} to its users moreso than meeting some preordained quality standard.

Regardless, it's useful for our discussion to think about general dimensions of data quality. Here, we will rely on six dimensions of data quality outlined by Data Management Association \citep{dama}.
Their official definitions are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Completeness}: The proportion of stored data against the potential of ``100\% complete''
\item
  \textbf{Uniqueness}: Nothing will be recorded more than once based upon how that thing is identified. It is the inverse of an assessment of the level of duplication
\item
  \textbf{Timeliness}: The degree to which data represent reality from the required point in time
\item
  \textbf{Validity}: Data are valid if it conforms to the syntax (format, type, range) of its definition
\item
  \textbf{Accuracy}: The degree to which data correctly describes the ``real world'' object or event
  being described.
\item
  \textbf{Consistency}: The absence of difference, when comparing two or more representations of a
  thing against a definition
\end{enumerate}

\hypertarget{questions-to-ask-todo}{%
\subsection{Questions to Ask (TODO)}\label{questions-to-ask-todo}}

Our goal of understanding data is to ensure can assess its data quality and fit for our purpose. Understanding both its structure and its production process helps to accomplish this.

\hypertarget{data-collection}{%
\section{Data Collection}\label{data-collection}}

One of the tricky nuances of data collection is understanding what precisely is getting captured and logged in the first place.
No matter how robust the sensors, loggers, or other mechanisms are that record our dataset, that data is still unfit for its purpose so long as the analyst does not fully understand what it represents.
In the next section, we will see how what data gets collected (and our understanding of it) can alter our notions of data completion and how we must handle it in our computations.

\hypertarget{what-makes-a-record-row}{%
\subsection{What Makes a Record (Row)}\label{what-makes-a-record-row}}

The first priority when starting to work with a dataset is understanding what a single record (row) represents and what causes it to be generated.

Consider something as simple as a login system where users must enter their credentials, endure a Captcha-like verification process to prove that they are not a robot, and enter a multi-factor authentication code. Figure \ref{fig:login-log} depicts such a process.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/data-dall/login-log} 

}

\caption{A diagram illustrating a multi-step process for a user to login to a website or app}\label{fig:login-log}
\end{figure}

Which of these events gets collected and recorded has a significant impact on subsequent data processing.
In a technical sense, no inclusion/exclusion decision here is \emph{incorrect}, per say, but if the producers' choices don't match the consumers' understandings, it can lead to misleading results.

For example, an analyst might seek out a \texttt{logins} table in order to calculate the rate of successful website logins.
Reasonably enough, they might compute this rate as the sum of successful events over the total.
Now, suppose two users attempt to login to their account, and ultimately, one succeeds in accessing their private information and the other doesn't.
The analyst would probably hope to compute and report a 50\% login success rate. However, depending on how the data is represented, they could quite easily compute nearly any value from 0\% to 100\%.

As a thought experiment, we can consider what types of events might be logged:

\begin{itemize}
\tightlist
\item
  \textbf{Per Attempt}: If data is logged once per overall login attempt, successful attempts only trigger one event, but a user who forgot their password may try (and fail) to login multiple times. In the case illustrated above, that deflates the successful login rate to \textbf{25\%}.
\item
  \textbf{Per Event}: If the logins table contains a row for every login-related event, each `success' will trigger a large number of positive events and each `failure' will trigger a negative event preceded by zero or more positive events. In the case illustrated below, this inflates our successful login rate to \textbf{86\%}.
\item
  \textbf{Per Conditional}: If the collector decided to only look at downstream events, perhaps to circumvent record duplication, they might decide to create a record only to denote the success or failure of the final step in the login process (MFA). However, login attempts that failed an upstream step would not generate any record for this stage because they've already fallen out of the funnel. In this case, the computed rate could reach \textbf{100\%}
\item
  \textbf{Per Intermediate}: Similarly, if the login was defined specifically as successful password verification, the computed rate could his \textbf{100\%} even if some users subsequently fail MFA
\end{itemize}

These different situations are further illustrated in Figure \ref{fig:login-rate}, and their calculations are shown in Table \ref{tab:login-rate-tbl}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/data-dall/login-rate} 

}

\caption{Login events recorded under different data collection paradigms}\label{fig:login-rate}
\end{figure}

\begin{table}

\caption{\label{tab:login-rate-tbl}Success rate naively computed under different data collection schemes}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
  & Session & Attempt & Event & Outcome & Intermediate\\
\hline
Success & 1 & 1 & 6 & 1 & 2\\
\hline
Total & 2 & 4 & 7 & 1 & 2\\
\hline
Rate (\%) & 50 & 25 & 86 & 100 & 100\\
\hline
\end{tabular}
\end{table}

(Note that we did not explicitly ``simulate'' data here, but the workflow is largely the same. We imagine a real-world process, how that might translate into a digital representation, and created a numerical example to understand the implications. Not all simulations require fancy code; sometimes paper and a pencil or a thought experiment works just fine.)

While humans have a shared intuition of what concepts like a user, session, or login are, the act of collecting data forces us to map that intuition onto an atomic event.
Any misunderstanding in precisely what that definition is can have massive impact on the perceived data quality; ``per event'' data will appear heavily duplicated if it is assumed to be ``per session'' data.

In some cases, this could be obvious to detect.
If the system outputs fields that are incredibly specific (e.g.~with some hyperbole, imagine a \texttt{step\_in\_the\_login\_process} field with values taking any of the human-readable descriptions of the fifteen processes listed in the image above), but depending how this source is organized (e.g.~in contrast to above, if we only have fields like \texttt{sourceid} and \texttt{processid} with unintuitive alphanumeric encoded values) and defined, it could be nearly impossible to understand the nuances without uncovering quality metadata or talking to a data producer.

\hypertarget{what-doesnt-make-a-record-row}{%
\subsection{What Doesn't Make a Record (Row)}\label{what-doesnt-make-a-record-row}}

Along with thinking about what \emph{does} count (or gets logged), it's equally important to understand what systemically does not generate a record. Consider users who have the intent or desire to login (motivated by a real-world DGP) but cannot find the login page, or users who load the login page but never click a button because they know that they've forgotten their password and see no way to request it.
Often, some of these corner cases may be some of the most critical and informative (e.g.~here, demonstrating some major flaws in our UI).
It's hard to \emph{computationally} validate what data doesn't exist, so \emph{conceptual} data validation is critical.

\hypertarget{records-versus-keys}{%
\subsection{Records versus Keys}\label{records-versus-keys}}

The preceding discussion on what types of real-world observations will or will not generate records in our resulting dataset is related to but distinct from another important concept from the world of relational databases: \textbf{primary keys}.

\textbf{Primary keys} are a minimal subset of variables in a dataset than define a unique record.
For example, in the previous discussion of customer logins this might consist of \textbf{natural keys}\footnote{Keys with semantic meanings that are naturally part of the dataset} such as the combination of a \texttt{session\_id} and a \texttt{timestamp} or \textbf{surrogate keys}\footnote{Keys without semantic meaning that exist primarily for the purpose of being keys} such as a global \texttt{event\_id} that is generated every time the system logs any event.

Understanding a table's primary keys can be useful for many reasons.
To name a few reasons, these fields are often useful for linking data from one table to another and for identifying data errors (if the uniqueness of these fields are not upheld).
They also can be suggestive of the true granularity of the table.

However, simply knowing a table's primary keys does \emph{not} resolve the issues we discussed in the prior two sections.
Any of the many different data collection strategies we considered are \emph{unique} by session and timestamp;
however, as we've seen, that is no guarantee that they \emph{must} contain every session and timestamp in the universe of events.

\hypertarget{what-defines-a-variable-column}{%
\subsection{What Defines a Variable (Column)}\label{what-defines-a-variable-column}}

Just as critical as understanding what constitutes a record (row) in a dataset is understanding the precise definition of each variable (column).
Superficially, this task seems easier: after all, each variable has a name which hopefully includes some semantic information. However, quite often this information can provide a false sense of security.
Just because you identify a variable with a promising sounding name, that does not mean that it is the most relevant data for your analysis.

For example, consider wanting to analyze patterns in customer spend amounts across orders on an e-commerce website.
You might find a table of orders with a field called \texttt{amt\_spend}. But what might this mean?

\begin{itemize}
\tightlist
\item
  If the dataset is sourced from a payment processor, it likely includes the total amount billed to a customers' credit card: including item prices less any discounts, shipping costs, taxes, etc. Alternatively, if this order was split across a gift card and a credit card, this field might only reflect the amount charged to the credit card
\item
  If the dataset is created for Finance, it might perhaps include only the total of item prices less discounts if this best corresponded to the data the Finance team needs for revenue reporting
\item
  Someone, somewhere, at some point might have assigned \texttt{amt\_spend} to the name of the variable containing gross spend (before accounting for any discounts) and there might be a different variable \texttt{amt\_spend\_net} which accounts for discounts applied
\end{itemize}

It's critical to understand what each variable \emph{actually} means.
The upside of this is that it forces analysts to think more crisply about their research questions and what the \emph{ideal} variables for their analysis would be.
As we've seen, concepts like ``spend'' may seem deceptively simple, but are not unambiguous.

\hypertarget{data-extraction-loading}{%
\section{Data Extraction \& Loading}\label{data-extraction-loading}}

Checking that data contains expected and \emph{only} expected records (that is, completeness, uniqueness, and timeliness) is one of the most common first steps in data validation.
However, the superficially simple act of loading data into a data warehouse or updating data between tables can introduce a variety of risks to data completeness which require different strategies to detect.
Data loading errors can result in data that is stale, missing, duplicate, inconsistently up-to-date across sources, or complete but for only a subset of the range you think.

While the data quality principles of \textbf{completeness}, \textbf{uniqueness}, and \textbf{timeliness} would suggest that records should exist once and only once, the reality of many haphazard data loading process means data may appear sometime between zero and a handful of times. Data loads can occur in many different ways.
For example, they might be:

\begin{itemize}
\tightlist
\item
  manually executed
\item
  scheduled (like a \href{https://en.wikipedia.org/wiki/Cron}{cron} job)
\item
  orchestrated (with a tool like \href{https://airflow.apache.org/}{Airflow} or \href{https://www.prefect.io/}{Prefect})
\end{itemize}

No approach is free from challenges.
For example, scheduled jobs risk executing before an upstream process has completed (resulting in stale or missing data);
poorly orchestrated jobs may be prevented from working due to one missing dependency or might allow multiple stream to get out of sync (resulting in multisource missing data).
Regardless of the method, all approaches must be carefully configured to handle failures gracefully to avoid creating duplicates, and the frequency at which they are executed may cause partial loading issues if it is incompatible with the granularity of the source data.

\hypertarget{data-load-failure-modes}{%
\subsubsection{Data Load Failure Modes}\label{data-load-failure-modes}}

To develop our understanding of the true data generating process and to formulate theories on how our data could be broken (and what we should validate), it is useful to understand the different ways data extraction and loading can fail.

Figure \ref{fig:data-load} illustrates a number of examples. Suppose that each row of boxes in the diagram represents one day of records in a table.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/data-dall/data-load} 

}

\caption{Different modes of data loading failure}\label{fig:data-load}
\end{figure}

Our dataset might be susceptible to:

\begin{itemize}
\tightlist
\item
  \textbf{Stale data} occurs when the data is not as up-to-date as would be expected from is regular refresh cadence. This could happen if a manual step was skipped, a scheduled job was executed before the upstream source was available, or orchestrated data checks found errors and quarantined new records
\item
  \textbf{Missing data} occurs when one data load fails but subsequent loads have succeeded
\item
  \textbf{Duplicate data} occurs when one data load is executed multiple times
\item
  \textbf{Multisource missing data} occurs when a table is loaded from multiple sources, and some have continued to update as expected while others have not
\item
  \textbf{Partial data} occurs when a table is loaded correctly as intended by the producer but contains less data than expected by the consumer (e.g.~a table loads ever 12 hours but because there is some data for a given date, the user assumes that all relevant records for that date have been loaded)
\end{itemize}

The differences in these failure modes become important when an analyst attempts to assess data completeness.
One of the first approaches an analyst might consider is simply to check the \texttt{min()} and \texttt{max()} event dates in their table.
However, this can only help detect stale data.
To catch missing data, an analyst might instead attempt to count the number of \texttt{distinct} days represented in the data; to detect duplicate data, that analyst might need to count records by day and examine the pattern.

\begin{longtable}[]{@{}llllll@{}}
\toprule
\begin{minipage}[b]{0.14\columnwidth}\raggedright
Metric\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedright
Stale\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedright
Missing\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedright
Duplicate\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedright
Multi\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedright
Partial\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.14\columnwidth}\raggedright
\texttt{min(date)} \texttt{max(date)}\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\textbf{13}\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
14\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
14\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
14\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
14\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\raggedright
\texttt{count(distinct\ date)}\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\textbf{3}\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\textbf{3}\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
4\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
4\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
4\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\raggedright
\texttt{count(1)\ by\ date}\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\textbf{1001001000}\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\textbf{1001000100}\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\textbf{100100200100}\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
1001006666\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
10010010050\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\raggedright
\texttt{count(1)} \texttt{count(distinct\ PKs)}\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
300300\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
300300\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\textbf{400300}\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
332332\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
350350\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

In a case like the toy example above where the correct number of rows per date is highly predictable and the number of dates is small, such eyeballing is feasible;
however when the expected number of records varies day-to-day or time series are long, this approach becomes subjective, error-prone, and intractable.
Additionally, it still might be hard to catch errors in mutli-source data or partial loads if the lower number of records was still within the bounds of reasonable deviation for a series.
These last two types deserve further exploration.

\hypertarget{multi-source}{%
\subsubsection{Multi-Source}\label{multi-source}}

A more effective strategy for assessing data completeness requires a better understanding of how data is being collected and loaded.
In the case of multi-source data, one single source stopping loading may not be a big enough change to disrupt aggregate counts but could still jeopardize meaningful analysis.
It would be more useful to conduct completeness checks by \emph{subgroup} to identify these discrepancies.

But not any subgroup will do;
the subgroup must correspond to the various data sources.
For example, suppose we run an e-commerce store and wish to look at sales from the past month by category.
Naturally, we might think to check the completeness of the data by category.
But what if sales data is sourced from three separate locations: our Shopify site (80\%), our Amazon Storefront (15\%), and phone sales (5\%).
Unless we explicitly check completeness by channel (a dimension we don't particularly care about for our analysis), it would be easy to miss if our data source for phone sales has stopped working or loads at a different frequency.

Another interesting aspect of multi-source data, is multiple sources can contribute either to different \emph{rows/records} or different \emph{columns/variables}.
Table-level frequency counts won't help us in the latter case since other sources might create the right total number of records but result in some specific fields in those records being missing or inaccurate.

\hypertarget{partial-loads}{%
\subsubsection{Partial Loads}\label{partial-loads}}

Partial loads really are not data errors at all, but are still important to detect since they can jeopardize an analysis.
A common scenario might occur if a job loads new data every 12 hours (say, data from the morning and afternoon of day n-1 loads on day n at 12AM and 12PM, respectively).
An analyst retrieving data at 11AM may be concerned to see an approximate \textasciitilde50\% drop in sales in the past day, despite confirming that their data looks to be ``complete'' since the maximum record date is, in fact, day n-1.
Of course, this concern could be somewhat easily allayed if they then checked a timestamp field, but such a field might not exists or might not have been used for validation since its harder to anticipate the appropriate maximum timestamp than it is the maximum date.

\hypertarget{delayed-or-transient-records}{%
\subsubsection{Delayed or Transient Records}\label{delayed-or-transient-records}}

The interaction between choices made in the data collection and data loading phases can introduce their own sets of problems.

Consider an \texttt{orders} table for an e-commerce company that analysts may use to track customer orders.
It might contain one record per \texttt{order\_id} x \texttt{event} (placement, processing, shipment), one record per order placed, one record per order shipping, or one record per order with a \texttt{status} field that changes over time to denote the order's current stage of life. Some of these options are illustrated in Figure \ref{fig:order-log}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/data-dall/order-log} 

}

\caption{Illustration of alternative data collection and extraction strategies for order data}\label{fig:order-log}
\end{figure}

Any of these modeling choices seem reasonable and the difference between them might appear immaterial.
But consider the \emph{collection} choice to record and report \emph{shipped} events.
Perhaps this might be operationally easier if shipment come from one source system whereas orders could come from many.
However, an interesting thing about shipments is that they are often lagged in a variable way from the order date.

Suppose the e-commerce company in question offers three shipping speeds at checkout. Figure \ref{fig:order-date} shows the range of possible shipment dates based on the order dates for the three different speeds (shown in different bars/colors).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/data-dall/order-date} 

}

\caption{A conceptual chart of when different classes of real-world events might materialize as records in our dataset}\label{fig:order-date}
\end{figure}

How might this effect our perceived data quality?

\begin{itemize}
\tightlist
\item
  Order data could appear \textbf{stale} or not timely since orders with a given \texttt{order\_date} would only load days later once shipped
\item
  Similar to \textbf{missing} or \textbf{multisource} data, the data \emph{range} in the table could lead to deceptive and incomplete data validation because some orders from a later order date might ship (and thus be logged) before all orders from a previous order date
\item
  Put another way, we could have multiple order dates demonstrating \textbf{partial} data loads
\item
  These features of the data might behave inconsistently across time due to seasonality (e.g.~no shipping on weekends or federal holidays), so heuristics developed to clean the data based on a small number of observations could fail
\item
  From an analytical perspective, orders with faster shipping would be disproportionately overrepresented in the ``tail'' (most recent) data. If shipping category correlated with other characteristics like total order spend, this could create an artificial trend in the data
\end{itemize}

Once again, understanding that data is \emph{collected} at point of shipment and reasoning how shipment timing varies and impacts \emph{loading} is necessary for successful validation.

If this thought experiment seems to vague, we can make it more concrete by mocking up a dataset with which to experiment.

In the simplest version, we will simply suppose one order is submited on each of 10 days with dates (represented for convenience as integers and not calendar dates) given by the \texttt{dt\_subm} vector.
Suppose shipping always takes three days, so we can easily calculate the shipment date (\texttt{dt\_ship}) based on the submission date.
The shipment date is the same as the date the data will be logged and loaded (\texttt{dt\_load}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# data simulation: single orders + deterministic ship dates {-}{-}{-}{-}}
\NormalTok{dt\_subm }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}
\NormalTok{days\_to\_ship }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{dt\_ship }\OtherTok{\textless{}{-}}\NormalTok{ dt\_subm }\SpecialCharTok{+}\NormalTok{ days\_to\_ship}
\NormalTok{dt\_load }\OtherTok{\textless{}{-}}\NormalTok{ dt\_ship}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(dt\_subm, dt\_ship, dt\_load)}
\FunctionTok{head}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   dt_subm dt_ship dt_load
## 1       1       4       4
## 2       2       5       5
## 3       3       6       6
## 4       4       7       7
## 5       5       8       8
## 6       6       9       9
\end{verbatim}

Suppose we are an analyst living in day 5 and wonder how many orders were submitted on day 3.
We can observe all shipments loaded before day 5 so we filter our data accordingly.
However, when we count how many records exist for day 3 we find none.
Instead, when we move ahead to an analysis date of day 7, we are able to observe the orders submitted on day 3.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\CommentTok{\# how many day{-}3 orders do we observe as of day{-}5? {-}{-}{-}{-}}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dt\_load }\SpecialCharTok{\textless{}=} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dt\_subm }\SpecialCharTok{==} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{nrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# how many day{-}3 orders do we observe as of day{-}7? {-}{-}{-}{-}}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dt\_load }\SpecialCharTok{\textless{}=} \DecValTok{7}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dt\_subm }\SpecialCharTok{==} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{nrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

(Note that these conditions could be checked much more succinctly with a base R expression such as \texttt{sum(df\$dt\_load\ \textless{}\ 7\ \&\ df\$dt\_subm\ ==\ 3)}.
However, there is sometimes virtue in option for more readable code even if it is less compact.
Here, we prefer the more verbose option for the claritfy of our exposition.
Such trade-offs, and general thoughts on coding style, are explored further in Chapter \ref{comp-code}.)

Now, this may seem to trivial. Clearly, if there were \emph{zero} records for a day, we would catch this in data validation, right?
We can make our synthetic data slightly more realistic to better illustrate the problem.
Let's not imagine that there are 10 orders each day, and each order is shipped sometime between 2 and 4 days after the order with equal probability.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# data simulation: multiple orders + random ship dates {-}{-}{-}{-}}
\NormalTok{dt\_subm }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{each =} \DecValTok{10}\NormalTok{)}
\NormalTok{days\_to\_ship }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{size =} \FunctionTok{length}\NormalTok{(dt\_subm), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{dt\_ship }\OtherTok{\textless{}{-}}\NormalTok{ dt\_subm }\SpecialCharTok{+}\NormalTok{ days\_to\_ship}
\NormalTok{dt\_load }\OtherTok{\textless{}{-}}\NormalTok{ dt\_ship}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(dt\_subm, dt\_ship, dt\_load)}
\FunctionTok{head}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   dt_subm dt_ship dt_load
## 1       1       5       5
## 2       1       4       4
## 3       1       3       3
## 4       1       5       5
## 5       1       5       5
## 6       1       5       5
\end{verbatim}

When we repeat the prior analysis, we now see that we have \emph{some} records for orders submitted on day 3 by the time we begin analysis on day 5.
In this case, we might be more easily tricked to believe this is \emph{all} orders.
However, when we repeat the analysis on day 7, we see the the number of orders on day 3 has increased.

Of course, you can imagine the real world is yet much more complicated than this example.
In reality, we would have a random number of orders each day.
Additionally, we might have a \emph{mixture} of different \emph{types} of orders.
There might be high-priced orders where customers tended to be willing to pay for faster shipping,
and low-priced orders where customers tend to chose slower shipping.
In a case like this, not only might naive validation miss the lack of data completeness, but the \emph{sample} of shipments we begin to see on day 5 could be unrepresentative of the population of orders placed on day 3.
This is a type of \textbf{selection bias} that we will examine further in Chapter \ref{incr-infe} (Incredible Inferences).

\hypertarget{data-encoding-transformation-wip}{%
\section{Data Encoding \& Transformation (WIP)}\label{data-encoding-transformation-wip}}

Once data is loaded into a more suitable location for processing and analysis (such as a data warehouse), it often undergoes numerous transformations to change its shape, structure, and content to be more suited for analytical use.

For example, recall the \texttt{logins} table that we discussed above. It might be filtered to a cleaner versions which represents only a subset of events, or event identifiers like \texttt{\textquotesingle{}1436\textquotesingle{}} might be recoded to more human-readable names like \texttt{\textquotesingle{}mfa-success\textquotesingle{}}.

Unfortunately, although these steps attempt to increase the data's usability, they are also not immune to inserting bugs.

\hypertarget{data-encoding}{%
\subsection{Data Encoding}\label{data-encoding}}

\hypertarget{data-types}{%
\subsubsection{Data Types}\label{data-types}}

One critical set of decisions in data encoding is what sort of \emph{data types} each field of interest should be. Data types such as integers, reals, character strings, logicals, dates, and times determine how data is stored and the types of manipulations that can be done to it.

We'll see more examples of the computational implications for our data types in Chapter \ref{comp-quan} (Computational Quandaries). This chapter specifically explores the unique complexities of string and date types.

\hypertarget{indicator-variables-todo}{%
\subsubsection{Indicator Variables (TODO)}\label{indicator-variables-todo}}

what is positive case?

\begin{quote}
``We had a bunch of zeros that should have been coded ones and the ones should have been coded zeroes.''
\end{quote}

(\url{https://retractionwatch.com/2013/01/04/paper-on-evidence-for-environmental-racism-in-epa-polluter-fines-retracted-for-coding-error/})

\hypertarget{general-representation-todo}{%
\subsubsection{General Representation (TODO)}\label{general-representation-todo}}

\begin{quote}
``These data sets often have multiple files that\ldots have unclear and sometimes duplicative variables. Such complexities are commonplace among many data systems\ldots{} I would not be surprised if coding errors were fairly common, and that the ones discovered constitute only the ``tip of the iceberg.''
''
\end{quote}

(\url{https://retractionwatch.com/2015/09/10/divorce-study-felled-by-a-coding-error-gets-a-second-chance/})

\hypertarget{the-many-meanings-of-null}{%
\subsubsection{The Many Meanings of Null}\label{the-many-meanings-of-null}}

Another major encoding decision is how to handle null values.
Previously, in the discussion of Data Collection, we considered the presence and absence of full \emph{records}.
However, when preparing data for analysis, both data produces and consumers need to decide how to cope with the presence or absence of individual \emph{fields}.

If records contain some but not all relevant information, they may be published with explicitly missing fields or the full record may not be published at all. The difference between implicit and explicit missingness on the resulting data is illustrated in Figure \ref{fig:null-impl-expl}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/data-dall/missing-imp-exp} 

}

\caption{A comparison of explicit versus implicit missingness}\label{fig:null-impl-expl}
\end{figure}

Understanding what the system implies by each \emph{explicitly} missing data field is also critical for validation and analysis.
Checks for data completeness usually include counting null values, but null data isn't always incorrect.
In fact, null data can be highly informative if we know what it means. Some meanings of null data might include:

\begin{itemize}
\tightlist
\item
  \textbf{Field is not relevant}: Perhaps our \texttt{logins} table reports the mobile phone operating system (iOS or Android) that was used to access the login page to track platform-specific issues. However, there is no valid value for this
\item
  \textbf{Relevant value is not known}: Our \texttt{logins} table might also have an \texttt{account\_id} field which attempts to match login attempts to known accounts/customers using different metadata like cookies or IP addresses. In theory, almost everyone trying to log in should have an account identifier, but our methods may not be good enough to identify them in all cases
\item
  \textbf{Relevant value is null}: Of course, sometimes someone without an account at all might try to log in for some reason. In this case, the correct value for an \texttt{account\_id} field truly \emph{is} null
\item
  \textbf{Relevant value was recorded incorrectly}: Sometimes systems have glitches. Without a doubt, every single login attempt \emph{should} have a timestamp, but such a field could be null if this data was somehow lost or corrupted at the source
\end{itemize}

Similarly, different systems might or might not report out these nulls in different ways such as:

\begin{itemize}
\tightlist
\item
  \textbf{True nulls}: Literally the entry in the resulting dataset is null
\item
  \textbf{Null-like non-nulls}: Blank values like an empty string (\texttt{\textquotesingle{}\textquotesingle{}}) that contain a null amount of information but won't be detected when counting null values
\item
  \textbf{Placeholder values}: Meaningless values like an \texttt{account\_id} of \texttt{00000000} for all unidentified accounts which preserve data \emph{validity} (the expected structure) but have no intrinsic meaning
\item
  \textbf{Sentinel/shadow values}: Abnormal values which attempt to indicate the reasons for null-ness such as an \texttt{account\_id} of \texttt{-1} when no browser cookies were found or \texttt{-2} when cookies were found but did not help link to any specific customer record
\end{itemize}

Each of these encoding choices changes the definitions of appropriate completeness and validity for each field and, even more critically, impacts the expectations and assertions we should form for data accuracy.
We can't expect 100\% completeness if nulls are a relevant value; we can't check validity of ranges as easily if sentinel values are used with values that are outside the normal range (hopefully, or we have much bigger problems!)
So, understanding how upstream systems \emph{should} work is essential for assessing if they \emph{do} work.

Similarly, understanding how our null data is collected has significant implications for how we subsequently process it. We will discuss this more in Chapter \ref{comp-quan} (Computational Quandaries).

\hypertarget{data-transformation}{%
\subsection{Data Transformation}\label{data-transformation}}

Finally, once the data is roughly where we want it, it likely undergoes many transformations to translate all of the system-generated fields we discussed in data collection into semantically-relevant dimensions for analytical consumers. Of course, the types of transformations that could be done are innumerable with far more variation than data loading. So, we'll just look at a few examples of common failure patterns.

\hypertarget{pre-aggregation}{%
\subsubsection{Pre-Aggregation}\label{pre-aggregation}}

Data transformations may include aggregating data up to higher levels of granularity for easier analysis. For example, a transformation might add up item-level purchase data to make it easier for an analyst to look at spend per \emph{order} of a specific user.

Data transformations not only transform our data, but they also transform how the dimensions of data quality manifest. If data with some of the \textbf{completeness} or \textbf{uniqueness} issues we discussed with data loading is pre-aggregated, these problems can turn into problems of \textbf{accuracy}. For example, the duplicate or partial data loads that we discussed when aggregated could suggest inaccurately high or low quantities respectively.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/data-dall/completeness-accuracy} \end{center}

\hypertarget{field-encoding}{%
\subsubsection{Field Encoding}\label{field-encoding}}

When we assess data consistency across tables,

Categorical fields in a data set might be created in any number of ways including:

\begin{itemize}
\tightlist
\item
  Directly taken from the source
\item
  Coded in a transformation script
\item
  Transformed with logic in a shared user-defined function (\href{https://docs.snowflake.com/en/sql-reference/user-defined-functions.html}{UDFs}) or \href{https://docs.getdbt.com/docs/building-a-dbt-project/jinja-macros/\#macros}{macro}
\item
  Joined from a shared look-up table
\end{itemize}

Each approach has different implications on data consistency and usability.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/data-dall/field-encoding} \end{center}

Using fields from the source simply is what it is -- there's no subjectivity or room for manual human error. If multiple tables come from the same source, it's likely but not guaranteed that they will be encoded in the same way.

Coding transformations in the ELT process is easy for data producers. There's no need to coordinate across multiple processes or use cases, and the transformation can be immediately modified when needed. However, that same lack of coordination can lead to different results for fields that should be the same.

Alternatively, macros, UDFs, and look-up tables provided centralized ways to map source data inputs to desired analytical data outputs in a systemic and consistent way. Of course, centralization has its own challenges. If something in the source data changes, the process of updating a centralized UDF or look-up table may be slowed down by the need to seek consensus and collaborate. So, data is more \emph{consistent} but potentially less \emph{accurate}.

Regardless, such engineered values require scrutiny -- paticularly if they are being used as a key to join multiple tables -- and the distinct values in them should be carefully examined.

\hypertarget{updating-transformations}{%
\subsubsection{Updating Transformations}\label{updating-transformations}}

Of course, data consistency is not only a problem across different data sources but within one data source. Regardless of the method of field encoding used in the previous step, the intersection of data loading and data transformation strategies can introduce data consistency errors over time.

Often, for computation efficiency, analytical tables are loaded using an \emph{incremental} loading strategy. This means that only new records (determined by time period, a set of unique keys, or other criteria) from the upstream source are loaded to the downstream table. This is in contrast to a \emph{full refresh} where the entire downstream table is recreated on each update.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/data-dall/incr-full-good} \end{center}

Incremental loads have many advantages. Rebuilding tables in entirety can be very time consuming and computationally expensive. In particular, in non-cloud data warehouses that are not able to scale computing power on demand, this sort of heavy duty processing job can noticeably drain resources from other queries that are trying to run in the database. Additionally, if the upstream staging data is ephemeral, fully rebuilding the table could mean failing to retain history.

However, in the case that our data transformations change, incremental loads may introduce inconsistency in our data overtime as only new records are created and inserted with the new logic.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/data-dall/incr-full-bad-col} \end{center}

This is also a problem more broadly if some short-term error is discovered either with data loading or transformation in historical data. Incremental strategies may not always update to include the corrected version of the data.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/data-dall/incr-full-good} \end{center}

Regardless, this underscores the need to validate entire datasets and to re-validate when repulling data.

\hypertarget{more-on-missing-data-todo}{%
\section{More on Missing Data (TODO)}\label{more-on-missing-data-todo}}

MCAR / MAR / MNAR

\hypertarget{other-types-of-data-todo}{%
\section{Other Types of Data (TODO)}\label{other-types-of-data-todo}}

\hypertarget{survey-data}{%
\subsection{Survey Data}\label{survey-data}}

\hypertarget{human-generated}{%
\subsection{Human-Generated}\label{human-generated}}

\hypertarget{strategies-todo}{%
\section{Strategies (TODO)}\label{strategies-todo}}

\hypertarget{understand-the-intent}{%
\subsection{Understand the intent}\label{understand-the-intent}}

\begin{itemize}
\tightlist
\item
  Why was data originally collected? By whom and for what purpose?
\item
  Are there clear, unique definitions of key concepts (e.g.~entites, measures)? What are they?
\item
  Documentation (metadata, dictionaries)
\end{itemize}

\hypertarget{understand-the-execution}{%
\subsection{Understand the execution}\label{understand-the-execution}}

\begin{itemize}
\tightlist
\item
  learn about the real-world systems
\item
  understand key steps in data production process
\item
  documentation (lineage, provenance)
\end{itemize}

\hypertarget{seek-expertise}{%
\subsection{Seek expertise}\label{seek-expertise}}

\begin{itemize}
\tightlist
\item
  talking to experts (upstream and downstream)
\end{itemize}

\hypertarget{trust-but-verify}{%
\subsection{Trust but verify}\label{trust-but-verify}}

\begin{itemize}
\tightlist
\item
  always on data validation

  \begin{itemize}
  \tightlist
  \item
    summaries
  \item
    context-informed assertions
  \end{itemize}
\item
  exploratory analysis
\end{itemize}

\hypertarget{real-world-disasters}{%
\section{Real World Disasters}\label{real-world-disasters}}

\hypertarget{data-loading-artificially-spikes-covid-cases}{%
\subsection{Data loading artificially spikes COVID cases}\label{data-loading-artificially-spikes-covid-cases}}

Data loading, particularly when a new process is introduced can go astray and lead to spurious results.
The city of El Paso discovered this after reporting an exceedingly high number of daily COVID cases \citep{borunda_2020}:

\begin{quote}
El Paso city and public health officials on Thursday admitted to a major blunder, saying that the more than 3,100 new cases reported a day earlier was incorrect.
\end{quote}

\begin{quote}
The number was the result of a two-day upload of cases in one day after the public health department went from imputing data manually to an automatic data upload that was intended to increase efficiency, Public Health Director Angela Mora said at news conference.
\end{quote}

This instance demonstrates not only how \emph{data} goes wrong but also how easy it is to trust discrepancies and find explanations in real-world phenomena.
The article goes on to suggest that the extremely high case numbers were not immediately caught given the overall ``alarming'' levels:

\begin{quote}
Mora said she accepted responsibility and that she should have taken a deeper look after noticing the abnormally high number of new cases.El Paso County is still seeing more than 1,000 new cases on a daily basis, officials said. The correct number of new cases Wednesday was 1,537.
\end{quote}

\begin{quote}
``The numbers are still high and they are still alarming but they were not as high as we reported,'' Mora said.
\end{quote}

Similar data artifacts also arose from a variety of other factors besides system changes. For example, delays in 9-to-5 office paperwork (the real-world events triggering the creation of some key data elements) over the holidays led to more artificial trends in data reporting around the end-of-year holidays:

\begin{quote}
While 238 deaths is the most reported in Illinois since the start of the pandemic, the IDPH noted in its release that some data was, ``delayed from the weekends, including this past holiday weekend.''
\end{quote}

- \href{https://wgntv.com/news/coronavirus/illinois-sees-biggest-spike-in-reported-covid-19-deaths-to-date-after-officials-say-holidays-delayed-some-data/}{Illinois sees biggest spike in reported COVID-19 deaths to date after holidays delay some data, officials say}, WGN9 (TODO CITATION)

\hypertarget{data-encoding-leads-to-incorrect-bmi-calculation}{%
\subsection{Data encoding leads to incorrect BMI calculation}\label{data-encoding-leads-to-incorrect-bmi-calculation}}

Data that is encoded inconsistently may be mishandled in automated decision processes.
For example, data manually entered into electronic health records (EHR) by countless care provides might be compiled into a single source although the meanings could be inconsistent.

In the early days of the COVID19 vaccination push, some regions chose to prioritize their vaccine supply by certain individual characteristics including a high body mass index (BMI), which is a function of one's weight relative to their height.

One 6ft 2in tall man's data was misinterpreted to suggest that he was 6.2 \emph{centimeters} tall; this caused his to get high priority for a vaccine.

As described in the BBC \citep{bbc_north_west_2021}:

\begin{quote}
A man in his 30s with no underlying health conditions was offered a Covid vaccine after an NHS error mistakenly listed him as just 6.2cm in height. Liam Thorp was told he qualified for the jab because his measurements gave him a body mass index of 28,000.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# but in reality this is just a data loading/recording issue {-}{-}{-}{-}}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(dt\_order, dt\_ship) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(dt\_order) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(dt\_ship) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{n\_recorded =} \FunctionTok{cumsum}\NormalTok{(n)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ dt\_ship, }\AttributeTok{y =}\NormalTok{ n\_recorded, }\AttributeTok{col =} \FunctionTok{as.character}\NormalTok{(dt\_order))) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-20-2.pdf}

\hypertarget{data-transformation-1}{%
\subsection{Data Transformation}\label{data-transformation-1}}

Finally, once the data is roughly where we want it, it likely undergoes many transformations to translate all of the system-generated fields we discussed in data collection into semantically-relevant dimensions for analytical consumers. Of course, the types of transformations that could be done are innumerable with far more variation than data loading. So, we'll just look at a few examples of common failure patterns.

\hypertarget{pre-aggregation-1}{%
\subsubsection{Pre-Aggregation}\label{pre-aggregation-1}}

Data transformations may include aggregating data up to higher levels of granularity for easier analysis. For example, a transformation might add up item-level purchase data to make it easier for an analyst to look at spend per \emph{order} of a specific user.

Data transformations not only transform our data, but they also transform how the dimensions of data quality manifest. If data with some of the \textbf{completeness} or \textbf{uniqueness} issues we discussed with data loading is pre-aggregated, these problems can turn into problems of \textbf{accuracy}. For example, the duplicate or partial data loads that we discussed when aggregated could suggest inaccurately high or low quantities respectively.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/data-dall/completeness-accuracy} \end{center}

\hypertarget{field-encoding-1}{%
\subsubsection{Field Encoding}\label{field-encoding-1}}

When we assess data consistency across tables,

Categorical fields in a data set might be created in any number of ways including:

\begin{itemize}
\tightlist
\item
  Directly taken from the source
\item
  Coded in a transformation script
\item
  Transformed with logic in a shared user-defined function (\href{https://docs.snowflake.com/en/sql-reference/user-defined-functions.html}{UDFs}) or \href{https://docs.getdbt.com/docs/building-a-dbt-project/jinja-macros/\#macros}{macro}
\item
  Joined from a shared look-up table
\end{itemize}

Each approach has different implications on data consistency and usability.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/data-dall/field-encoding} \end{center}

Using fields from the source simply is what it is -- there's no subjectivity or room for manual human error. If multiple tables come from the same source, it's likely but not guaranteed that they will be encoded in the same way.

Coding transformations in the ELT process is easy for data producers. There's no need to coordinate across multiple processes or use cases, and the transformation can be immediately modified when needed. However, that same lack of coordination can lead to different results for fields that should be the same.

Alternatively, macros, UDFs, and look-up tables provided centralized ways to map source data inputs to desired analytical data outputs in a systemic and consistent way. Of course, centralization has its own challenges. If something in the source data changes, the process of updating a centralized UDF or look-up table may be slowed down by the need to seek consensus and collaborate. So, data is more \emph{consistent} but potentially less \emph{accurate}.

Regardless, such engineered values require scrutiny -- paticularly if they are being used as a key to join multiple tables -- and the distinct values in them should be carefully examined.

\hypertarget{updating-transformations-1}{%
\subsubsection{Updating Transformations}\label{updating-transformations-1}}

Of course, data consistency is not only a problem across different data sources but within one data source. Regardless of the method of field encoding used in the previous step, the intersection of data loading and data transformation strategies can introduce data consistency errors over time.

Often, for computation efficiency, analytical tables are loaded using an \emph{incremental} loading strategy. This means that only new records (determined by time period, a set of unique keys, or other criteria) from the upstream source are loaded to the downstream table. This is in contrast to a \emph{full refresh} where the entire downstream table is recreated on each update.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/data-dall/incr-full-good} \end{center}

Incremental loads have many advantages. Rebuilding tables in entirety can be very time consuming and computationally expensive. In particular, in non-cloud data warehouses that are not able to scale computing power on demand, this sort of heavy duty processing job can noticeably drain resources from other queries that are trying to run in the database. Additionally, if the upstream staging data is ephemeral, fully rebuilding the table could mean failing to retain history.

However, in the case that our data transformations change, incremental loads may introduce inconsistency in our data overtime as only new records are created and inserted with the new logic.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/data-dall/incr-full-bad-col} \end{center}

This is also a problem more broadly if some short-term error is discovered either with data loading or transformation in historical data. Incremental strategies may not always update to include the corrected version of the data.

\begin{center}\includegraphics[width=0.9\linewidth]{figures/data-dall/incr-full-good} \end{center}

Regardless, this underscores the need to validate entire datasets and to re-validate when repulling data.

\hypertarget{human-generated-data}{%
\section{Human-Generated Data}\label{human-generated-data}}

Ahhhh

\hypertarget{other-encoding-issues}{%
\section{Other Encoding Issues}\label{other-encoding-issues}}

\begin{itemize}
\tightlist
\item
  for indicators which is 1?
\item
  field values changing over time
\end{itemize}

\hypertarget{strategies}{%
\section{Strategies}\label{strategies}}

--\textgreater{}

\hypertarget{comp-quan}{%
\chapter{Computational Quandaries (WIP)}\label{comp-quan}}

After gaining confidence in one's data (or, at least, making peace with it), the next step in a data analysis is often to start cleaning and exploring that data with summary statistics, plots, and models.
Generally, this requires a computational tool like SQL, R, or python.

The process of computation itself can be fraught with challenges.
Computational tools are extremely literal; they are excellent at doing \emph{precisely what they were told to do} but not often what analysts might have \emph{meant} or \emph{wished} that they would do.
Additionally, the moment an analyst begins to use a tool, the conversation is no longer between them and the data;
suddenly, the mental model of how every single tool developer thought you might want to do analysis affects the tools' behaviors and the analysts' results.

In this chapter, we will explore common ways that tools may do something technically correct, reasonable, and as-intended but very much not what analysts may expect.
Along the way, we will see how computational methods interact with the data encoding choices we discussed in Chapter \ref{data-dall} (Data Dalliances).

\hypertarget{preliminaries---data-computation}{%
\section{Preliminaries - Data Computation}\label{preliminaries---data-computation}}

Before we think about specific tools or failure modes, we can first consider the common types of operations that the analytical tools allow us to do with our data.

\hypertarget{single-table-operations}{%
\subsection{Single Table Operations}\label{single-table-operations}}

Given a single data table, we may wish to do operations (illustrated in Figure \ref{fig:filt-aggr-tran}) such as:

\begin{itemize}
\tightlist
\item
  \textbf{Filtering}: Extracting a subset of a dataset for analysis based on certain inclusion criteria for each record
\item
  \textbf{Aggregation}: Grouping our data table by one or more variables and condensing information across records with \emph{aggregate functions} like counts, sums, and averages
\item
  \textbf{Transformation}: Create new columns or modifying existing columns to represent more complex or domain-specific context
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/comp-quan/filt-aggr-tran} 

}

\caption{Illustration of basic single-table data wrangling operations}\label{fig:filt-aggr-tran}
\end{figure}

\hypertarget{multiple-table-operations}{%
\subsection{Multiple Table Operations}\label{multiple-table-operations}}

Often, we can get additional value in an analysis by combining multiple types of information from difference tables.
When working with multiple tables, we may be interested in:

\begin{itemize}
\tightlist
\item
  \textbf{Combining Row-wise}: Taking multiple tables with the same schemas (column names and data types) and creating a single table which contains the union (all records), intersection (only matching), or difference (only in one) of the records in the two tables
\item
  \textbf{Combining Column-wise}: Appending additional fields to existing records through joining (also known as merging) multiple tables
\end{itemize}

\hypertarget{mechanics}{%
\subsection{Mechanics}\label{mechanics}}

All of these operations rely on a few core computational tasks:

\begin{itemize}
\tightlist
\item
  \textbf{Arithmetic}: Basic addition, subtraction, multiplication, and division to aggregate and transform data
\item
  \textbf{Equality}: Comparing whether or not two values are equal is critical for data filtering, column-wise combination, and certain types of data transformation
\item
  \textbf{Casting}: Converting data types of different elements into a comparable format is necessary for row-wise combination and often a prerequisite to certain equality and arithmetic tasks
\end{itemize}

While these operations may seem simple, their behavior within certain tools and when employed for certain data types may sometimes lead to unintuitive or misleading results.

\hypertarget{null-values}{%
\section{Null Values}\label{null-values}}

In Chapter \ref{data-dall} (Data Dalliances), we discuss how null values may represent many different concepts and be encoded in multiple different ways.
In addition to those semantic challenges, various representations of null values may cause different computational problems.\footnote{This problem is not isolated to data analysis tools. For an entertaining example, see the 2019 WIRED article ``How a `NULL' License Plate Landed One Hacker in Ticket Hell'' \citep{barrett} which a real-world software system producing unintended and undesirable behavior when asked to deal with a word \texttt{\textquotesingle{}NULL\textquotesingle{}}.}
In this section, we will explore these potential failure modes.

\hypertarget{types-of-null-values}{%
\subsection{Types of Null Values}\label{types-of-null-values}}

Not only can null values represent many different things (as explored in Chapter \ref{data-dall}), they also may be represented in many different ways. Understanding how nulls are encoded in one's dataset is a critical prerequisite to attempting any of the computations described in the subsequent sections.

\hypertarget{language-representations}{%
\subsubsection{Language representations}\label{language-representations}}

Different programming languages each offer their own versions of null values -- and sometimes more than one. For example, the R language includes \texttt{NA}, typed \texttt{NA}s (e.g.~\texttt{NA\_integer\_}, \texttt{NA\_character\_}), \texttt{NaN}, and \texttt{NULL}; meanwhile, core python has \texttt{None} and the \texttt{numpy} module provides a \texttt{nan}.

These different values carry different semantic and functional meanings. For example R's \texttt{NA} generally means ``the presence of an absence'' whereas \texttt{NULL} is ``the absence of a presence''. This is articulated more clearly if we examine the lengths of these objects and observe that \texttt{NA} has a length 1 whereas \texttt{NULL} has a length 0.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(}\ConstantTok{NA}\NormalTok{)}
\FunctionTok{length}\NormalTok{(}\ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
## [1] 0
\end{verbatim}

As further proof that these are not interchangeable, we may use the helper functions \texttt{is.na()} and \texttt{is.null()}. It's false that \texttt{NA} is \texttt{NULL} and essentially unable to be evaluated if \texttt{NULL} is \texttt{NA} because \texttt{NULL}s are truly nothing.\footnote{You'll notice that the code chunk below contains four commands but only three boolean outputs. Why is that? \texttt{is.na(NULL)} returns \texttt{logical(0)}, a zero-length value. It's yet another form of missingness!}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{is.na}\NormalTok{(}\ConstantTok{NA}\NormalTok{)}
\FunctionTok{is.null}\NormalTok{(}\ConstantTok{NULL}\NormalTok{)}
\FunctionTok{is.na}\NormalTok{(}\ConstantTok{NULL}\NormalTok{)}
\FunctionTok{is.null}\NormalTok{(}\ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
## [1] TRUE
## logical(0)
## [1] FALSE
\end{verbatim}

To further complicate matters, we have \texttt{NaN} (``not a number''), along with \texttt{-Inf} and \texttt{Inf}, which generally arise when we attempt to abuse R's calculator. Somewhat charmingly, \texttt{Inf} and \texttt{-Inf} may be used in some rudimentary calculations where the limit is returned.\footnote{From calculus, we know 1/Inf approaches 0, but Inf/Inf is undefined.}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\SpecialCharTok{/}\DecValTok{0}
\DecValTok{0}\SpecialCharTok{/}\DecValTok{0}
\DecValTok{1}\SpecialCharTok{/}\ConstantTok{Inf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] Inf
## [1] NaN
## [1] 0
\end{verbatim}

\hypertarget{sentinel-value-encoding}{%
\subsubsection{Sentinel value encoding}\label{sentinel-value-encoding}}

Beyond these null types offered natively by different programming languages, there are also many different data management \emph{conventions} for null values. Because null values can have many meanings, sometimes missing fields are encoded with ``out of range'' values which intend to suggest a type of missingness.

For example, the US Census Bureau's Medical Expenditure Panel Survey uses the following reserved codes to denote different types of missingness: (TODO: cite p10 \url{https://www.meps.ahrq.gov/data_stats/download_data/pufs/h206a/h206adoc.pdf})

\begin{verbatim}
- -1 INAPPLICABLE Question was not asked due to skip pattern
- -7 REFUSED Question was asked and respondent refused to answer question
- -8 DK Question was asked and respondent did not know answer
- -14 NOT YET TAKEN/USED Respondent answered that the medicine has not yet been used
- -15 CANNOT BE COMPUTED Value cannot be derived from data
\end{verbatim}

This approach preserves a lot of relevant information while, at the same time, being readily apparent that these values are not valid when the data is manually inspect. Unfortunately, manually inspecting every data field is rarely possible, and such sentinel values may go undetected when looking at higher-level summaries.

Consider a survey of a population of retired adults where age is coded as \texttt{999} if not provided. Below, we simulate 100,000 such observations that are uniformly distributed between the age of 65 and 95 (hence, have an expected value of 80). Next, we replace merely \emph{half of a percent} with our ``null'' values of \texttt{999}. Taking the mean with these false values results in a mean of about 85. This number alone might not raise the alarm; after all, we know the dataset's population is older adults. However, accidentally treating these as valid values biases our results by a somewhat remarkable five years.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100000}
\NormalTok{p }\OtherTok{\textless{}{-}} \FloatTok{0.01} \SpecialCharTok{/} \DecValTok{2}
\NormalTok{ages }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n, }\DecValTok{65}\NormalTok{, }\DecValTok{95}\NormalTok{)}

\NormalTok{ages\_nulls }\OtherTok{\textless{}{-}}\NormalTok{ ages}
\NormalTok{ages\_nulls[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{p)] }\OtherTok{\textless{}{-}} \DecValTok{999}

\FunctionTok{mean}\NormalTok{(ages)}
\FunctionTok{mean}\NormalTok{(ages\_nulls)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 79.98
## [1] 84.57
\end{verbatim}

So, the first order of business with null values is understanding how they are encoded and translating them to the most computationally appropriate form. However, that is only the beginning of the story.

\hypertarget{other-types-of-nulls-todo}{%
\subsubsection{Other types of nulls (TODO)}\label{other-types-of-nulls-todo}}

might be blank string which won't get detected in standard null checks

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \StringTok{""}
\FunctionTok{is.na}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

same thing is true in dataframes

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{""}\NormalTok{, }\StringTok{"d"}\NormalTok{))}
\FunctionTok{summarize\_all}\NormalTok{(df, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(.)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x
## 1 0
\end{verbatim}

in such cases, need to explicitly check for such alternative encodings like blank strings

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \StringTok{""}
\FunctionTok{is.na}\NormalTok{(x) }\SpecialCharTok{|}\NormalTok{ x }\SpecialCharTok{==} \StringTok{""}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

of course, a blank string isn't the only choice\\
could also have an empty string of any length

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \StringTok{"  "}
\FunctionTok{is.na}\NormalTok{(x) }\SpecialCharTok{|}\NormalTok{ x }\SpecialCharTok{==} \StringTok{""}
\FunctionTok{is.na}\NormalTok{(x) }\SpecialCharTok{|} \FunctionTok{trimws}\NormalTok{(x) }\SpecialCharTok{==} \StringTok{""}
\FunctionTok{is.na}\NormalTok{(x) }\SpecialCharTok{|} \FunctionTok{nchar}\NormalTok{(}\FunctionTok{trimws}\NormalTok{(x)) }\SpecialCharTok{==} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
## [1] TRUE
## [1] TRUE
\end{verbatim}

we'll see more about how fluid strings can be in the string section below

\hypertarget{aggregation}{%
\subsection{Aggregation}\label{aggregation}}

Once null values are coded as ``true'' nulls, how these nulls are handled in the simple aggregation of data varies both across different languages and across different functions within a language.
To better understand the problems this might cause, we will look at examples in R and SQL.

To explore aggregation, let's build a simple dataset. We will suppose that we are working with a subscription-based e-commerce service and that we are looking at a \texttt{spend} dataset with one record per customer and information about the amount they spent and returned in a given month:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spend }\OtherTok{\textless{}{-}}
  \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{AMT\_SPEND =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{AMT\_RETURN =} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{  )}

\FunctionTok{head}\NormalTok{(spend)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   AMT_SPEND AMT_RETURN
## 1        10         NA
## 2        20         NA
## 3        NA         NA
\end{verbatim}

To compute the average amount spent (\texttt{AMT\_SPEND}) with the \texttt{dplyr} package, an analyst might first reasonably write the following \texttt{summarize()} statement.
However, as we can see, due to the presence of null values within the \texttt{AMT\_SPEND} column, the result of this aggregation is for the whole quantity of \texttt{AVG\_SPEND} to be set to the value \texttt{NA}.

A glance at the documentation for the \texttt{mean()} function\footnote{This documentation can be obtained in R by typing \texttt{?mean} in the console.} reveals that the function has a parameter called \texttt{na.rm}.
This parameter's default value is \texttt{FALSE}, but, when it is set to \texttt{TRUE}, it removes null values from our dataset.
Adding this argument to the previous statement allows us to reach a numerical answer.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summarize}\NormalTok{(spend, }
          \AttributeTok{AVG\_SPEND =} \FunctionTok{mean}\NormalTok{(AMT\_SPEND),}
          \AttributeTok{AVG\_SPEND\_NARM =} \FunctionTok{mean}\NormalTok{(AMT\_SPEND, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   AVG_SPEND AVG_SPEND_NARM
## 1        NA             15
\end{verbatim}

However, is this the \emph{right} numerical answer?
Recall that what \texttt{na.rm\ =\ TRUE} does is \emph{drop} the null values from the set of numbers being averaged.
However, suppose the null values represent that no purchases were made for a given customer in a given month.
That is, zero dollars were spent.
In effect, we have removed all non-purchasers from the data being averaged.

More precisely, we have switched from taking the average

\[ \frac{ \sum_{1}^{n} Spend }{\sum_{1}^{n} 1} \] over all \(n\) customers

to taking the average

\[ \frac{ \sum_{Spend > 0} Spend }{\sum_{Spend > 0} 1} \] over only those customers with spend.

At face value, we could say that the code above is giving the incorrect answer; by dropping some low (zero) purchase amounts, the average amount spend per customer is inflated.
A second perspective, which is someone more philosophically troubling, is that this tiny change to the code which fixed the \emph{obvious} problem (returning a null value) has introduced a \emph{non-obvious} problem by fundamentally changing the question that we are asking.
By dropping all accounts from our table who made no purchases, we are no longer answering ``What is the average amount spent by all of our customer?'' but rather ``What is the average amount spent by an actively engaged customer?''
In the language of probability theory, we might say that we have then changed our estimand from the expected value of spend to expected value of spend conditional on spend being positive.
This technical quirk has significant analytical impact.

To answer the real question at hand, we have a couple of options.
We could manually \texttt{sum()} the amount spent with the option to drop nulls but then divide by the correct denominator (all observations -- not just those with spend) or we could explicitly recode null values in \texttt{AMT\_SPEND} to zero before taking the average.\footnote{Recoding can be done with a number of different general purpose functions like \texttt{ifelse} or \texttt{dplyr::case\_when} in R. Different SQL varaints often offer different options for this purpose with functions such as \texttt{nvl()} or \texttt{zeroifnull()}. A common version across many language is \texttt{coalesce()} which takes the first non-null argument listed.}
Either of these options lead to the correct conclusion of a lower average spend amount.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summarize}\NormalTok{(}
\NormalTok{    spend,}
    \AttributeTok{AVG\_SPEND\_MANUAL =} \FunctionTok{sum}\NormalTok{(AMT\_SPEND, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{/} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{AVG\_SPEND\_RECODE =} \FunctionTok{mean}\NormalTok{(}\FunctionTok{coalesce}\NormalTok{(AMT\_SPEND, }\DecValTok{0}\NormalTok{))}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   AVG_SPEND_MANUAL AVG_SPEND_RECODE
## 1               10               10
\end{verbatim}

This is all well and good if we could just accept that the behaviors above are simply how nulls work, but further complexity comes as we see that there is no industry standard across tools.
For example, as the SQL code below shows, SQL's \texttt{avg()} function behaves more like R's \texttt{mean()} \emph{with} the \texttt{na.rm\ =\ TRUE} option set (whereas, you may recall that R's \texttt{mean()} behaves with \texttt{na.rm\ =\ FALSE} by default).
That is, the default behavior of SQL is to only operate on the valid and available values.
The result of this default may mean that it is less obvious when our dataset has null values.
SQL, unlike R, does not ask for ``permission'' to drop out nulls;
instead, it unilaterally makes a decision how to handle these variables.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} 
  \FunctionTok{avg}\NormalTok{(amt\_spend) }\KeywordTok{as}\NormalTok{ AVG\_SPEND}
\KeywordTok{FROM}\NormalTok{ spend}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   AVG_SPEND
## 1        15
\end{verbatim}

However, this is not to suggest that null values cannot also be ``destructive'' in SQL (that is, returning null).
While aggregation functions (which compute over the \emph{rows/records}) like \texttt{sum()} and \texttt{avg()} drop nulls, operators like \texttt{+} and \texttt{-} (which compute \emph{across columns/variables} in the \emph{same row/record}) do not exhibit the same behavior.
Consider, for example, if we wish to calculate the average net purchase amount (purchases minus returns) instead of the gross (total) purchase amount.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} 
  \FunctionTok{avg}\NormalTok{(amt\_spend }\OperatorTok{{-}}\NormalTok{ amt\_return) }\KeywordTok{as}\NormalTok{ AVG\_SPEND\_NET}
\KeywordTok{FROM}\NormalTok{ spend}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   AVG_SPEND_NET
## 1            NA
\end{verbatim}

Despite what we learned above about SQL's \texttt{avg()} function, the query above returns only a null value.
What has happened?
In our \texttt{spend} dataset, the \texttt{amt\_return} column is completely null (representing no return purchases).
Because the subtraction occurs before the average is taken, subtracting null values in the \texttt{amt\_return} variable from valid numbers in \texttt{amt\_spend} variable creates a new variable of all null values.
This new variable, which is already all null, is passed to the \texttt{avg()} function.
This process is shown step-by-step below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}
\NormalTok{  amt\_spend, }
\NormalTok{  amt\_return, }
\NormalTok{  amt\_spend}\OperatorTok{{-}}\NormalTok{amt\_return }
\KeywordTok{FROM}\NormalTok{ spend}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   AMT_SPEND AMT_RETURN amt_spend-amt_return
## 1        10         NA                   NA
## 2        20         NA                   NA
## 3        NA         NA                   NA
\end{verbatim}

\hypertarget{comparison}{%
\subsection{Comparison}\label{comparison}}

Null values don't just introduce complexity when doing arithmetic.
Difficulties also arise any time multiple variables are assessed for equality or inequality.
Since a null value is unknown, most programming languages generally will \emph{not} consider nulls to be comparable with other nulls.

We can observe simple examples of this in both R and SQL. In neither language can a null value be assessed for equality or inequality versus either another number or another null.

\begin{Shaded}
\begin{Highlighting}[]
\ConstantTok{NA} \SpecialCharTok{==} \DecValTok{3} 
\ConstantTok{NA} \SpecialCharTok{\textgreater{}} \DecValTok{10} 
\ConstantTok{NA} \SpecialCharTok{==} \ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
## [1] NA
## [1] NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}
\NormalTok{  (}\KeywordTok{NULL} \OperatorTok{=} \DecValTok{3}\NormalTok{) }\KeywordTok{as}\NormalTok{ NULL\_EQ\_NUM,}
\NormalTok{  (}\KeywordTok{NULL} \OperatorTok{\textgreater{}} \DecValTok{10}\NormalTok{) }\KeywordTok{as}\NormalTok{ NULL\_GT\_NUM,}
\NormalTok{  (}\KeywordTok{NULL} \OperatorTok{=} \KeywordTok{NULL}\NormalTok{) }\KeywordTok{as}\NormalTok{ NULL\_EQ\_NULL}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   NULL_EQ_NUM NULL_GT_NUM NULL_EQ_NULL
## 1          NA          NA           NA
\end{verbatim}

In these toy examples, such outcomes may seem perfectly logical.
However, this same reasoning can arise in sneakier ways and lead to uninteded results when equality evaluations are \emph{implicit} in the task at hand instead of the singular focus.
We'll now see examples from data filtering, joining, and transformation.

If you're familiar with SQL, you may have been surprised to notice that there is no \texttt{FROM} clause in the query above. In fact, SQL queries can treat values just like variables containing only a single record.

We will use this trick throughout the chapter for exploring how SQL works when we don't have an ideal sample dataset to test certain scenarios. Beyond exposition in this book, this trick is also useful in practice.

\hypertarget{filtering}{%
\subsubsection{Filtering}\label{filtering}}

Suppose we want to split our dataset into two datasets based on high or low values of spend.
We might assume the following two lines of code will create a clear partition (implying that each record would fall into exactly one group.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spend\_lt20 }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(spend, AMT\_SPEND }\SpecialCharTok{\textless{}} \DecValTok{20}\NormalTok{)}
\NormalTok{spend\_gte20 }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(spend, AMT\_SPEND }\SpecialCharTok{\textgreater{}=} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

However, if we examine the resulting datasets, we see that \emph{neither} contains the third record which had a null value for the \texttt{AMT\_SPEND} variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spend\_lt20}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   AMT_SPEND AMT_RETURN
## 1        10         NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spend\_gte20}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   AMT_SPEND AMT_RETURN
## 1        20         NA
\end{verbatim}

The same situation results in SQL.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
\KeywordTok{FROM}\NormalTok{ spend}
\KeywordTok{WHERE}\NormalTok{ AMT\_SPEND }\OperatorTok{\textless{}} \DecValTok{20}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   AMT_SPEND AMT_RETURN
## 1        10         NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
\KeywordTok{FROM}\NormalTok{ spend}
\KeywordTok{WHERE}\NormalTok{ AMT\_SPEND }\OperatorTok{\textgreater{}=} \DecValTok{20}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   AMT_SPEND AMT_RETURN
## 1        20         NA
\end{verbatim}

This is a direct result of the fact that nulls cannot be compared for equality any inequality.
We can think of data filtering as a two-step process:
first evaluate whether the condition is TRUE or FALSE,
then return only the records for which the condition holds true.
When we conduct the more manual process of filtering step-by-step, we see that the null value of \texttt{AMT\_SPEND} does not get a ``truth value'' when compared with a number.
Thus, it is not contained in either ``truth value'' subset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mutate}\NormalTok{(spend, }\AttributeTok{is\_lt20 =}\NormalTok{ (AMT\_SPEND }\SpecialCharTok{\textless{}} \DecValTok{20}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   AMT_SPEND AMT_RETURN is_lt20
## 1        10         NA    TRUE
## 2        20         NA   FALSE
## 3        NA         NA      NA
\end{verbatim}

Thus, whenever our data has null values, the very common act of data filtering risks excluding important information.

\hypertarget{joining}{%
\subsubsection{Joining}\label{joining}}

The same phenomenon as described above also happens when joining multiple datasets.

Suppose we have multiple datasets we wish to merge based on columns denoting a record's name and date of birthday.
For ease of exploration, we will make the simplest possible such dataset and simply try to merge it to itself.
(This may seem silly, but often when trying to understand \emph{computationally} complex things, it is a good idea to make the scenario as simple as possible.
In fact, this idea is core to the concept of computational unit tests which we will discuss at the end of this chapter.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bday }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{NAME =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Anne\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bob\textquotesingle{}}\NormalTok{), }\AttributeTok{BIRTHDAY =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}2000{-}01{-}01\textquotesingle{}}\NormalTok{, }\ConstantTok{NA}\NormalTok{))}
\NormalTok{bday}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   NAME   BIRTHDAY
## 1 Anne 2000-01-01
## 2  Bob       <NA>
\end{verbatim}

In SQL, if we try to join this table, the records in row 1 will match because \texttt{\textquotesingle{}Anne\textquotesingle{}\ ==\ \textquotesingle{}Anne\textquotesingle{}} and \texttt{\textquotesingle{}2000-01-01\textquotesingle{}\ ==\ \textquotesingle{}2000-01-01\textquotesingle{}}.
However, poor Bob's record is eliminated because his birth date is logged as null, and \texttt{NA\ ==\ NA} is false.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ a.}\OperatorTok{*}
\KeywordTok{FROM}
\NormalTok{  bday }\KeywordTok{as}\NormalTok{ a}
  \KeywordTok{INNER} \KeywordTok{JOIN}
\NormalTok{  bday }\KeywordTok{as}\NormalTok{ b}
  \KeywordTok{ON}
\NormalTok{  a.NAME }\OperatorTok{=}\NormalTok{ b.NAME }\KeywordTok{and}
\NormalTok{  a.BIRTHDAY }\OperatorTok{=}\NormalTok{ b.BIRTHDAY}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   NAME   BIRTHDAY
## 1 Anne 2000-01-01
\end{verbatim}

In contrast, R's \texttt{dplyr::inner\_join()} function will not do this by default.
This function lets us specifically control how nulls are matches with the \texttt{na\_matches} argument, with a default option to match on \texttt{NA} values.
(You may read more about the argument by typing \texttt{?dplyr::inner\_join} in the R console to pull up the documentation.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{inner\_join}\NormalTok{(bday, bday, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}NAME\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}BIRTHDAY\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   NAME   BIRTHDAY
## 1 Anne 2000-01-01
## 2  Bob       <NA>
\end{verbatim}

This example then is not only a cautionary tale for how null values may unintentionally corrupt our data transformations but also how ``brittle'' our knowledge and intuition may be when moving between tools.
Neither of these default behaviors is strictly better or worse, but they are definitely different and have real implications on our analysis.

\hypertarget{transformation}{%
\subsubsection{Transformation}\label{transformation}}

A common task in data analysis is to aggregate results by subgroup.
For example, we might want to summarize how many customers (rows/records) spent more or less than \$10. To discern this, we might create a categorical variable for high versus low purchase amounts, group by this variable and count.

The psuedocode would read something like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{HIGH\_LOW =} \SpecialCharTok{\textless{}}\ErrorTok{\textless{}}\NormalTok{ transform AMT\_SPEND }\SpecialCharTok{\textgreater{}}\ErrorTok{\textgreater{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(HIGH\_LOW) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

To define the \texttt{HIGH\_LOW} variable, we might use a function like \texttt{ifelse()}, \texttt{dplyr::if\_else()}, or \texttt{dplyr::case\_when()}.
However, once again, we have the issue of how values are \emph{partitioned} when nulls are included.
If we recode any records with \texttt{AMT\_SPEND} of less than or equal to 10 to ``Low'' and default the rest to ``High'', we will accidentally count all null values in the ``High'' group.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spend }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{HIGH\_LOW =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    AMT\_SPEND }\SpecialCharTok{\textless{}=} \DecValTok{10} \SpecialCharTok{\textasciitilde{}} \StringTok{"Low"}\NormalTok{, }
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"High"}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(HIGH\_LOW) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
## # Groups:   HIGH_LOW [2]
##   HIGH_LOW     n
##   <chr>    <int>
## 1 High         2
## 2 Low          1
\end{verbatim}

Instead, it is more accurate and transparent (unless we know specifically what null values mean and what group they should be part of) to not let one of our ``core'' categories by the ``default'' case in our logic.
We can explicitly encode any residual values as something like ``OTHER'' or ``ERROR'' to help us see that there is a problem requiring extra attention.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spend }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{HIGH\_LOW =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    AMT\_SPEND }\SpecialCharTok{\textless{}=} \DecValTok{10} \SpecialCharTok{\textasciitilde{}} \StringTok{"Low"}\NormalTok{, }
\NormalTok{    AMT\_SPEND }\SpecialCharTok{\textgreater{}} \DecValTok{10} \SpecialCharTok{\textasciitilde{}} \StringTok{"High"}\NormalTok{,}
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"OTHER"}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(HIGH\_LOW) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 2
## # Groups:   HIGH_LOW [3]
##   HIGH_LOW     n
##   <chr>    <int>
## 1 High         1
## 2 Low          1
## 3 OTHER        1
\end{verbatim}

While nulls contribute to this issue, it's important to realize that nulls are not the only factor causing this error nor or they the solution.
The more substantial issue is the careless use of defaults and \emph{implicit} encoding versus \emph{explicit} encoding.
In the second form of the SQL query above, we are more specific about exactly what is allowed in each category which ensures any unexpected inputs will not be allowed to ``sneak'' into ordinary outputs.

\hypertarget{logicals-todo}{%
\section{Logicals (TODO)}\label{logicals-todo}}

Much like the different versions of nulls that we met in the last section, logical data types use reserved keywords to represent \texttt{TRUE} and \texttt{FALSE}.
(The exact formats of logical reserved keywords vary by language. R and SQL use \texttt{TRUE} and \texttt{FALSE} and python uses \texttt{TRUE} and \texttt{FALSE}.)
This means that these names function like a number or a letter which intrinsically hold one specific value and cannot take on a different value besides their own.

\begin{Shaded}
\begin{Highlighting}[]
\ConstantTok{TRUE} \OtherTok{=} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in TRUE = 5: invalid (do_set) left-hand side to assignment
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \OtherTok{=} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in 2 = 5: invalid (do_set) left-hand side to assignment
\end{verbatim}

Across languages, \texttt{TRUE} and \texttt{FALSE} are considered equivalent to the numeric representations of 1 and 0 respectively.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.numeric}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{as.numeric}\NormalTok{(}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
## [1] 0
\end{verbatim}

A consequence of this numerical equivalency is that \texttt{TRUE} and \texttt{FALSE} may be compared to each other or other numbers and be included in mathematical expressions.

\begin{Shaded}
\begin{Highlighting}[]
\ConstantTok{TRUE} \SpecialCharTok{\textgreater{}} \ConstantTok{FALSE}
\ConstantTok{TRUE} \SpecialCharTok{\textless{}} \DecValTok{5}
\ConstantTok{FALSE} \SpecialCharTok{\textgreater{}} \SpecialCharTok{{-}}\DecValTok{1}
\ConstantTok{TRUE} \SpecialCharTok{+} \DecValTok{1}
\ConstantTok{TRUE} \SpecialCharTok{*} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
## [1] TRUE
## [1] TRUE
## [1] 2
## [1] 5
\end{verbatim}

similar in SQL

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}
  \KeywordTok{TRUE} \OperatorTok{\textgreater{}} \KeywordTok{FALSE} \KeywordTok{as}\NormalTok{ a,}
  \KeywordTok{TRUE} \OperatorTok{\textless{}} \DecValTok{5} \KeywordTok{as}\NormalTok{ b,}
  \KeywordTok{FALSE} \OperatorTok{\textgreater{}} \OperatorTok{{-}}\DecValTok{1} \KeywordTok{as}\NormalTok{ c,}
  \KeywordTok{TRUE} \OperatorTok{+} \DecValTok{1} \KeywordTok{as}\NormalTok{ d,}
  \KeywordTok{TRUE} \OperatorTok{*} \DecValTok{5} \KeywordTok{as}\NormalTok{ e}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-66}1 records}
\centering
\begin{tabular}[t]{r|r|r|r|r}
\hline
a & b & c & d & e\\
\hline
1 & 1 & 1 & 2 & 5\\
\hline
\end{tabular}
\end{table}

\hypertarget{language-specific-nuances-cut}{%
\subsection{Language-specific nuances (CUT?)}\label{language-specific-nuances-cut}}

\hypertarget{keyword-abbrevations}{%
\subsubsection{Keyword abbrevations}\label{keyword-abbrevations}}

R also recognizes the abbreviations of \texttt{T} and \texttt{F} as \texttt{TRUE} and \texttt{FALSE} respectively;
however this is not recommended.
\texttt{T} and \texttt{F} are \emph{not} reserved keywords, so they can be overwritten with a different value.
This makes code using such abbreviations ``brittle'' and less reliable.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (T) \{}\StringTok{"Hello"}\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}\StringTok{"Goodbye"}\NormalTok{\}}
\ControlFlowTok{if}\NormalTok{ (F) \{}\StringTok{"Hello"}\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}\StringTok{"Goodbye"}\NormalTok{\}}
\NormalTok{T }\OtherTok{=} \DecValTok{0}
\ControlFlowTok{if}\NormalTok{ (T) \{}\StringTok{"Hello"}\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}\StringTok{"Goodbye"}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Hello"
## [1] "Goodbye"
## [1] "Goodbye"
\end{verbatim}

\hypertarget{alternative-representations}{%
\subsubsection{Alternative representations}\label{alternative-representations}}

We've previously seen that logicals have associated numerical values.
Different languages may also treat their string representations differently.

For example, R believes that the string \texttt{"TRUE"} is equal to the logical value \texttt{TRUE} when directly compared.
However, this relationships breaks the transitive property of mathematics\footnote{If a = b and b = c then a = c} because \texttt{TRUE} equals both \texttt{1} and \texttt{"TRUE"}, yet \texttt{"TRUE"} does not equal \texttt{1} so the mathematical operations that can be done with logical \texttt{TRUE} cannot be done with string \texttt{"TRUE"}.

\begin{Shaded}
\begin{Highlighting}[]
\ConstantTok{TRUE} \SpecialCharTok{==} \DecValTok{1}
\ConstantTok{TRUE} \SpecialCharTok{==} \StringTok{"TRUE"}
\ConstantTok{TRUE} \SpecialCharTok{==} \StringTok{"True"}
\ConstantTok{TRUE} \SpecialCharTok{*} \DecValTok{5}
\StringTok{"TRUE"} \SpecialCharTok{*} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in "TRUE" * 5: non-numeric argument to binary operator
\end{verbatim}

\begin{verbatim}
## [1] TRUE
## [1] TRUE
## [1] FALSE
## [1] 5
\end{verbatim}

In contrast neither SQL not python honor the string forms of their respective logical reserved keywords at all.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select} 
\NormalTok{  (}\KeywordTok{TRUE} \OperatorTok{==} \DecValTok{1}\NormalTok{) }\KeywordTok{as}\NormalTok{ is\_int\_true,}
\NormalTok{  (}\KeywordTok{TRUE} \OperatorTok{==} \StringTok{\textquotesingle{}TRUE\textquotesingle{}}\NormalTok{) }\KeywordTok{as}\NormalTok{ is\_char\_true,}
  \KeywordTok{TRUE}\OperatorTok{*}\DecValTok{5} \KeywordTok{as}\NormalTok{ true\_times\_five,}
  \StringTok{\textquotesingle{}TRUE\textquotesingle{}}\OperatorTok{*}\DecValTok{5} \KeywordTok{as}\NormalTok{ true\_str\_time\_five}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   is_int_true is_char_true true_times_five
## 1           1            0               5
##   true_str_time_five
## 1                  0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{True} \OperatorTok{==} \DecValTok{1}
\VariableTok{True} \OperatorTok{==} \StringTok{"True"}
\VariableTok{True} \OperatorTok{==} \StringTok{"TRUE"}
\VariableTok{True} \OperatorTok{*} \DecValTok{5}
\CommentTok{"True"} \OperatorTok{*} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## True
## False
## False
## 5
## 'TrueTrueTrueTrueTrue'
\end{verbatim}

\hypertarget{comparison-todo}{%
\subsection{Comparison (TODO)}\label{comparison-todo}}

The nuances of logical representation and handling seem straightforward in isolation.
However, when encountered in real-world data problems, they are not isolated and are unlikely to be our main focus.

Imagine two datasets which all encode the same information but use boolean, string, and integer representations of a logical respectively.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ID =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\AttributeTok{IND =} \FunctionTok{rep}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{X =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)}
\NormalTok{df2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ID =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\AttributeTok{IND =} \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}TRUE\textquotesingle{}}\NormalTok{, }\DecValTok{5}\NormalTok{), }\AttributeTok{Y =} \DecValTok{11}\SpecialCharTok{:}\DecValTok{15}\NormalTok{)}
\NormalTok{df3 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ID =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\AttributeTok{IND =} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{), }\AttributeTok{Z =} \DecValTok{21}\SpecialCharTok{:}\DecValTok{25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

By simple inspection, the logical and string representations in particular look superficially similar and yet they will behave differently.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(df1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ID  IND X
## 1  1 TRUE 1
## 2  2 TRUE 2
## 3  3 TRUE 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(df2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ID  IND  Y
## 1  1 TRUE 11
## 2  2 TRUE 12
## 3  3 TRUE 13
## 4  4 TRUE 14
## 5  5 TRUE 15
\end{verbatim}

If we use R's \texttt{dplyr::filter()} or \texttt{base::subset()} function to subset the data, the value of \texttt{df1} will correctly subset based on the boolean values of \texttt{IND}. However, R will not know how to interpret the string version in \texttt{df2}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{filter}\NormalTok{(df1, IND)}
\FunctionTok{filter}\NormalTok{(df2, IND)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error: Problem with `filter()` input `..1`.
## x Input `..1` must be a logical vector, not a character.
## i Input `..1` is `IND`.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{subset}\NormalTok{(df2, df2}\SpecialCharTok{$}\NormalTok{IND)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in subset.data.frame(df2, df2$IND): 'subset' must be logical
\end{verbatim}

\begin{verbatim}
##   ID  IND X
## 1  1 TRUE 1
## 2  2 TRUE 2
## 3  3 TRUE 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{filter}\NormalTok{(df1, }\FunctionTok{isTRUE}\NormalTok{(IND))}
\FunctionTok{filter}\NormalTok{(df2, }\FunctionTok{isTRUE}\NormalTok{(IND))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] ID  IND X  
## <0 rows> (or 0-length row.names)
## [1] ID  IND Y  
## <0 rows> (or 0-length row.names)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{filter}\NormalTok{(df2, }\FunctionTok{isTRUE}\NormalTok{(IND))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] ID  IND Y  
## <0 rows> (or 0-length row.names)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{left\_join}\NormalTok{(df1, df2, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"ID"}\NormalTok{, }\StringTok{"IND"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error: Can't join on `x$IND` x `y$IND` because of incompatible types.
## i `x$IND` is of type <logical>>.
## i `y$IND` is of type <character>>.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{merge}\NormalTok{(df1, df2, }\AttributeTok{by.x =} \FunctionTok{c}\NormalTok{(}\StringTok{"ID"}\NormalTok{, }\StringTok{"IND"}\NormalTok{), }\AttributeTok{by.y =} \FunctionTok{c}\NormalTok{(}\StringTok{"ID"}\NormalTok{, }\StringTok{"IND"}\NormalTok{), }\AttributeTok{all.x =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ID  IND X  Y
## 1  1 TRUE 1 11
## 2  2 TRUE 2 12
## 3  3 TRUE 3 13
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{ df1.}\OperatorTok{*}\NormalTok{, df2.}\OperatorTok{*}
\KeywordTok{from} 
\NormalTok{ df1 }
 \KeywordTok{left} \KeywordTok{join}
\NormalTok{ df2}
 \KeywordTok{on}
\NormalTok{ df1.}\KeywordTok{id} \OperatorTok{=}\NormalTok{ df2.}\KeywordTok{id} \KeywordTok{and}
\NormalTok{ df1.ind }\OperatorTok{=}\NormalTok{ df2.ind}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-79}3 records}
\centering
\begin{tabular}[t]{l|r|r|r|l|r}
\hline
ID & IND & X & ID & IND & Y\\
\hline
1 & 1 & 1 & NA & NA & NA\\
\hline
2 & 1 & 2 & NA & NA & NA\\
\hline
3 & 1 & 3 & NA & NA & NA\\
\hline
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd }

\NormalTok{data1 }\OperatorTok{=}\NormalTok{ \{}\StringTok{\textquotesingle{}ID\textquotesingle{}}\NormalTok{: }\BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)),}
        \StringTok{\textquotesingle{}IND\textquotesingle{}}\NormalTok{: [}\VariableTok{True}\NormalTok{] }\OperatorTok{*} \DecValTok{3}\NormalTok{,}
        \StringTok{\textquotesingle{}X\textquotesingle{}}\NormalTok{: }\BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\NormalTok{        \}}
\NormalTok{data2 }\OperatorTok{=}\NormalTok{ \{}\StringTok{\textquotesingle{}ID\textquotesingle{}}\NormalTok{: }\BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{)),}
         \StringTok{\textquotesingle{}IND\textquotesingle{}}\NormalTok{: [}\StringTok{\textquotesingle{}True\textquotesingle{}}\NormalTok{] }\OperatorTok{*} \DecValTok{5}\NormalTok{,}
        \StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{: }\BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{16}\NormalTok{))\}}

\NormalTok{df1 }\OperatorTok{=}\NormalTok{ pd.DataFrame(data1)}
\NormalTok{df2 }\OperatorTok{=}\NormalTok{ pd.DataFrame(data2)}

\NormalTok{df1}
\NormalTok{df2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    ID   IND  X
## 0   1  True  1
## 1   2  True  2
## 2   3  True  3
##    ID   IND   Y
## 0   1  True  11
## 1   2  True  12
## 2   3  True  13
## 3   4  True  14
## 4   5  True  15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pd.merge(left }\OperatorTok{=}\NormalTok{ df1, right }\OperatorTok{=}\NormalTok{ df2, how }\OperatorTok{=} \StringTok{"left"}\NormalTok{, on }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}ID\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}IND\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    ID   IND  X   Y
## 0   1  True  1 NaN
## 1   2  True  2 NaN
## 2   3  True  3 NaN
\end{verbatim}

\hypertarget{numbers-todo}{%
\section{Numbers (TODO)}\label{numbers-todo}}

\hypertarget{integer-division}{%
\subsection{Integer division}\label{integer-division}}

In R (mostly what we'd expect)

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\SpecialCharTok{/}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ifelse}\NormalTok{(}\DecValTok{3}\SpecialCharTok{/}\DecValTok{4} \SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{, }\StringTok{\textquotesingle{}High\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Low\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "High"
\end{verbatim}

In some SQL dialect SQL (SQLite shown. ``Modern'' interfaces like Snowflake and BigQuery don't do this)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ (}\DecValTok{1}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\KeywordTok{as}\NormalTok{ one\_div\_two}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   one_div_two
## 1           0
\end{verbatim}

This rounding can get masked when we are recoding or doing subsequent calculations

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}
  \ControlFlowTok{case}
    \ControlFlowTok{when} \DecValTok{3}\OperatorTok{/}\DecValTok{4} \OperatorTok{\textgreater{}} \FloatTok{0.5} \ControlFlowTok{then} \StringTok{\textquotesingle{}High\textquotesingle{}} 
    \ControlFlowTok{else} \StringTok{\textquotesingle{}Low\textquotesingle{}}
  \ControlFlowTok{end} \KeywordTok{as}\NormalTok{ high\_low\_int,}
  \ControlFlowTok{case}
    \ControlFlowTok{when} \FloatTok{3.0} \OperatorTok{/} \DecValTok{4} \OperatorTok{\textgreater{}} \FloatTok{0.5} \ControlFlowTok{then} \StringTok{\textquotesingle{}High\textquotesingle{}}
    \ControlFlowTok{else} \StringTok{\textquotesingle{}Low\textquotesingle{}}
  \ControlFlowTok{end} \KeywordTok{as}\NormalTok{ high\_low\_float}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   high_low_int high_low_float
## 1          Low           High
\end{verbatim}

The above works because of implicit casting. We can also do explicit casting, but where we do this matters

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}
  \ControlFlowTok{case} 
    \ControlFlowTok{when} \FunctionTok{cast}\NormalTok{(}\DecValTok{3} \KeywordTok{as} \DataTypeTok{float}\NormalTok{) }\OperatorTok{/} \DecValTok{4} \OperatorTok{\textgreater{}} \FloatTok{0.5} \ControlFlowTok{then} \StringTok{\textquotesingle{}High\textquotesingle{}}
    \ControlFlowTok{else} \StringTok{\textquotesingle{}Low\textquotesingle{}}
  \ControlFlowTok{end} \KeywordTok{as}\NormalTok{ cast\_first,}
  \ControlFlowTok{case}
    \ControlFlowTok{when} \FunctionTok{cast}\NormalTok{(}\DecValTok{3}\OperatorTok{/}\DecValTok{4} \KeywordTok{as} \DataTypeTok{float}\NormalTok{) }\OperatorTok{\textgreater{}} \FloatTok{0.5} \ControlFlowTok{then} \StringTok{\textquotesingle{}High\textquotesingle{}}
    \ControlFlowTok{else} \StringTok{\textquotesingle{}Low\textquotesingle{}}
  \ControlFlowTok{end} \KeywordTok{as}\NormalTok{ cast\_last}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   cast_first cast_last
## 1       High       Low
\end{verbatim}

\hypertarget{inexact-storage-and-comparison}{%
\subsection{Inexact storage and comparison}\label{inexact-storage-and-comparison}}

In R

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DecValTok{3} \SpecialCharTok{{-}} \FloatTok{2.9}\NormalTok{) }\SpecialCharTok{\textless{}=} \FloatTok{0.1}
\NormalTok{(}\DecValTok{2} \SpecialCharTok{{-}} \FloatTok{1.9}\NormalTok{) }\SpecialCharTok{\textless{}=} \FloatTok{0.1}
\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.9}\NormalTok{) }\SpecialCharTok{\textless{}=} \FloatTok{0.1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
## [1] FALSE
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DecValTok{3} \SpecialCharTok{{-}} \FloatTok{2.9}\NormalTok{) }\SpecialCharTok{==} \FloatTok{0.1}
\NormalTok{(}\DecValTok{2} \SpecialCharTok{{-}} \FloatTok{1.9}\NormalTok{) }\SpecialCharTok{==} \FloatTok{0.1}
\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.9}\NormalTok{) }\SpecialCharTok{==} \FloatTok{0.1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
## [1] FALSE
## [1] FALSE
\end{verbatim}

despite the fact that we see no difference

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{3} \SpecialCharTok{{-}} \FloatTok{2.9}
\DecValTok{2} \SpecialCharTok{{-}} \FloatTok{1.9}
\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.9}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1
## [1] 0.1
## [1] 0.1
\end{verbatim}

In python - same problem but in slightly different cases

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DecValTok{3} \OperatorTok{{-}} \FloatTok{2.9}\NormalTok{) }\OperatorTok{\textless{}=} \FloatTok{0.1}
\NormalTok{(}\DecValTok{2} \OperatorTok{{-}} \FloatTok{1.9}\NormalTok{) }\OperatorTok{\textless{}=} \FloatTok{0.1}
\NormalTok{(}\DecValTok{1} \OperatorTok{{-}} \FloatTok{0.9}\NormalTok{) }\OperatorTok{\textless{}=} \FloatTok{0.1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## False
## False
## True
\end{verbatim}

here we can see the differences

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{3} \OperatorTok{{-}} \FloatTok{2.9}
\DecValTok{2} \OperatorTok{{-}} \FloatTok{1.9}
\DecValTok{1} \OperatorTok{{-}} \FloatTok{0.9}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 0.10000000000000009
## 0.10000000000000009
## 0.09999999999999998
\end{verbatim}

Same thing with SQL where differences are masked

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}
  \DecValTok{3} \OperatorTok{{-}} \FloatTok{2.9} \KeywordTok{as}\NormalTok{ sub\_three,}
  \DecValTok{2} \OperatorTok{{-}} \FloatTok{1.9} \KeywordTok{as}\NormalTok{ sub\_two,}
  \DecValTok{1} \OperatorTok{{-}} \FloatTok{0.9} \KeywordTok{as}\NormalTok{ sub\_one,}
\NormalTok{  (}\DecValTok{3} \OperatorTok{{-}} \FloatTok{2.9}\NormalTok{) }\OperatorTok{\textless{}=} \FloatTok{0.1} \KeywordTok{as}\NormalTok{ sub\_compare\_three,}
\NormalTok{  (}\DecValTok{2} \OperatorTok{{-}} \FloatTok{1.9}\NormalTok{) }\OperatorTok{\textless{}=} \FloatTok{0.1} \KeywordTok{as}\NormalTok{ sub\_compare\_two,}
\NormalTok{  (}\DecValTok{1} \OperatorTok{{-}} \FloatTok{0.9}\NormalTok{) }\OperatorTok{\textless{}=} \FloatTok{0.1} \KeywordTok{as}\NormalTok{ sub\_compare\_one}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   sub_three sub_two sub_one sub_compare_three
## 1       0.1     0.1     0.1                 0
##   sub_compare_two sub_compare_one
## 1               0               1
\end{verbatim}

Instead, can use either built-in equality checker (python equivalent is \texttt{math.isclose}) or check that difference between two numbers is in very small window

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{all.equal}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.9}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}

\FunctionTok{abs}\NormalTok{( (}\DecValTok{1}\FloatTok{{-}0.9}\NormalTok{) }\SpecialCharTok{{-}} \FloatTok{0.1}\NormalTok{ ) }\SpecialCharTok{\textless{}=} \FloatTok{1e{-}10}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
## [1] TRUE
\end{verbatim}

\hypertarget{division-by-zero}{%
\subsection{Division by zero}\label{division-by-zero}}

more of a design issue about the right way to handle

we've seen before how we have to understand other peoples use of null values

this is a case where we he to decide what makes most sense

\hypertarget{strings-wip}{%
\section{Strings (WIP)}\label{strings-wip}}

String data can be inherently appealing. At their best, strings are used to bring more readable and human interpretable values into a dataset. However, string data and the processing thereof comes with its own challenges.

First, unlike numbers, human language strings can be ambiguously defined. \texttt{2} is the only number to represent the value of two. However, the incorporation of human language means that many different words, phrases, and formatting choices can represent the same concept. This is confounded by instances where string data was manually entered, as is the case with user-input data.\footnote{I can accurately say my name is ``Emily'', ``Emily Riederer'', ``Emily E. Riederer'', ``Ms.~Emily Riederer''. Additionally, if I spell my name over the phone, I can likely end up ``Emily Rieder'' or if I type it and mindlessly accept autocorrect, I'm ``Emily Reindeer''. This inconsistently may be problematic if you are trying to combine my data across different sources where I provided my name in different ways.}

Secondly, string data is one of the most flexible datatypes and can contain any other types of information -- from should-be-logical values (\texttt{"yes"/"no"}, \texttt{"true"/"false"}), should-be-numeric values (\texttt{"27"}), should-be-date values (\texttt{"2020-01-01"}), and even complex data encodings like JSON blobs (\texttt{"\{"name":\{"first":"emily","last":"riederer"\},"social":\{"twitter":"emilyriederer","github":"emilyriederer","linkedin":"emilyriederer"\}\}"} with hideous formatting for emphasis.) For a data publisher, this may be a convenience, but as we will see it can turn into a frustration or a liability when functions and comparison operations are attempted with strings that semantically represent a different type of value.

\hypertarget{dirty-strings-todo}{%
\subsection{Dirty Strings (TODO)}\label{dirty-strings-todo}}

whitespace

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"a"} \SpecialCharTok{==} \StringTok{"a"}
\StringTok{"a b"} \SpecialCharTok{==} \StringTok{"a b"}
\StringTok{"a b"} \SpecialCharTok{==} \StringTok{"a  b"}
\StringTok{"a b"} \SpecialCharTok{==} \StringTok{"a b "}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
## [1] TRUE
## [1] FALSE
## [1] FALSE
\end{verbatim}

``fancy'' characters (alternate encodings like ms word)

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{\textquotesingle{} " \textquotesingle{}} \SpecialCharTok{==} \StringTok{\textquotesingle{} " \textquotesingle{}}
\StringTok{\textquotesingle{}  \textquotesingle{}} \SpecialCharTok{==} \StringTok{\textquotesingle{} " \textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
## [1] FALSE
\end{verbatim}

special characters and display versus values

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \StringTok{"a}\SpecialCharTok{\textbackslash{}t}\StringTok{b"}
\FunctionTok{cat}\NormalTok{(x) }\CommentTok{\# what you see...}
\NormalTok{x }\SpecialCharTok{==} \StringTok{"a    b"} \CommentTok{\# ...is not what you get}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## a    b[1] FALSE
\end{verbatim}

\hypertarget{regular-expressions-todo}{%
\subsection{Regular Expressions (TODO)}\label{regular-expressions-todo}}

we promised not to be solution oriented, but

\emph{not} knowing regex is a disaster when trying to work with string data\ldots{}

\hypertarget{comparison-1}{%
\subsection{Comparison}\label{comparison-1}}

TODO

\hypertarget{string-ordering}{%
\subsubsection{String ordering}\label{string-ordering}}

Strings are ranked based on \emph{alphabetical order} just like a dictionary. Some properties of this ordering include that:

\begin{itemize}
\tightlist
\item
  numbers are smaller than letters (\texttt{1\ \textless{}\ "a"})
\item
  lower-case is smaller than upper case (\texttt{"a"\ \textless{}\ "A"})
\item
  fewer characters are smaller than more characters (\texttt{"a"\ \textless{}\ "aa"})
\end{itemize}

Such rules make perfect sense for true characters. However, when strings are used as a ``catch all'' to represent other structures, typical comparison operators can produce odd results. For example, it is generally uncontroverisal that ninety-one is less than one hundred twenty. However, the string \texttt{"91"} is \emph{greater than} \texttt{"120"} because only the character \texttt{"9"} is compared to the character \texttt{"1"}.\footnote{Similarly, \texttt{"91"\ \textgreater{}\ "905"} because since they both start with 9, we move on to compare 1 which is greater than 0.}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{91} \SpecialCharTok{\textless{}} \DecValTok{120}
\StringTok{"91"} \SpecialCharTok{\textless{}} \StringTok{"120"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
## [1] FALSE
\end{verbatim}

When strings are used to represent dates and times, comparison operators may or may not work depending on the precise formatting conversions. Below, we see that ``YYYYQQ''-formats sort correctly because the information is hierarchically nested; millenia are compared before centuries, centuries before decades, decades before years, and years before quarters. However, many other string representations of dates, like ``QQ-YYYY'' will not order correctly. Related topics will be discussed in the ``Dates and Times'' section.

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"20190Q4"} \SpecialCharTok{\textless{}} \StringTok{"2020Q3"} \CommentTok{\# string (alphabetic) ordering same as semantic ordering}
\StringTok{"Q4{-}2019"} \SpecialCharTok{\textless{}} \StringTok{"Q3{-}2020"} \CommentTok{\# string and semantic orderings are different}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
## [1] FALSE
\end{verbatim}

These examples demonstrate that we shouldn't rely on sorting schemes that follow different rules. Before doing comparisons on such types, its a safer bet to cast them to the format most truly representative of their types. If, for some reason, you do wish to keep them as strings, the second example shows that its is wise to format them in the most conducive format possible so things just work.

\hypertarget{type-coercion}{%
\subsubsection{Type coercion}\label{type-coercion}}

We discussed string comparison before when looking at ``dirty'' strings. More unexpected behavior arises when strings are compared across different data types. Many computing programs will attempt to \emph{coerce} the objects to a similar and comparable type. Sometimes, this can be convenient as operations ``just work'', but as always there is a cost for convenience. As we'll see, delegating important decisions to our computing engine may not always capture the semantic relationships that we are most interested in.

For example, consider compare a string and a number. To make them more comparable, R will convert them both to strings before checking for equality. Thus, the number \texttt{2020} is equivalent to the string \texttt{"2020"} but not the string \texttt{"02020"}.

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"2020"} \SpecialCharTok{==} \DecValTok{2020}
\StringTok{"02020"} \SpecialCharTok{==} \DecValTok{2020}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
## [1] FALSE
\end{verbatim}

In contrast, SQLite\footnote{Different versions of SQL may differ} thinks that the string \texttt{\textquotesingle{}2020\textquotesingle{}} is greater than the number 2020 and that these two quantities are not equal.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}
  \ControlFlowTok{case} \ControlFlowTok{when}     \StringTok{\textquotesingle{}2020\textquotesingle{}} \OperatorTok{=}  \DecValTok{2020} \ControlFlowTok{then} \DecValTok{1} \ControlFlowTok{else} \DecValTok{0} \ControlFlowTok{end} \KeywordTok{as}\NormalTok{ is\_eq,}
  \ControlFlowTok{case} \ControlFlowTok{when} \KeywordTok{not} \StringTok{\textquotesingle{}2020\textquotesingle{}} \OperatorTok{==} \DecValTok{2020} \ControlFlowTok{then} \DecValTok{1} \ControlFlowTok{else} \DecValTok{0} \ControlFlowTok{end} \KeywordTok{as}\NormalTok{ not\_eq,}
  \ControlFlowTok{case} \ControlFlowTok{when}     \StringTok{\textquotesingle{}2020\textquotesingle{}} \OperatorTok{\textless{}}  \DecValTok{2020} \ControlFlowTok{then} \DecValTok{1} \ControlFlowTok{else} \DecValTok{0} \ControlFlowTok{end} \KeywordTok{as}\NormalTok{ is\_lt,}
  \ControlFlowTok{case} \ControlFlowTok{when}     \StringTok{\textquotesingle{}2020\textquotesingle{}} \OperatorTok{\textgreater{}}  \DecValTok{2020} \ControlFlowTok{then} \DecValTok{1} \ControlFlowTok{else} \DecValTok{0} \ControlFlowTok{end} \KeywordTok{as}\NormalTok{ is\_gt}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   is_eq not_eq is_lt is_gt
## 1     0      1     0     1
\end{verbatim}

\^{}TODO: where this could cause problems (FIPS example?)

\hypertarget{transformation-todo}{%
\subsection{Transformation (TODO)}\label{transformation-todo}}

basic things like addition differ by language

in R, returns error:

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{\textquotesingle{}a\textquotesingle{}} \SpecialCharTok{+} \StringTok{\textquotesingle{}b\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in "a" + "b": non-numeric argument to binary operator
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{\textquotesingle{}a\textquotesingle{}} \SpecialCharTok{*} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in "a" * 5: non-numeric argument to binary operator
\end{verbatim}

in SQLite, goes to zero:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}
  \StringTok{\textquotesingle{}a\textquotesingle{}} \OperatorTok{+} \StringTok{\textquotesingle{}b\textquotesingle{}} \KeywordTok{as}\NormalTok{ string\_add,}
  \StringTok{\textquotesingle{}a\textquotesingle{}}\OperatorTok{*}\DecValTok{5} \KeywordTok{as}\NormalTok{ string\_mult}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   string_add string_mult
## 1          0           0
\end{verbatim}

in python, does concatenation for \texttt{+} and analogous (concatenation of repeat) for \texttt{*}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\textquotesingle{}a\textquotesingle{}} \OperatorTok{+} \StringTok{\textquotesingle{}b\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'ab'
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\textquotesingle{}a\textquotesingle{}} \OperatorTok{*} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'aaaaa'
\end{verbatim}

\hypertarget{dates-and-times-wip}{%
\section{Dates and Times (WIP)}\label{dates-and-times-wip}}

Unlike character strings, dates and times seem like they should be well defined with distinct, quantifiable components like years, months, and days. However, many different conventions for date formatting and underlying storage formats exist. This leads to similar challenges with dates and times as we saw with strings before.

Some common formats in the wild are:

\begin{itemize}
\tightlist
\item
  YYYYMMDD
\item
  YYYYMM
\item
  MMDDYYYY
\item
  DDMMYYYY
\item
  MM/DD/YYYY
\item
  MM/DD/YY
\item
  DD/MM/YYYY
\item
  YYYY-MM-DD (ISO8601)
\end{itemize}

In addition to how dates are \emph{formatted}, they may be stored in a variety of different ways ``under the hood'' such as Unix time (seconds since 1970-01-01 00:00:00 UTC) and Julian time (days since noon in Greenwich on November 24, 4714 B.C) (TODO).

To complicate matters further, many of these formats may be represented either by native date types in various programs or by more basic data types (such as integers for the first four and strings for the last four). In addition, analogous formats exist for \emph{timestamps} which encode both calendar date and time of day (hour, minute, and second information).

TODO: why ISO8601?

\hypertarget{comparison-2}{%
\subsection{Comparison}\label{comparison-2}}

Automatic conversion of data types
Dates versus timestamps

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_dt }\OtherTok{\textless{}{-}}
\FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{DT\_ENROLL =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2020{-}01{-}01"}\NormalTok{),}
  \AttributeTok{DT\_PURCH  =} \DecValTok{20200101}\NormalTok{,}
  \AttributeTok{DT\_LOGIN  =} \FunctionTok{as.POSIXlt}\NormalTok{(}\StringTok{"2020{-}01{-}01T12:00:00"}\NormalTok{) }
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

none of these are equal so nothing returns on filtering

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{filter}\NormalTok{(df\_dt, DT\_ENROLL }\SpecialCharTok{==}\NormalTok{ DT\_PURCH) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{nrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{filter}\NormalTok{(df\_dt, DT\_ENROLL }\SpecialCharTok{==}\NormalTok{ DT\_LOGIN) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{nrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in mask$eval_all_filter(dots, env_filter):
## Incompatible methods ("Ops.Date", "Ops.POSIXt") for
## "=="
\end{verbatim}

\begin{verbatim}
## [1] 0
\end{verbatim}

the same thing happens in sql

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select} \OperatorTok{*} \KeywordTok{from}\NormalTok{ df\_dt }\KeywordTok{where}\NormalTok{ DT\_ENROLL }\OperatorTok{=}\NormalTok{ DT\_PURCH}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] DT_ENROLL DT_PURCH  DT_LOGIN 
## <0 rows> (or 0-length row.names)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select} \OperatorTok{*} \KeywordTok{from}\NormalTok{ df\_dt }\KeywordTok{where}\NormalTok{ DT\_ENROLL }\OperatorTok{=}\NormalTok{ DT\_LOGIN}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] DT_ENROLL DT_PURCH  DT_LOGIN 
## <0 rows> (or 0-length row.names)
\end{verbatim}

in what way aren't they equal? to understand this its helpful to know how the computer encodes these dates

with \texttt{as.numeric()} in R we can see the numeric representation of the date

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.numeric}\NormalTok{(df\_dt}\SpecialCharTok{$}\NormalTok{DT\_ENROLL)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 18262
\end{verbatim}

this works the same way in SQL

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select} \FunctionTok{cast}\NormalTok{(DT\_ENROLL }\KeywordTok{as} \DataTypeTok{integer}\NormalTok{), }\FunctionTok{cast}\NormalTok{(DT\_PURCH }\KeywordTok{as} \DataTypeTok{integer}\NormalTok{) }\KeywordTok{from}\NormalTok{ df\_dt}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   cast(DT_ENROLL as integer) cast(DT_PURCH as integer)
## 1                      18262                  20200101
\end{verbatim}

this has the implication that things that are on the same date have an inequality relationship in both languages

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{filter}\NormalTok{(df\_dt, DT\_ENROLL }\SpecialCharTok{\textless{}}\NormalTok{ DT\_PURCH) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{nrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select} 
  \FunctionTok{cast}\NormalTok{(DT\_ENROLL }\KeywordTok{as} \DataTypeTok{integer}\NormalTok{), }
  \ControlFlowTok{case} \ControlFlowTok{when}\NormalTok{ DT\_ENROLL }\OperatorTok{\textless{}} \DecValTok{18000} \ControlFlowTok{then} \DecValTok{1} \ControlFlowTok{else} \DecValTok{0} \ControlFlowTok{end} \KeywordTok{as}\NormalTok{ lt\_18000,}
  \ControlFlowTok{case} \ControlFlowTok{when}\NormalTok{ DT\_ENROLL }\OperatorTok{\textless{}} \DecValTok{19000} \ControlFlowTok{then} \DecValTok{1} \ControlFlowTok{else} \DecValTok{0} \ControlFlowTok{end} \KeywordTok{as}\NormalTok{ lt\_19000,}
  \ControlFlowTok{case} \ControlFlowTok{when}\NormalTok{ DT\_ENROLL }\OperatorTok{\textless{}}\NormalTok{ DT\_PURCH }\ControlFlowTok{then} \DecValTok{1} \ControlFlowTok{else} \DecValTok{0} \ControlFlowTok{end} \KeywordTok{as}\NormalTok{ lt\_purch}
\KeywordTok{from}\NormalTok{ df\_dt}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   cast(DT_ENROLL as integer) lt_18000 lt_19000
## 1                      18262        0        1
##   lt_purch
## 1        1
\end{verbatim}

Note this this can affect both filters and joins

and this similarly causes a more general problem when comparing a date to a date-as-an-integer

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.Date}\NormalTok{(}\StringTok{"2020{-}01{-}01"}\NormalTok{) }\SpecialCharTok{\textgreater{}} \DecValTok{20160501}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select} \FunctionTok{cast}\NormalTok{(}\StringTok{\textquotesingle{}2020{-}01{-}01\textquotesingle{}} \KeywordTok{as} \DataTypeTok{date}\NormalTok{) }\OperatorTok{\textgreater{}} \DecValTok{20160501}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   cast('2020-01-01' as date) > 20160501
## 1                                     0
\end{verbatim}

\hypertarget{programming-errors-todo}{%
\section{Programming Errors (TODO)}\label{programming-errors-todo}}

\hypertarget{default-cases-wip}{%
\subsection{Default Cases (WIP)}\label{default-cases-wip}}

see case-when example in nulls section

\hypertarget{order-of-operations-wip}{%
\subsection{Order of Operations (WIP)}\label{order-of-operations-wip}}

PEMDAS but sometimes still ambiguous

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}  \SpecialCharTok{*} \DecValTok{2} \SpecialCharTok{/} \DecValTok{3} \SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*} \DecValTok{2} \SpecialCharTok{/} \DecValTok{3} \SpecialCharTok{{-}} \DecValTok{1}
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{*} \DecValTok{2} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{3} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6667
## [1] 0.3333
## [1] 2
\end{verbatim}

SQL clause order of evaluations

\hypertarget{object-references-wip}{%
\subsection{Object References (WIP)}\label{object-references-wip}}

Copying and modifying object overview

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/comp-quan/ref-val} 

}

\caption{Different relationships between named variables and their values}\label{fig:unnamed-chunk-129}
\end{figure}

When might each be preferred?

What risks are there if we don't understand which we are doing?

In Python

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x}
\NormalTok{y.append(}\DecValTok{4}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(y)}
\BuiltInTok{print}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1, 2, 3, 4]
## [1, 2, 3, 4]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{=}\NormalTok{ x.copy()}
\NormalTok{z.append(}\DecValTok{5}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(z)}
\BuiltInTok{print}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1, 2, 3, 4, 5]
## [1, 2, 3, 4]
\end{verbatim}

pandas DataFrame methods with \texttt{inplace} arg (\texttt{False} is default)

In R

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(data.table)}

\NormalTok{DT }\OtherTok{\textless{}{-}} \FunctionTok{data.table}\NormalTok{(}\AttributeTok{a=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{b=}\FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{,}\DecValTok{12}\NormalTok{))}
\FunctionTok{print}\NormalTok{(DT)}

\NormalTok{newDT }\OtherTok{\textless{}{-}}\NormalTok{ DT        }\CommentTok{\# reference, not copy}
\NormalTok{newDT[}\DecValTok{1}\NormalTok{, a }\SpecialCharTok{:}\ErrorTok{=} \DecValTok{100}\NormalTok{] }\CommentTok{\# modify new DT}

\FunctionTok{print}\NormalTok{(DT)          }\CommentTok{\# DT is modified too.}

\NormalTok{DT }\OtherTok{=} \FunctionTok{data.table}\NormalTok{(}\AttributeTok{a=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{b=}\FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{,}\DecValTok{12}\NormalTok{))}
\NormalTok{newDT }\OtherTok{\textless{}{-}}\NormalTok{ DT        }
\NormalTok{newDT}\SpecialCharTok{$}\NormalTok{b[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{200}  \CommentTok{\# new operation}
\NormalTok{newDT[}\DecValTok{1}\NormalTok{, a }\SpecialCharTok{:}\ErrorTok{=} \DecValTok{100}\NormalTok{]}

\FunctionTok{print}\NormalTok{(DT)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    a  b
## 1: 1 11
## 2: 2 12
##      a  b
## 1: 100 11
## 2:   2 12
##    a  b
## 1: 1 11
## 2: 2 12
\end{verbatim}

From \url{https://stackoverflow.com/questions/10225098/understanding-exactly-when-a-data-table-is-a-reference-to-vs-a-copy-of-another}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\CommentTok{\# set{-}up sample data {-}{-}{-}{-}}
\NormalTok{data }\OperatorTok{=}\NormalTok{ \{}\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{: [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{], }
        \StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{: [}\DecValTok{11}\NormalTok{, }\DecValTok{12}\NormalTok{]\}}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(data }\OperatorTok{=}\NormalTok{ data)}

\CommentTok{\# rename columns without replacing {-}{-}{-}{-}}
\NormalTok{df.rename(columns }\OperatorTok{=}\NormalTok{ \{}\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    x   b
## 0  1  11
## 1  2  12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}

\CommentTok{\# rename columns with replacing {-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    a   b
## 0  1  11
## 1  2  12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.rename(columns }\OperatorTok{=}\NormalTok{ \{}\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{\}, inplace }\OperatorTok{=} \VariableTok{True}\NormalTok{)}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    x   b
## 0  1  11
## 1  2  12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{add\_ones }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
  
\NormalTok{  data}\SpecialCharTok{$}\NormalTok{x0 }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(data))}
  
\NormalTok{\}}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x1 =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x1
## 1  1
## 2  2
## 3  3
## 4  4
## 5  5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{add\_ones}\NormalTok{(df)}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x1
## 1  1
## 2  2
## 3  3
## 4  4
## 5  5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{add\_ones }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
  
\NormalTok{  data}\SpecialCharTok{$}\NormalTok{x0 }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(data))}
  \FunctionTok{return}\NormalTok{(data)}
  
\NormalTok{\}}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x1 =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x1
## 1  1
## 2  2
## 3  3
## 4  4
## 5  5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{add\_ones}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x1 x0
## 1  1  0
## 2  2  0
## 3  3  0
## 4  4  0
## 5  5  0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x1
## 1  1
## 2  2
## 3  3
## 4  4
## 5  5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df2 }\OtherTok{\textless{}{-}} \FunctionTok{add\_ones}\NormalTok{(df)}
\NormalTok{df2 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x1 x0
## 1  1  0
## 2  2  0
## 3  3  0
## 4  4  0
## 5  5  0
\end{verbatim}

\hypertarget{trusting-tools}{%
\section{Trusting Tools}\label{trusting-tools}}

\hypertarget{delegating-decisions}{%
\subsection{Delegating decisions}\label{delegating-decisions}}

A theme throughout this book is the fundamentally \emph{social} nature of data analysis. Data analysis is fraught without understanding the countless decisions made along the way by those who generated it (whose data is reflected), those who collected it, those who migrated it, and those who have posed questions of it. On one hand, this is a beautiful aspect of analysis; on the other hand, it means that analysts and their analyses are subject to all of the cognitive and social psychological biases of everyday humans.

One such bias is ``social proof'': assuming that if a tool behaves a certain way, it must be because it is correct.

Assuming that our tools know best is admittedly an attractive proposition. It appeals to a desire to think that someone, somewhere is ``in charge'' and, perhaps more critically, helps us avoid a domino effect of distrust (If we \emph{don't} trust our tools how can we trust our results? And if we can't trust our results, how can we trust anything at all?) Unfortunately, there are many reasons are tools might not know best. For example, the tool's developer might have:

\begin{itemize}
\tightlist
\item
  Made a mistake
\item
  Had a different analysis problem in mind with a different optimal approach
\item
  Been optimizing for a different constraint (e.g.~explainability vs.~accuracy, speed vs.~theoretical properties)
\item
  Come from a community with different norms
\item
  Been affording users the flexibility to do things many ways even if they don't agree
\item
  Built a certain feature for a different purpose than how you are using it
\item
  Not thought about it at all
\end{itemize}

As a few concrete examples from popular open source tools. We'll look briefly at the prominent python library \texttt{scikitlearn} for machine learning and Apache Spark, an engine for large-scale distributed data processing.

\hypertarget{defaults-in-scikitlearn}{%
\subsubsection{\texorpdfstring{Defaults in \texttt{scikitlearn}}{Defaults in scikitlearn}}\label{defaults-in-scikitlearn}}

\texttt{scikitlearn}'s default behavior for logistic regression modeling\footnote{A classic modeling technique for predicting binary (yes/no) outcomes} automatically applies L2 regularization. You might or might not know what this means, and you might or might not want to apply it to your problem. That's fine. The important thing is that it \emph{will} change your estimates and predictions, and it is \emph{not} a part of the classical definition of that algorithm (for modelers coming from a statistical background.)

Of course, there's nothing inherently wrong about this choice; the library authors just had different goals than a typical statistical. \texttt{scikitlearn} developer Olivier Grisel explains \href{https://twitter.com/ogrisel/status/1167438229655773186?s=20}{on Twitter} that this choice (and others in the library) is explained because ``Scikit-learn was always designed to make it easy to get good predictive accuracy (eg as measured by CV) rather than as statistical inference library.'' Additionally, this choice is documented in bold \href{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html}{in the function documentation}.

However, an analyst could easily miss this nuance if they do not \emph{read} the documentation. Or, if they \emph{misinterpret} this choice as social proof that regularization is always the right approach, they might not make the best choice for their own analysis.

\hypertarget{algorithms-in-spark}{%
\subsubsection{\texorpdfstring{Algorithms in \texttt{Spark}}{Algorithms in Spark}}\label{algorithms-in-spark}}

As a second example, according to a 2015 \href{https://issues.apache.org/jira/browse/SPARK-5133}{Jira ticket}, developers of Spark considered multiple methodologies they could use when adding the functionality to compute feature importance for a random forest. Ultimately, a core contributor advised against permutation importance due to its computational cost.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/comp-quan/spark-jira} 

}

\caption{JIRA ticket for Spark with a discussion of which random forest variable importance algorithm to implement}\label{fig:unnamed-chunk-136}
\end{figure}

Clearly, no one wants a workflow that is too costly or timely to run. So, once again, there is no right or wrong. However, since every approach to feature importance has its own biases, pitfalls, and challenges in interpretation, it's a mistake for an end-user to not carefully understand which algorithm is used and why.

\hypertarget{off-label-use-todo}{%
\subsection{``Off-Label'' Use (TODO)}\label{off-label-use-todo}}

coined in \url{https://www.rstudio.com/resources/rstudioglobal-2021/maintaining-the-house-the-tidyverse-built/}

\hypertarget{security-todo}{%
\subsection{Security (TODO)}\label{security-todo}}

namespace squatting

executable code

\hypertarget{inefficient-processing-todo}{%
\section{Inefficient Processing (TODO)}\label{inefficient-processing-todo}}

\hypertarget{strategies-wip}{%
\section{Strategies (WIP)}\label{strategies-wip}}

Paragraph 1 TODO

Some computational quandaries are inherent to our tools themselves, but often they are a function both of the tools and the ways we chose to use them. More strategies related to writing robust and resilient code will be discussed in Chapter \ref{comp-code} (Complexify Code).

\hypertarget{understand-the-intent-1}{%
\subsection{Understand the intent}\label{understand-the-intent-1}}

\begin{itemize}
\tightlist
\item
  read the docs
\item
  look at examples
\item
  don't carry default knowledge between languages
\end{itemize}

\hypertarget{understand-the-execution-1}{%
\subsection{Understand the execution}\label{understand-the-execution-1}}

\begin{itemize}
\tightlist
\item
  test out simple examples (like we've been doing)
\item
  specificlly try out corner cases
\end{itemize}

\hypertarget{be-explicit-not-implicit}{%
\subsection{Be explicit not implicit}\label{be-explicit-not-implicit}}

\begin{itemize}
\tightlist
\item
  default arguments
\item
  examples above with casting, coalescingW
\end{itemize}

\hypertarget{real-world-disasters-wip}{%
\section{Real World Disasters (WIP)}\label{real-world-disasters-wip}}

\url{https://www.theguardian.com/politics/2020/oct/05/how-excel-may-have-caused-loss-of-16000-covid-tests-in-england}

\begin{quote}
The data error, which led to 15,841 positive tests being left off the official daily figures, means than 50,000 potentially infectious people may have been missed by contact tracers and not told to self-isolate.
\end{quote}

\hypertarget{part-analysis}{%
\part*{Analysis}\label{part-analysis}}


\hypertarget{eg-agg}{%
\chapter{Egregious Aggregations (WIP)}\label{eg-agg}}

Once armed with an understanding of the data and tools available for analysis, a common start to analysis is exploring data with \emph{aggregation}.
At its heart, any sort of data analysis is the process of condensing raw data into something more manageable and useful while giving up as little of the information as possible.
From linear regressions and hypothesis testing to random forests and beyond, much of data analysis could truly be called ``applied sums an averages''.

Many elementary tools for this task are much better at the comprehension task than the preservation one.
We learn rigorous assumptions to consider and validate when studying linear regression, but basic arithmetic aggregation presents itself as agnostic and welcome to any type of data.
However, the underlying distributions of our variables and the relationships between them have a significant impact on the how informative and interpretable various summarizations are.

In this chapter, we will explore different ways that univariate and multivariate aggregations can be naive or uninformative.

\hypertarget{motivating-example-similar-in-summary}{%
\section{Motivating Example: Similar in Summary}\label{motivating-example-similar-in-summary}}

To begin, we will look at a whimsical toy example.
This may feel trite or manufactured, but the subsequent sections will aim to convince you that these issues are not just esoteric.
Consider the ``datasaurus dozen'' dataset \citep{datasaurus} which is available within the \texttt{datasauRus} R package \citep{R-datasauRus}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(datasauRus)}
\end{Highlighting}
\end{Shaded}

This dataset contains 12 sets of data stacked on top of one another and identified by the \texttt{dataset} column.\footnote{If you are following along in R, you might run \texttt{unique(df\$dataset)} to see all the values of this column. We won't do that now as to not ruin the surprise}.
Besides the identifier column, the data is fairly small and contains only two variables \texttt{x} and \texttt{y}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ datasauRus}\SpecialCharTok{::}\NormalTok{datasaurus\_dozen}
\FunctionTok{head}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   dataset     x     y
##   <chr>   <dbl> <dbl>
## 1 dino     55.4  97.2
## 2 dino     51.5  96.0
## 3 dino     46.2  94.5
## 4 dino     42.8  91.4
## 5 dino     40.8  88.3
## 6 dino     38.7  84.9
\end{verbatim}

A quick analysis of summary statistics reveals that each of the 12 datasets is very consistent in its summary statistics.
The means and variances of \texttt{x} and \texttt{y} and even their correlations are nearly identifcal.

\begin{table}

\caption{\label{tab:datasaurus-tbl}Summary statistics for Datasaurus Dozen datasets}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
dataset & mean(x) & mean(y) & var(x) & var(y) & cor(x, y)\\
\hline
away & 54.27 & 47.84 & 281.2 & 725.8 & -0.064\\
\hline
bullseye & 54.27 & 47.83 & 281.2 & 725.5 & -0.069\\
\hline
circle & 54.27 & 47.84 & 280.9 & 725.2 & -0.068\\
\hline
dino & 54.26 & 47.83 & 281.1 & 725.5 & -0.064\\
\hline
dots & 54.26 & 47.84 & 281.2 & 725.2 & -0.060\\
\hline
h\_lines & 54.26 & 47.83 & 281.1 & 725.8 & -0.062\\
\hline
high\_lines & 54.27 & 47.84 & 281.1 & 725.8 & -0.069\\
\hline
slant\_down & 54.27 & 47.84 & 281.1 & 725.6 & -0.069\\
\hline
slant\_up & 54.27 & 47.83 & 281.2 & 725.7 & -0.069\\
\hline
star & 54.27 & 47.84 & 281.2 & 725.2 & -0.063\\
\hline
v\_lines & 54.27 & 47.84 & 281.2 & 725.6 & -0.069\\
\hline
wide\_lines & 54.27 & 47.83 & 281.2 & 725.7 & -0.067\\
\hline
x\_shape & 54.26 & 47.84 & 281.2 & 725.2 & -0.066\\
\hline
\end{tabular}
\end{table}

However, as shown in Figure \ref{fig:datasaurus-plot}, when we visualize this data, we find that the 12 datasets reveal remarkably different patterns.

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/datasaurus-plot-1.pdf}
\caption{\label{fig:datasaurus-plot}Scatterplots for Datasaurus Dozen datasets}
\end{figure}

This dataset is a more elaborate version of Anscombe's Quartet, a well-known set of four datasets which exhibit similar properties.
Examining a simiilar plot for Anscombe's Quartet (with data from the \texttt{anscombe} dataset which ships in R's \texttt{datasets} package), we can get better intuition for how the phenomenon is manufactured.
Figure \ref{fig:anscombe} shows as similar plot to \ref{fig:datasaurus-plot}.
Comparing datasets 1 and 3, for example, we can see a trade-off between a semi-strong trend with a consistent-seeming amount of noise and an nearly perfect linear trend with a single outlier.

\begin{verbatim}
## Warning: Values are not uniquely identified; output will contain list-cols.
## * Use `values_fn = list` to suppress this warning.
## * Use `values_fn = length` to identify where the duplicates arise
## * Use `values_fn = {summary_fun}` to summarise duplicates
\end{verbatim}

\begin{verbatim}
## Warning: `cols` is now required when using unnest().
## Please use `cols = c(x, y)`
\end{verbatim}

\begin{verbatim}
## `geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/anscombe-1.pdf}
\caption{\label{fig:anscombe}Scatterplots for Anscombe's Quartet}
\end{figure}

Figure \ref{fig:anscombe} also plots the simple linear regression line for each dataset.
Similar to the summary statistics, these are also identical.
We know this because the regression coefficient for a simple linear regression is given by \texttt{cov(x,y)}/\texttt{sd(x)sd(y)}. You'll notice I do not write ``we can see that\ldots{}'' because, in fact, we can only \emph{see} similarity not equality.
The message of this section may seem to be ``don't summarize your data without plotting it'', but conducting ``visual analytics'' without looking at the numbers is also problematic.
We'll explore the latter topic more in Chapter \ref{vexi-visu} (Vexing Visualiztions).

While there are clearly a contrived example (and, if you so chose to check out the ``Datasaurus Dozen'' paper, a very cleverly contrived example!), its also a cautionary tale.
Summary statistics are not just insufficient when they focus on central tendency (e.g.~mean) instead of spread.
In this example, even an examination of variation and covariation led to an overly simplistic view of the underlying data.

\hypertarget{averages-wip}{%
\section{Averages (WIP)}\label{averages-wip}}

\hypertarget{implicit-assumptions-todo}{%
\subsection{Implicit assumptions (TODO)}\label{implicit-assumptions-todo}}

When statistics students study linear regression, they are introduced to a number of canonical assumptions including:

\begin{itemize}
\tightlist
\item
  The true functional form between the dependent and independent variables is linear / additive
\item
  Errors are independent
\item
  Errors have constant variance (that is, they are homoskedastic)
\item
  Errors have a normal distribution
\end{itemize}

Of course, whether or not these assumptions hold, there's nothing stopping anyone from \emph{mechanically} fit at linear regression\footnote{In fact, the only mechanical constraint to computing linear regression output is that no column of the design matrix (no independent variable) is a precise linear combination of the other columns. Yet this constraint is not typically included among the standard statement of assumptions.}. Instead, these assumptions are required to make the output of a linear regression \emph{meaningful} and, more specifically, for conducting correct inference.

Similarly, there are no limitations on mechanically computing an average

\hypertarget{averaging-skewed-data}{%
\subsection{Averaging skewed data}\label{averaging-skewed-data}}

Arithmetic average versus colloquial meaning of average as ``typical''

Skewed data

Multimodal data / mixture models

\hypertarget{no-average-observation}{%
\subsection{No ``average'' observation}\label{no-average-observation}}

In the previous section, the average represented a point in the relevant data \emph{range} even if it was not perhaps the one most representative of a ``typical'' observation.
We discussed how in some situations this quantity may be a reasonable answer to certain types of questions and an aid for certain types of decisions.

However, when we seek an average \emph{profile} over multiple variables, the problems of averages are further compounded.
We may end up with a set of ``average'' summary statistics that are not representative of any part of our population.

To see this, let's assume we are working with data for a company with a subscription business model.
We might be interested in profiling the age of each account (how long they have been a subscriber) and their activity (measured by amount spent on an e-commerce platform, files downloaded on a streaming service, etc.)

The following code simulates a set of observations:
80\% of accounts are between 0 to 3 years in age and have an average activity level of 100 while 20\% of accounts are older than 3 years in age and have an average activity level of 500.
(Don't over-think the specific probability distributions lived here.
We are concerned with interrogating the properties of the average and not with simulating a realistic data generating process.
Giving ourselves permission to be wrong or ``lazy'' about unimportant things gives us more energy to focus on what matters.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# define simulation parameters {-}{-}{-}{-}}
\DocumentationTok{\#\# n: total observations}
\DocumentationTok{\#\# p: proportion of observations in group 1}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{p }\OtherTok{\textless{}{-}} \FloatTok{0.8}
\NormalTok{n1 }\OtherTok{\textless{}{-}}\NormalTok{ n}\SpecialCharTok{*}\NormalTok{p}
\NormalTok{n2 }\OtherTok{\textless{}{-}}\NormalTok{ n}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{p)}

\CommentTok{\# generate fake dataset with two groups {-}{-}{-}{-}}
\NormalTok{df }\OtherTok{\textless{}{-}} 
  \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\FunctionTok{runif}\NormalTok{(n1,   }\DecValTok{0}\NormalTok{,  }\DecValTok{3}\NormalTok{), }\FunctionTok{runif}\NormalTok{(n2,   }\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{)),}
    \AttributeTok{act =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n1, }\DecValTok{100}\NormalTok{, }\DecValTok{10}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(n2, }\DecValTok{500}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Figure \ref{fig:multivar-avg} shows a scatterplot of the relationship between account age (x-axis) and activity level (y-axis).
Meanwhile, the marginal rug plots shows the univariate distribution of each variable.
The sole red dot denotes the coordinates of the average age and average activity.
Notably, this dot exists in a region of ``zero density'';
that is, it is not representative of \emph{any} customer.
Strategic decisions made with this sort of observation in mind as the ``typical'' might not be destined for success.

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/multivar-avg-1.pdf}
\caption{\label{fig:multivar-avg}A scatterplot of two variables and their averages}
\end{figure}

\hypertarget{the-product-of-averages}{%
\subsection{The product of averages}\label{the-product-of-averages}}

As the above example shows, averages of multivariate data can produce poor summaries -- particularly when these variables are interrelated\footnote{We intentionally avoid the word \emph{correlated} here to emphasize the fact that \emph{correlation} refers more strictly to linear relationships}.

A second implication of this observation is that deriving additional computations based on pre-averaged numbers is likely to obtain inaccurate results.

For example, consider that we wish to estimate the average dollar amount of returns per any e-commerce order.
Orders may generally be a mixture of low-price orders (around \$50 on average) and high-price orders (around \$250 on average).
Low-price orders may have a 10\% probability of being returned while high price orders have a 20\% probability.
(Again, are these numbers, distributions, or relationships hyper-realistic?
Not at all.
However, once again we are telling ourselves a story just to reason about numerical properties, so we have to give ourselves permission to not focus on irrelevant details.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# define simulation parameters {-}{-}{-}{-}}
\DocumentationTok{\#\# n: observations per group}
\DocumentationTok{\#\# pr[1|2]: mean price per group}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{pr1 }\OtherTok{\textless{}{-}} \DecValTok{50}
\NormalTok{pr2 }\OtherTok{\textless{}{-}} \DecValTok{250}
\NormalTok{pr\_sd }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{re1 }\OtherTok{\textless{}{-}} \FloatTok{0.1}
\NormalTok{re2 }\OtherTok{\textless{}{-}} \FloatTok{0.2}

\CommentTok{\# simulate spend amounts and return indicators {-}{-}{-}{-}}
\NormalTok{amt\_spend  }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n, pr1, pr\_sd), }\FunctionTok{rnorm}\NormalTok{(n, pr2, pr\_sd))}
\NormalTok{ind\_return }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, re1),    }\FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, re2))}

\CommentTok{\# compute summary statistics {-}{-}{-}{-}}
\NormalTok{average\_of\_product }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(amt\_spend }\SpecialCharTok{*}\NormalTok{ ind\_return)}
\NormalTok{product\_of\_average }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(amt\_spend) }\SpecialCharTok{*} \FunctionTok{mean}\NormalTok{(ind\_return)}
\end{Highlighting}
\end{Shaded}

The \emph{true} average amount returned across all of our orders is 36.0438 (from the \texttt{average\_of\_product} variable).
However, if instead we already knew an average spend amount and an average return proportion, we might be inclined to compute the \texttt{product\_of\_average} method which returns a value of 26.9923.
(This is a difference of 9.05 relative to an average purchase amount of 150.)

At first, this may seem unintuitive until we write out the formulas and realize that these metrics are, in fact, two very different quantities:

\[ \frac{ \sum_{1}^{n} Spend }{\sum_{1}^{n} 1} \ * \frac{ \sum_{1}^{n} I(Return) }{\sum_{1}^{n} 1} \] over all \(n\) orders

versus

\[ \frac{\sum_{1}^{n} Spend * I(Return)}{\sum_{1}^{n} 1} \]

If this still feels counterintuitive, we can see how much of the difference is accounted for by the interrelation between our two variables.
In the following code, we break the relationship between the variables by randomly reordering the \texttt{ind\_return} variable so it is no longer has any true relationship to the corresponding \texttt{amt\_spend} variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# randomly reorder one of two variables to break relationships {-}{-}{-}{-}}
\NormalTok{ind\_return }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(ind\_return, }\AttributeTok{size =} \DecValTok{200}\NormalTok{)}

\CommentTok{\# recompute variables {-}{-}{-}{-}}
\NormalTok{average\_of\_product }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(amt\_spend }\SpecialCharTok{*}\NormalTok{ ind\_return)}
\NormalTok{product\_of\_average }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(amt\_spend) }\SpecialCharTok{*} \FunctionTok{mean}\NormalTok{(ind\_return)}
\end{Highlighting}
\end{Shaded}

After redoing the calculations, we find that th two values are much closer.
\texttt{average\_of\_product} is now 24.1041 and \texttt{product\_of\_average} is now 26.9923.
These are notably still not the same number so that does not mean that these two equations are equivalent if variables are unrelated;
however, this second result once again illustrates the extent to which interrelations can defy our naive intuitions.

\hypertarget{average-over-what-todo}{%
\subsection{Average over what? (TODO)}\label{average-over-what-todo}}

no such thing as an unweighted average (just sometimes weights are equal)

formal definition of expected value forces you to pick a probability distribution

eg avg mpg by time vs by mileage?

not strictly an error but our language allows an ill-defined problem

\hypertarget{dichotomization-and-distributions}{%
\subsection{Dichotomization and distributions}\label{dichotomization-and-distributions}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000}

\CommentTok{\# simulate x and y: uniformly distributed x {-}{-}{-}{-}}
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n)}
\NormalTok{y1 }\OtherTok{\textless{}{-}} \DecValTok{5} \SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{x1 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\CommentTok{\# simulate x and y: same relationship, more concentrated distribution of x {-}{-}{-}{-}}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{( }\FunctionTok{runif}\NormalTok{(n}\SpecialCharTok{*}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.00}\NormalTok{, }\FloatTok{0.44}\NormalTok{), }
         \FunctionTok{runif}\NormalTok{(n}\SpecialCharTok{*}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.45}\NormalTok{, }\FloatTok{0.55}\NormalTok{), }
         \FunctionTok{runif}\NormalTok{(n}\SpecialCharTok{*}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.55}\NormalTok{, }\FloatTok{1.00}\NormalTok{) }
\NormalTok{        )}
\NormalTok{y2 }\OtherTok{\textless{}{-}} \DecValTok{5} \SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{x2 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\CommentTok{\# com}
\NormalTok{g1 }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(x1 }\SpecialCharTok{\textless{}} \FloatTok{0.5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{means1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{mean}\NormalTok{(y1[g1 }\SpecialCharTok{==} \DecValTok{0}\NormalTok{]), }\FunctionTok{mean}\NormalTok{(y1[g1 }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]))}
\NormalTok{means1 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.814 7.254
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g2 }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(x2 }\SpecialCharTok{\textless{}} \FloatTok{0.5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{means2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{mean}\NormalTok{(y2[g2 }\SpecialCharTok{==} \DecValTok{0}\NormalTok{]), }\FunctionTok{mean}\NormalTok{(y2[g2 }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]))}
\NormalTok{means2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6.218 6.763
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{means1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.814 7.254
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{means2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6.218 6.763
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(x1, y1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6424
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(x2, y2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3688
\end{verbatim}

\hypertarget{small-sample-sizes}{%
\subsection{Small sample sizes}\label{small-sample-sizes}}

\hypertarget{proportions-wip}{%
\section{Proportions (WIP)}\label{proportions-wip}}

note that these are of course just a type of average (average of indicators) but helpful to examine challenges separately

\hypertarget{picking-the-right-denominator}{%
\subsection{Picking the right denominator}\label{picking-the-right-denominator}}

\hypertarget{sample-size-effects}{%
\subsection{Sample size effects}\label{sample-size-effects}}

\hypertarget{variation-todo}{%
\section{Variation (TODO)}\label{variation-todo}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{20}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\FunctionTok{mean}\NormalTok{(x)}
\FunctionTok{var}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
## [1] 175
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{15}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{15}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{15}\NormalTok{)}
\FunctionTok{mean}\NormalTok{(x)}
\FunctionTok{var}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
## [1] 158.3
\end{verbatim}

\hypertarget{correlation-wip}{%
\section{Correlation (WIP)}\label{correlation-wip}}

As shown in \ref{fig:fig-fun-corr},

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/fig-fun-corr-1.pdf}
\caption{\label{fig:fig-fun-corr}Plots of x from 1 to 10 over a range of common functions}
\end{figure}

\hypertarget{linear-relationships-only}{%
\subsection{Linear relationships only}\label{linear-relationships-only}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{linear =}\NormalTok{ x, }\AttributeTok{quadratic =}\NormalTok{ x}\SpecialCharTok{**}\DecValTok{2}\NormalTok{, }\AttributeTok{cubic =}\NormalTok{ x}\SpecialCharTok{**}\DecValTok{3}\NormalTok{, }\AttributeTok{quartic =}\NormalTok{ x}\SpecialCharTok{**}\DecValTok{4}\NormalTok{)}

\FunctionTok{vapply}\NormalTok{(y, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(z) }\FunctionTok{cor}\NormalTok{(x, z, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{), }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    linear quadratic     cubic   quartic 
##    1.0000    0.9746    0.9284    0.8817
\end{verbatim}

\hypertarget{multiple-forms}{%
\subsection{Multiple forms}\label{multiple-forms}}

Traditional (Pearson) correlation depends on specific values whereas Spearman and Kendall focus on order statistics

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# polynomials {-}{-}{-}{-}}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{linear =}\NormalTok{ x, }\AttributeTok{quadratic =}\NormalTok{ x}\SpecialCharTok{**}\DecValTok{2}\NormalTok{, }\AttributeTok{cubic =}\NormalTok{ x}\SpecialCharTok{**}\DecValTok{3}\NormalTok{, }\AttributeTok{quartic =}\NormalTok{ x}\SpecialCharTok{**}\DecValTok{4}\NormalTok{)}

\FunctionTok{vapply}\NormalTok{(y, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(z) }\FunctionTok{cor}\NormalTok{(x, z, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{), }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\FunctionTok{vapply}\NormalTok{(y, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(z) }\FunctionTok{cor}\NormalTok{(x, z, }\AttributeTok{method =} \StringTok{"spearman"}\NormalTok{), }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\FunctionTok{vapply}\NormalTok{(y, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(z) }\FunctionTok{cor}\NormalTok{(x, z, }\AttributeTok{method =} \StringTok{"kendall"}\NormalTok{), }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    linear quadratic     cubic   quartic 
##    1.0000    0.9746    0.9284    0.8817 
##    linear quadratic     cubic   quartic 
##         1         1         1         1 
##    linear quadratic     cubic   quartic 
##         1         1         1         1
\end{verbatim}

Similar results with a different set of functions

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# other functional forms {-}{-}{-}{-}}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\FunctionTok{sin}\NormalTok{(x), }\FunctionTok{sqrt}\NormalTok{(x), }\FunctionTok{exp}\NormalTok{(x), }\FunctionTok{log}\NormalTok{(x))}

\FunctionTok{vapply}\NormalTok{(y, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(z) }\FunctionTok{cor}\NormalTok{(x, z, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{), }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\FunctionTok{vapply}\NormalTok{(y, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(z) }\FunctionTok{cor}\NormalTok{(x, z, }\AttributeTok{method =} \StringTok{"spearman"}\NormalTok{), }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\FunctionTok{vapply}\NormalTok{(y, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(z) }\FunctionTok{cor}\NormalTok{(x, z, }\AttributeTok{method =} \StringTok{"kendall"}\NormalTok{), }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.1705  0.9892  0.7169  0.9517
## [1] -0.1394  1.0000  1.0000  1.0000
## [1] -0.1111  1.0000  1.0000  1.0000
\end{verbatim}

\hypertarget{sensitivity-to-domain}{%
\subsection{Sensitivity to domain}\label{sensitivity-to-domain}}

The ``strength of relationship'' (completely deterministic) is the same in both cases\\
However, the summarization of the relationships changes

Here's same case as before:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# polynomials {-}{-}{-}{-}}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{linear =}\NormalTok{ x, }\AttributeTok{quadratic =}\NormalTok{ x}\SpecialCharTok{**}\DecValTok{2}\NormalTok{, }\AttributeTok{cubic =}\NormalTok{ x}\SpecialCharTok{**}\DecValTok{3}\NormalTok{, }\AttributeTok{quartic =}\NormalTok{ x}\SpecialCharTok{**}\DecValTok{4}\NormalTok{)}

\FunctionTok{vapply}\NormalTok{(y, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(z) }\FunctionTok{cor}\NormalTok{(x, z, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{), }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\FunctionTok{vapply}\NormalTok{(y, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(z) }\FunctionTok{cor}\NormalTok{(x, z, }\AttributeTok{method =} \StringTok{"spearman"}\NormalTok{), }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\FunctionTok{vapply}\NormalTok{(y, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(z) }\FunctionTok{cor}\NormalTok{(x, z, }\AttributeTok{method =} \StringTok{"kendall"}\NormalTok{), }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    linear quadratic     cubic   quartic 
##    1.0000    0.9746    0.9284    0.8817 
##    linear quadratic     cubic   quartic 
##         1         1         1         1 
##    linear quadratic     cubic   quartic 
##         1         1         1         1
\end{verbatim}

And here's a different range:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# polynomials, diff range {-}{-}{-}{-}}
\NormalTok{x }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{10}\SpecialCharTok{:}\DecValTok{10}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{linear =}\NormalTok{ x, }\AttributeTok{quadratic =}\NormalTok{ x}\SpecialCharTok{**}\DecValTok{2}\NormalTok{, }\AttributeTok{cubic =}\NormalTok{ x}\SpecialCharTok{**}\DecValTok{3}\NormalTok{, }\AttributeTok{quartic =}\NormalTok{ x}\SpecialCharTok{**}\DecValTok{4}\NormalTok{)}

\FunctionTok{vapply}\NormalTok{(y, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(z) }\FunctionTok{cor}\NormalTok{(x, z, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{), }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\FunctionTok{vapply}\NormalTok{(y, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(z) }\FunctionTok{cor}\NormalTok{(x, z, }\AttributeTok{method =} \StringTok{"spearman"}\NormalTok{), }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\FunctionTok{vapply}\NormalTok{(y, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(z) }\FunctionTok{cor}\NormalTok{(x, z, }\AttributeTok{method =} \StringTok{"kendall"}\NormalTok{), }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    linear quadratic     cubic   quartic 
##    1.0000    0.0000    0.9179    0.0000 
##    linear quadratic     cubic   quartic 
##         1         0         1         0 
##    linear quadratic     cubic   quartic 
##         1         0         1         0
\end{verbatim}

\hypertarget{partial-correlation}{%
\subsection{Partial correlation}\label{partial-correlation}}

A lot of EDA starts with some sort of correlation matrix\\
This won't always account for the fact that some variables can mask correlation between others

Consider two groups with trends in different directions

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_obsvs }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{n\_group }\OtherTok{\textless{}{-}}  \DecValTok{2}

\NormalTok{group }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_group, }\AttributeTok{each =}\NormalTok{ n\_obsvs)}
\NormalTok{var1  }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_obsvs, }\AttributeTok{times =}\NormalTok{ n\_group)}
\NormalTok{var2  }\OtherTok{\textless{}{-}}\NormalTok{ var1 }\SpecialCharTok{*} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{), }\AttributeTok{each =}\NormalTok{ n\_obsvs)}
\NormalTok{var3  }\OtherTok{\textless{}{-}}\NormalTok{ var1 }\SpecialCharTok{*} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,  }\DecValTok{5}\NormalTok{), }\AttributeTok{each =}\NormalTok{ n\_obsvs)}
\end{Highlighting}
\end{Shaded}

As Figure \ref{fig:fig-cor-0} shows

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/fig-cor-0-1.pdf}
\caption{\label{fig:fig-cor-0}Subgroups demonstrating opposing linear relationships}
\end{figure}

Because of the opposing trends, their correlation becomes zero

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(var1, var2, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{)}
\FunctionTok{cor}\NormalTok{(var1, var2, }\AttributeTok{method =} \StringTok{"spearman"}\NormalTok{)}
\FunctionTok{cor}\NormalTok{(var1, var2, }\AttributeTok{method =} \StringTok{"kendall"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
## [1] 0
## [1] 0
\end{verbatim}

However, by group the correlation is 1

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(var1[group }\SpecialCharTok{==} \DecValTok{1}\NormalTok{], var2[group }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}
\FunctionTok{cor}\NormalTok{(var1[group }\SpecialCharTok{==} \DecValTok{2}\NormalTok{], var2[group }\SpecialCharTok{==} \DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
## [1] -1
\end{verbatim}

A similar thing happens when the relationship has the same sign but different slopes

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(var1, var3, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5704
\end{verbatim}

while the correlation is still one within group

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(var1[group }\SpecialCharTok{==} \DecValTok{1}\NormalTok{], var3[group }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}
\FunctionTok{cor}\NormalTok{(var1[group }\SpecialCharTok{==} \DecValTok{2}\NormalTok{], var3[group }\SpecialCharTok{==} \DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
## [1] 1
\end{verbatim}

Even partial correlation doesn't help in case of opposing signs

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ppcor)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'ppcor' was built under R version
## 4.0.5
\end{verbatim}

\begin{verbatim}
## Loading required package: MASS
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'MASS'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:patchwork':
## 
##     area
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:dplyr':
## 
##     select
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pcor}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(var1, var2, group))}\SpecialCharTok{$}\NormalTok{estimate}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       var1    var2   group
## var1     1  0.0000  0.0000
## var2     0  1.0000 -0.8864
## group    0 -0.8864  1.0000
\end{verbatim}

It \emph{improves} the strength of the estimated correlation in the case of \texttt{var3} but still failes to estimate it correctly.
Partial correlation would be assuming a form like \texttt{var3\ \textasciitilde{}\ var1\ +\ group} and not \texttt{var3\ \textasciitilde{}\ var1\ *\ group}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pcor}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(var1, var3, group))}\SpecialCharTok{$}\NormalTok{estimate}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          var1   var3   group
## var1   1.0000 0.8321 -0.7375
## var3   0.8321 1.0000  0.8864
## group -0.7375 0.8864  1.0000
\end{verbatim}

\hypertarget{trends}{%
\section{Trends}\label{trends}}

\hypertarget{if-trends-continue}{%
\subsection{\texorpdfstring{``If trends continue\ldots{}''}{``If trends continue\ldots''}}\label{if-trends-continue}}

Figure \ref{fig:fig-if-trends-continue} shows that\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{300}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\FunctionTok{ggplot}\NormalTok{(}
  \FunctionTok{data.frame}\NormalTok{(x, y),}
  \FunctionTok{aes}\NormalTok{(x, y)}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(x}\SpecialCharTok{**}\DecValTok{1}\NormalTok{), }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{fullrange =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(x}\SpecialCharTok{**}\DecValTok{2}\NormalTok{), }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{fullrange =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(x}\SpecialCharTok{**}\DecValTok{3}\NormalTok{), }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{fullrange =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{limit =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/fig-if-trends-continue-1.pdf}
\caption{\label{fig:fig-if-trends-continue}Extrapolated linear, quadratic, and cubic fits of data}
\end{figure}

\hypertarget{seasonality}{%
\subsection{Seasonality}\label{seasonality}}

\hypertarget{comparisons-todo}{%
\section{Comparisons (TODO)}\label{comparisons-todo}}

\hypertarget{percents-versus-percentage-points}{%
\subsection{Percents versus percentage points}\label{percents-versus-percentage-points}}

\hypertarget{changes-with-small-bases}{%
\subsection{Changes with small bases}\label{changes-with-small-bases}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{0.015} \SpecialCharTok{{-}} \FloatTok{0.010}\NormalTok{) }\SpecialCharTok{/} \FloatTok{0.010}
\NormalTok{(}\FloatTok{0.805} \SpecialCharTok{{-}} \FloatTok{0.800}\NormalTok{) }\SpecialCharTok{/} \FloatTok{0.800}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5
## [1] 0.00625
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.015} \SpecialCharTok{/} \FloatTok{0.010}
\FloatTok{0.805} \SpecialCharTok{/} \FloatTok{0.800}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.5
## [1] 1.006
\end{verbatim}

\hypertarget{strategies-todo-1}{%
\section{Strategies (TODO)}\label{strategies-todo-1}}

\hypertarget{real-world-disasters-todo}{%
\section{Real World Disasters (TODO)}\label{real-world-disasters-todo}}

Straight vs weighted averages in COVID positivity rates \citep{indy_rates}

\begin{quote}
The changes could result in real-world differences for Hoosiers, because the state uses a county's positivity rate as one of the numbers to determine which restrictions that county will face. Those restrictions determine how many people may gather, among other items.
\end{quote}

\begin{quote}
Some Hoosiers may see loosened restrictions because of the changes. While Box said the county-level impact will be mixed, she predicted some smaller counties will see a decline in positivity rate after the changes.
\end{quote}

\begin{quote}
``The change to the methodology is how we calculate the seven-day positivity rate for counties. In the past, similar to many states, we've added each day's positivity rate for seven days and divided by seven to obtain the week's positivity rate. Now we will add all of the positive tests for the week and divide by the total tests done that week to determine the week's positivity rate. This will help to minimize the effect that a high variability in the number of tests done each day can have on the week's overall positivity, especially for our smaller counties.''
\end{quote}

three issues here

first straight versus weighted averages

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{avg\_of\_ratios }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{10}\SpecialCharTok{/}\DecValTok{100} \SpecialCharTok{+} \DecValTok{90}\SpecialCharTok{/}\DecValTok{100}\NormalTok{) }\SpecialCharTok{/} \DecValTok{2}

\NormalTok{ratio\_of\_sums }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{10} \SpecialCharTok{+} \DecValTok{90}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{100} \SpecialCharTok{+} \DecValTok{100}\NormalTok{)}

\NormalTok{avg\_of\_ratios }\SpecialCharTok{==}\NormalTok{ ratio\_of\_sums}

\NormalTok{avg\_of\_ratios\_uneq }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{10}\SpecialCharTok{/}\DecValTok{100} \SpecialCharTok{+} \DecValTok{180} \SpecialCharTok{/} \DecValTok{200}\NormalTok{) }\SpecialCharTok{/} \DecValTok{2}

\NormalTok{ratio\_of\_sums\_uneq }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{10} \SpecialCharTok{+} \DecValTok{180}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{100} \SpecialCharTok{+} \DecValTok{200}\NormalTok{)}

\NormalTok{avg\_of\_ratios\_uneq }\SpecialCharTok{==}\NormalTok{ ratio\_of\_sums\_uneq}

\NormalTok{weightavg\_of\_ratios\_uneq }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{100}\SpecialCharTok{/}\DecValTok{300}\NormalTok{)}\SpecialCharTok{*}\NormalTok{(}\DecValTok{10}\SpecialCharTok{/}\DecValTok{100}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{200}\SpecialCharTok{/}\DecValTok{300}\NormalTok{)}\SpecialCharTok{*}\NormalTok{(}\DecValTok{180}\SpecialCharTok{/}\DecValTok{200}\NormalTok{)}

\NormalTok{ratio\_of\_sums\_uneq }\SpecialCharTok{==}\NormalTok{ weightavg\_of\_ratios\_uneq}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
## [1] FALSE
## [1] TRUE
\end{verbatim}

then back to the data for why it matters.

if data is from same distribution, this could increase variance but shouldn't effect mean

Recall that the standard deviation of sample proportion is \(\sqrt(p*(1-p)/n)\)

link to discussions of sample size and different types of averages

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# define simulation parameters {-}{-}{-}{-}}
\DocumentationTok{\#\# n: total draws from binomial distribution}
\DocumentationTok{\#\# p: proportion of successes}
\NormalTok{p }\OtherTok{\textless{}{-}} \FloatTok{0.5}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000}

\CommentTok{\# sample from binomials of different size {-}{-}{-}{-}}
\NormalTok{s010 }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n,  }\DecValTok{10}\NormalTok{, p) }\SpecialCharTok{/}  \DecValTok{10}
\NormalTok{s100 }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{100}\NormalTok{, p) }\SpecialCharTok{/} \DecValTok{100}
\NormalTok{s500 }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{500}\NormalTok{, p) }\SpecialCharTok{/} \DecValTok{500}

\CommentTok{\# set results as dataframe for inspection {-}{-}{-}{-}}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{s =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{500}\NormalTok{), }\AttributeTok{each =}\NormalTok{ n),}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(s010, s100, s500)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ df) }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{col =} \FunctionTok{as.character}\NormalTok{(s)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ p, }\AttributeTok{col =} \StringTok{\textquotesingle{}darkgrey\textquotesingle{}}\NormalTok{, }\AttributeTok{linetype =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Sampling Distribution for p = 0.5"}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"Sample Size"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
    \AttributeTok{legend.position =} \StringTok{"bottom"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-199-1.pdf}

but low sample days based on real world are probably also a sign of a different distribution (only very urgent cases get tested?)

\hypertarget{vex-viz}{%
\chapter{Vexing Visualization (TODO)}\label{vex-viz}}

\hypertarget{incr-infe}{%
\chapter{Incredible Inferences (TODO)}\label{incr-infe}}

Previously, we have seen how different inputs like data, tools, and methods can add risks to our data analysis. However, the battle is not won simply when we get our first set of \emph{outputs}. In this chapter, we will explore common errors in interpreting the results of our analysis by exploring aspects of bias, missingness, and confounding.

\hypertarget{common-biases}{%
\section{Common Biases}\label{common-biases}}

\hypertarget{policy-induced-relationships}{%
\section{Policy-induced relationships}\label{policy-induced-relationships}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n)}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{\textgreater{}} \DecValTok{1}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(x1, x2, y)}

\FunctionTok{with}\NormalTok{(df, }\FunctionTok{cor}\NormalTok{(x1, x2))}
\FunctionTok{with}\NormalTok{(df[df}\SpecialCharTok{$}\NormalTok{y,], }\FunctionTok{cor}\NormalTok{(x1, x2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.05928
## [1] -0.5003
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{ggplot}\NormalTok{(df) }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x1, }\AttributeTok{y =}\NormalTok{ x2, }\AttributeTok{col =}\NormalTok{ y) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-205-1.pdf}

\hypertarget{feature-leakage}{%
\section{Feature leakage}\label{feature-leakage}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{minutes\_month1 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n, }\DecValTok{60}\NormalTok{, }\DecValTok{1200}\NormalTok{)}
\NormalTok{minutes\_month2 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n, }\DecValTok{60}\NormalTok{, }\DecValTok{1200}\NormalTok{) }
\NormalTok{minutes\_tot }\OtherTok{\textless{}{-}}\NormalTok{ minutes\_month1 }\SpecialCharTok{+}\NormalTok{ minutes\_month2}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(minutes\_month1, minutes\_month2, minutes\_tot)}
\end{Highlighting}
\end{Shaded}

Figure \ref{fig:sep-sum} shows\ldots{}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{bookdown_files/figure-latex/sep-sum-1} 

}

\caption{Correlation of independent versus cumulative quantities}\label{fig:sep-sum}
\end{figure}

\hypertarget{diligent-data-dredging}{%
\section{``Diligent'' data dredging}\label{diligent-data-dredging}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{random\_test }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  
\NormalTok{  indices }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(x), }\FunctionTok{length}\NormalTok{(x)}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{  group1 }\OtherTok{\textless{}{-}}\NormalTok{ x[indices]}
\NormalTok{  group2 }\OtherTok{\textless{}{-}}\NormalTok{ x[}\SpecialCharTok{{-}}\NormalTok{indices]}
\NormalTok{  tt }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(group1, group2)}
  \FunctionTok{return}\NormalTok{(tt}\SpecialCharTok{$}\NormalTok{p.value)}
  
\NormalTok{\}}

\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{vapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10000}\NormalTok{, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(...) \{}\FunctionTok{random\_test}\NormalTok{(x)\}, }\AttributeTok{FUN.VALUE =} \FunctionTok{numeric}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\FunctionTok{sum}\NormalTok{(p }\SpecialCharTok{\textless{}} \FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 500
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_obsv }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n\_vars }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{mat\_cat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}
  \AttributeTok{data =} \FunctionTok{rbinom}\NormalTok{(n\_obsv }\SpecialCharTok{*}\NormalTok{ n\_vars, }\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
  \AttributeTok{nrow =}\NormalTok{ n\_obsv,}
  \AttributeTok{ncol =}\NormalTok{ n\_vars}
\NormalTok{  )}
\NormalTok{mat\_all }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(x, mat\_cat)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(mat\_all)}
\FunctionTok{names}\NormalTok{(df) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\FunctionTok{paste0}\NormalTok{(}\StringTok{"v"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_vars))}
\FunctionTok{head}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          x v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 v11 v12 v13
## 1 -0.56048  1  1  0  1  1  0  1  0  0   1   1   0   1
## 2 -0.23018  1  1  0  0  0  0  0  1  1   1   1   0   0
## 3  1.55871  1  0  0  0  1  0  1  1  0   1   0   0   0
## 4  0.07051  1  0  1  1  0  0  1  0  1   1   1   0   0
## 5  0.12929  1  1  1  0  0  0  0  0  0   1   1   1   1
## 6  1.71506  1  0  0  1  1  0  1  1  1   1   1   0   1
##   v14 v15 v16 v17 v18 v19 v20 v21 v22 v23 v24 v25 v26
## 1   1   0   1   0   0   0   1   0   0   1   1   1   0
## 2   1   1   1   1   1   0   0   1   0   1   0   1   0
## 3   1   1   1   1   1   0   0   1   0   1   1   0   1
## 4   0   0   0   1   0   0   0   1   1   0   1   1   0
## 5   0   1   1   0   0   0   1   1   0   0   0   1   0
## 6   1   0   0   0   0   1   1   0   0   0   1   1   0
##   v27 v28 v29 v30 v31 v32 v33 v34 v35 v36 v37 v38 v39
## 1   0   0   0   1   0   0   1   0   0   0   1   1   1
## 2   1   0   0   1   1   0   1   0   0   0   1   0   0
## 3   0   0   0   1   0   1   0   0   1   1   1   1   0
## 4   0   0   0   1   1   1   1   0   0   1   0   0   0
## 5   1   0   0   1   0   0   0   0   1   0   1   0   1
## 6   0   1   1   0   1   1   0   0   0   1   1   0   1
##   v40 v41 v42 v43 v44 v45 v46 v47 v48 v49 v50 v51 v52
## 1   0   0   0   1   1   1   0   1   0   0   1   1   0
## 2   0   0   0   0   0   0   0   1   0   1   0   1   1
## 3   0   0   1   0   1   1   0   0   1   0   1   1   1
## 4   0   1   0   1   0   0   0   1   0   0   0   0   1
## 5   0   0   1   0   1   0   1   1   1   1   0   1   0
## 6   1   1   1   1   0   0   0   1   0   0   1   0   0
##   v53 v54 v55 v56 v57 v58 v59 v60 v61 v62 v63 v64 v65
## 1   0   0   0   1   0   1   1   0   0   1   0   0   0
## 2   0   0   1   0   1   0   1   1   0   1   0   1   1
## 3   1   1   1   0   0   1   0   1   0   1   1   1   1
## 4   1   1   0   0   0   0   1   0   1   1   1   1   1
## 5   0   0   1   1   1   0   0   0   0   0   1   0   1
## 6   1   1   0   0   1   0   1   1   0   1   1   0   1
##   v66 v67 v68 v69 v70 v71 v72 v73 v74 v75 v76 v77 v78
## 1   1   1   1   0   0   0   0   1   0   0   1   0   0
## 2   1   1   0   1   1   0   1   1   1   1   1   1   1
## 3   1   0   1   0   1   1   0   1   1   0   0   0   0
## 4   0   1   0   0   0   1   0   1   0   0   0   0   0
## 5   0   0   0   1   0   1   1   1   0   1   0   0   0
## 6   0   1   0   1   0   0   0   1   1   0   1   0   0
##   v79 v80 v81 v82 v83 v84 v85 v86 v87 v88 v89 v90 v91
## 1   1   0   1   0   0   0   0   1   1   1   0   0   1
## 2   1   1   0   1   1   1   0   0   1   1   0   0   0
## 3   1   0   0   1   0   0   0   0   1   0   1   0   0
## 4   0   0   0   0   1   0   1   1   0   0   0   0   0
## 5   1   1   1   0   0   1   1   0   0   1   1   0   1
## 6   1   0   1   1   0   0   0   0   1   1   0   1   0
##   v92 v93 v94 v95 v96 v97 v98 v99 v100
## 1   1   1   0   0   0   1   1   0    0
## 2   0   1   0   1   1   0   1   0    1
## 3   1   0   0   0   1   0   1   0    1
## 4   0   0   1   1   0   0   1   1    1
## 5   0   1   1   1   1   1   0   1    0
## 6   0   0   0   0   1   1   0   0    1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v1, }\AttributeTok{data =}\NormalTok{ df)}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v2, }\AttributeTok{data =}\NormalTok{ df)}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v3, }\AttributeTok{data =}\NormalTok{ df)}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v4, }\AttributeTok{data =}\NormalTok{ df)}\SpecialCharTok{$}\NormalTok{p.value}
\CommentTok{\# etc.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.09771
## [1] 0.8734
## [1] 0.02182
## [1] 0.1525
\end{verbatim}

Success! ..Or success?

sample splitting with ``train''

(obviously a very ugly way to do this, but that's the point)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v1, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v2, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v3, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v4, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v5, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v6, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v7, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v8, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v9, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v10, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v11, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v12, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v13, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v14, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v15, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v16, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v17, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v18, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v19, }\AttributeTok{data =}\NormalTok{ df[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_obsv}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),])}\SpecialCharTok{$}\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6022
## [1] 0.4947
## [1] 0.196
## [1] 0.3682
## [1] 0.2115
## [1] 0.7113
## [1] 0.3127
## [1] 0.8142
## [1] 0.9033
## [1] 0.8212
## [1] 0.4416
## [1] 0.2564
## [1] 0.5292
## [1] 0.1715
## [1] 0.0855
## [1] 0.2285
## [1] 0.6277
## [1] 0.01318
## [1] 0.2556
\end{verbatim}

and ``test''

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ v18, }\AttributeTok{data =}\NormalTok{ df[(n\_obsv}\SpecialCharTok{/}\DecValTok{2} \SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{n\_obsv,])}\SpecialCharTok{$}\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1691
\end{verbatim}

\hypertarget{superficial-stories}{%
\section{Superficial stories}\label{superficial-stories}}

\hypertarget{regression-to-the-mean}{%
\subsection{Regression to the mean}\label{regression-to-the-mean}}

simulate truly independent spend amounts across two periods

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\NormalTok{n  }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{mu }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{sd }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{spend1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, mu, sd)}
\NormalTok{spend2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, mu, sd)}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(spend1, spend2)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(spend1 }\SpecialCharTok{\textgreater{}}\NormalTok{ mu) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"spend"}\NormalTok{)), mean) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pct\_change =} \FunctionTok{round}\NormalTok{((spend2 }\SpecialCharTok{{-}}\NormalTok{ spend1) }\SpecialCharTok{/}\NormalTok{ spend1, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 4
##   `spend1 > mu` spend1 spend2 pct_change
##   <lgl>          <dbl>  <dbl>      <dbl>
## 1 FALSE           92.2   99.7      0.081
## 2 TRUE           108.   101.      -0.063
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{spend1\_bin =} \FunctionTok{cut}\NormalTok{(spend1, }\DecValTok{5}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(spend1\_bin) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"spend"}\NormalTok{)), mean) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pct\_change =} \FunctionTok{round}\NormalTok{((spend2 }\SpecialCharTok{{-}}\NormalTok{ spend1) }\SpecialCharTok{/}\NormalTok{ spend1, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 4
##   spend1_bin spend1 spend2 pct_change
##   <fct>       <dbl>  <dbl>      <dbl>
## 1 (71.8,84]    80.5   97.8      0.215
## 2 (84,96.1]    91.1  100.       0.098
## 3 (96.1,108]  102.   101.      -0.012
## 4 (108,120]   113.   101.      -0.101
## 5 (120,132]   124.   103.      -0.167
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{spend1\_bin =} \FunctionTok{cut}\NormalTok{(spend1, }\DecValTok{5}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(spend1\_bin) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{corr =} \FunctionTok{cor}\NormalTok{(spend1, spend2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` ungrouping output (override with `.groups` argument)
\end{verbatim}

\begin{verbatim}
## # A tibble: 5 x 2
##   spend1_bin    corr
##   <fct>        <dbl>
## 1 (71.8,84]   0.281 
## 2 (84,96.1]  -0.0149
## 3 (96.1,108]  0.0438
## 4 (108,120]   0.101 
## 5 (120,132]  -0.165
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(spend1 }\SpecialCharTok{\textgreater{}}\NormalTok{ spend2)}
\FunctionTok{mean}\NormalTok{(spend1 }\SpecialCharTok{\textless{}}\NormalTok{ spend2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.49
## [1] 0.51
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{((spend1 }\SpecialCharTok{\textgreater{}}\NormalTok{ mu) }\SpecialCharTok{*}\NormalTok{ (spend1 }\SpecialCharTok{\textgreater{}}\NormalTok{ spend2)) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(spend1 }\SpecialCharTok{\textgreater{}}\NormalTok{ mu)}
\FunctionTok{sum}\NormalTok{((spend1 }\SpecialCharTok{\textless{}}\NormalTok{ mu) }\SpecialCharTok{*}\NormalTok{ (spend1 }\SpecialCharTok{\textless{}}\NormalTok{ spend2)) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(spend1 }\SpecialCharTok{\textgreater{}}\NormalTok{ mu)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7168
## [1] 0.7267
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{ggplot}\NormalTok{(df) }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ spend1, }\AttributeTok{y =}\NormalTok{ spend2) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/unnamed-chunk-219-1.pdf}

\hypertarget{distribution-shifts}{%
\subsection{Distribution shifts}\label{distribution-shifts}}

Figure \ref{fig:fig-shift} shows that\ldots{}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/fig-shift-1.pdf}
\caption{\label{fig:fig-shift}Trends within and between customer behavioral groups}
\end{figure}

Figure \ref{fig:fig-shift-v2} shows that\ldots{}

\begin{verbatim}
## Warning: Removed 1 row(s) containing missing values
## (geom_path).
\end{verbatim}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/fig-shift-v2-1.pdf}
\caption{\label{fig:fig-shift-v2}Possible subgroup trends contributing to aggregate trend}
\end{figure}

The code used to generate this mock dataset is shown below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hi\_engagement }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{lo\_engagement }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{pr\_engagement }\OtherTok{\textless{}{-}} \FloatTok{0.85}\SpecialCharTok{\^{}}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{24}\NormalTok{)}
\NormalTok{avg\_engagement }\OtherTok{\textless{}{-}} \DecValTok{10}\SpecialCharTok{*}\NormalTok{pr\_engagement }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{pr\_engagement)}

\NormalTok{df }\OtherTok{\textless{}{-}} 
  \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{t =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(avg\_engagement), }
\NormalTok{    avg\_engagement, }
\NormalTok{    hi\_engagement, }
\NormalTok{    lo\_engagement}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tricky-timing-issues-wip}{%
\section{Tricky timing issues (WIP)}\label{tricky-timing-issues-wip}}

\hypertarget{censored-data}{%
\subsection{Censored data}\label{censored-data}}

Suppose we are wondering how long our subscription customers will stay put\\
mean lifetime of customers in 24 and uses exponential distrib (see appendix on distribs)\\
we are analyzing a cohort of customers 18 months after they first subscribed

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# time{-}to{-}event censored {-}{-}{-}{-}}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{curr\_time }\OtherTok{\textless{}{-}} \DecValTok{18}
\NormalTok{mean\_lifetime }\OtherTok{\textless{}{-}} \DecValTok{24}

\NormalTok{lifetime }\OtherTok{\textless{}{-}} \FunctionTok{rexp}\NormalTok{(n, }\AttributeTok{rate =} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ mean\_lifetime)}
\FunctionTok{mean}\NormalTok{(lifetime)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 24.72
\end{verbatim}

Because we are only 18 months in, we cannot observe the lifetimes of all customers\\
for those that left before 18 months we have complete data\\
but for those who left after 18 months we only know their lifetime exceeds 18 months.\\
Thus, if we look at the mean only where we can observe it, it's biased towards lower lifetimes.
(Recall that we know what the correct value is)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\textgreater{} observed {-}{-}{-}{-}}
\NormalTok{lifetime\_observed }\OtherTok{\textless{}{-}}\NormalTok{ lifetime}
\NormalTok{lifetime\_observed[lifetime }\SpecialCharTok{\textgreater{}}\NormalTok{ curr\_time] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\FunctionTok{mean}\NormalTok{(lifetime\_observed, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7.919
\end{verbatim}

Of course, we do know more than nothing (null) about the ``surviving customers''.
We know that their lifetime is \emph{at least} as large as the current time.
So alternatively, we could use the current time in our calculations.
This makes for a slightly less biased estimate, but it is still wrong and guaranteed to underestimate the actual average.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\textgreater{} max {-}{-}{-}{-}}
\NormalTok{lifetime\_max }\OtherTok{\textless{}{-}} \FunctionTok{pmin}\NormalTok{(lifetime, curr\_time)}
\FunctionTok{mean}\NormalTok{(lifetime\_max)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12.88
\end{verbatim}

This scenario illustrates the concept of \textbf{censored data}. Figure \ref{fig:fig-censor} illustrates the fundamental problem more clearly.

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/fig-censor-1.pdf}
\caption{\label{fig:fig-censor}A sample of observations of customer lifetimes showing observed and censored data}
\end{figure}

So what can we do instead?
A common approach is to examine \emph{quantiles} (such as the median) which can make more full use of the data we have.
Since we know that rank of our observations (that is, that the censored observations are all larger than the observed datapoints),
we can reliable calculate the p-th quantile so long as p percent of the data is not censored.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\textgreater{} quantile {-}{-}{-}{-}}
\FunctionTok{sum}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(lifetime\_observed)) }\SpecialCharTok{/}\NormalTok{ n}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.508
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lifetime\_quantile }\OtherTok{\textless{}{-}}\NormalTok{ lifetime\_observed}
\NormalTok{lifetime\_observed[}\FunctionTok{is.na}\NormalTok{(lifetime\_observed)] }\OtherTok{\textless{}{-}} \DecValTok{100}\SpecialCharTok{*}\NormalTok{curr\_time}
\FunctionTok{quantile}\NormalTok{(lifetime\_observed, }\AttributeTok{p =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   50% 
## 17.55
\end{verbatim}

\hypertarget{immortal-time-bias}{%
\subsection{Immortal time bias}\label{immortal-time-bias}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rollout\_time }\OtherTok{\textless{}{-}} \DecValTok{12}
\NormalTok{used\_feature }\OtherTok{\textless{}{-}}\NormalTok{ (lifetime }\SpecialCharTok{\textgreater{}}\NormalTok{ rollout\_time) }\SpecialCharTok{*} \FunctionTok{rbinom}\NormalTok{(n, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.5}\NormalTok{)}
\FunctionTok{aggregate}\NormalTok{(lifetime, }\AttributeTok{by =} \FunctionTok{list}\NormalTok{(used\_feature), }\AttributeTok{FUN =}\NormalTok{ mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Group.1     x
## 1       0 19.51
## 2       1 36.81
\end{verbatim}

\hypertarget{section}{%
\section{}\label{section}}

\hypertarget{cava-caus}{%
\chapter{Cavalier Causality (TODO)}\label{cava-caus}}

In Chapter \ref{incr-infe} (Incredible Inferences), we began to see that we can be tricked by biases when we lack \emph{causal} thinking and an underlying theory for the data generating process. In this chapter, we will revisit some of these same disasters and introduce some specific frameworks to help us more rigorously explore our analysis for errors and biases and, even better, strategize the best ways to fix them.

\hypertarget{mind-mod}{%
\chapter{Mindless Modeling (TODO)}\label{mind-mod}}

\hypertarget{features}{%
\section{Features}\label{features}}

\hypertarget{targets}{%
\section{Targets}\label{targets}}

\hypertarget{evaluation-metrics}{%
\section{Evaluation Metrics}\label{evaluation-metrics}}

\hypertarget{unsupervised-learning}{%
\section{Unsupervised Learning}\label{unsupervised-learning}}

\hypertarget{lifecycle-management}{%
\section{Lifecycle Management}\label{lifecycle-management}}

\hypertarget{fair-and-ethical-modeling}{%
\section{Fair and Ethical Modeling}\label{fair-and-ethical-modeling}}

\hypertarget{alt-alg}{%
\chapter{Alternative Algorithms (TODO)}\label{alt-alg}}

As the consummate showman, P.T. Barnum is often quoted as saying ``Leave them wanting more''. Unfortunately, statistics professors have less of a flare for drama. Introductory statistics courses will typically introduce a few types of models (for example, linear and \emph{perhaps} logistic regression), and that's a wrap. It's often until students start \emph{taking} the subsequent courses that they are exposed to the true limitations of previous techniques and taught to demand more.

This chapter attempts to flip that paradigm by briefly surveying a broad number of modeling techniques. The goal is not to go into all of the rigorous deals that one should understand to use these models. Instead, we hope to build a ``mental toolbox'' of techniques so that you know where to focus your study when you encounter a problem in the real world.

\hypertarget{not-modeling}{%
\section{Not Modeling}\label{not-modeling}}

\hypertarget{first-principles}{%
\subsection{First Principles}\label{first-principles}}

\hypertarget{simple-analyses}{%
\subsection{Simple Analyses}\label{simple-analyses}}

\hypertarget{extending-linear-regression}{%
\section{Extending Linear Regression}\label{extending-linear-regression}}

\hypertarget{modeling-binary-outcomes}{%
\subsection{Modeling Binary Outcomes}\label{modeling-binary-outcomes}}

\hypertarget{modeling-counts}{%
\subsection{Modeling Counts}\label{modeling-counts}}

\hypertarget{modeling-time-until-an-event}{%
\subsection{Modeling Time Until an Event}\label{modeling-time-until-an-event}}

\hypertarget{modeling-repeated-measures-on-a-population}{%
\subsection{Modeling Repeated Measures on a Population}\label{modeling-repeated-measures-on-a-population}}

\hypertarget{modeling-observations-in-a-nested-hierarchy}{%
\subsection{Modeling Observations in a Nested Hierarchy}\label{modeling-observations-in-a-nested-hierarchy}}

\hypertarget{causal-analysis-patterns}{%
\section{Causal Analysis Patterns}\label{causal-analysis-patterns}}

Similar to \url{https://emilyriederer.netlify.app/post/causal-design-patterns/}

\hypertarget{special-data-types}{%
\section{Special Data Types}\label{special-data-types}}

\hypertarget{duration-analysis}{%
\subsection{Duration Analysis}\label{duration-analysis}}

\hypertarget{time-space-data}{%
\subsection{Time \& Space Data}\label{time-space-data}}

\hypertarget{bayesian-methods}{%
\section{Bayesian Methods}\label{bayesian-methods}}

\hypertarget{simulation-methods}{%
\section{Simulation Methods}\label{simulation-methods}}

\hypertarget{agent-based}{%
\subsection{Agent-Based}\label{agent-based}}

\hypertarget{discrete-event}{%
\subsection{Discrete Event}\label{discrete-event}}

\hypertarget{clustering-beyond-k-means}{%
\section{Clustering (beyond K-Means)}\label{clustering-beyond-k-means}}

\hypertarget{density-based}{%
\subsection{Density-Based}\label{density-based}}

\hypertarget{mixture-models}{%
\subsection{Mixture Models}\label{mixture-models}}

\hypertarget{part-workflow}{%
\part*{Workflow}\label{part-workflow}}


\hypertarget{futi-find}{%
\chapter{Futile Findings (TODO)}\label{futi-find}}

\hypertarget{comp-code}{%
\chapter{Complexifying Code (TODO)}\label{comp-code}}

\hypertarget{making-code-unreadable}{%
\section{Making code unreadable}\label{making-code-unreadable}}

\hypertarget{naming}{%
\subsection{Naming}\label{naming}}

structure for sorting, ordering, clear semantics

\hypertarget{whitespace}{%
\subsection{Whitespace}\label{whitespace}}

linters / stylers

\hypertarget{making-a-monolith}{%
\section{Making a monolith}\label{making-a-monolith}}

not using functions / files / templates / variables

\hypertarget{making-code-inflexible-variables}{%
\subsection{Making code inflexible (variables)}\label{making-code-inflexible-variables}}

\hypertarget{making-useful-code-chunks-hard-to-reuse-functions}{%
\subsection{Making useful code chunks hard to reuse (functions)}\label{making-useful-code-chunks-hard-to-reuse-functions}}

\hypertarget{project-organization}{%
\section{Project organization}\label{project-organization}}

\hypertarget{decoding}{%
\section{Decoding}\label{decoding}}

rubber duck decoding

\hypertarget{reje-repr}{%
\chapter{Rejecting Reproducibility (TODO)}\label{reje-repr}}

``Good Enough Practices in Computational Reproducibility'' \citep{wilson_bryan_cranston_kitzes_nederbragt_teal_2017}

Package managment \citep{R-renv}

environment management

\cleardoublepage

\hypertarget{appendix-appendix}{%
\appendix \addcontentsline{toc}{chapter}{\appendixname}}


\hypertarget{useful-data-generation-functions-todo}{%
\chapter{Useful Data Generation Functions (TODO)}\label{useful-data-generation-functions-todo}}

\hypertarget{common-probability-distributions-todo}{%
\chapter{Common Probability Distributions (TODO)}\label{common-probability-distributions-todo}}

  \bibliography{book.bib,packages.bib}

\backmatter
\printindex

\end{document}
