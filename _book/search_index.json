[["index.html", "Data Disasters Preface Structure of the book Assumed knowledge Software information and conventions Acknowledgments", " Data Disasters Emily Riederer 2021-05-31 Preface The art and science of data analysis has a rigorous hidden curriculum. Introductory statistics and machine learning courses too often focus on the happy path of analysis. New methods are introduced by stating relevant assumptions (e.g. independent and identically distributed data) and observing successful examples of appropriate applications. However, data analysis in the real world is far more perilous. Required analytical assumptions are often hard to achieve with real-world observational data. Even worse, behind each method  from a simple mean to a complex model  there are countless more unstated assumptions; without any discussion, we assume that the data is clean, correct, and measuring quantities of interest, that our metrics and methods intend to answer the same questions we have practical interest in, and that our interpretation of their outputs can avoid falling into countless biases. This book, of course, does not pretend to offer solutions to this myraid of problems. To do so would be to solve all of statistics and to recycle the work of countless authors across many disciplines (from classical statistics to political science, epidemiology, economics, etc.) Instead, as the title of Data Disasters implies, this book aspires to expose these fundamental gaps and pitfalls in data analysis early in the readers career with the hope of helping the reader avoid having to relearn each of these lessons on an important, real-life analysis. Beyond simply recognizing these problems in practice, this text also aims to build the readers mental index to help them recognize problems, name them, and effectively seek out appropriate resources to solve them when they are encountered. Statistics as a field suffers from a crisis of continual reinvention as methods become intrinsically linked to one discipline (e.g. survival analysis in the biological sciences) and remain underappreciated outside. By briefly highlighting different methods, R packages, and further readings about pitfalls and potential solutions, this book functions as something like a literature review for data analysis and empowers readers to learn more when needed. Structure of the book This book is best thought of as anthology of stories of how data analysis can go wrong. Each chapter and, moreover, each section is nearly self-contained and should be able to be read and understood in isolation. Chapter 3 focus on errors in understanding common data structures and programming tools during initial retrieval and cleaning. Chapters 4 and 5 focus on errors in exploratory data analysis. Chapters 6 and 7 . Chapters 8 and 9 delve into the more specialized topic of statistical modeling, and finally Chapter 10 discusses the importance and implementation of computational reproducibility. Assumed knowledge This book was written with the goal of requiring minimal prerequisite knowledge. However, different chapters will be most useful and relevant to readers with certain prior knowledge. Without exception, this book relates to the process of analyzing structured, tabular data (e.g. rows as observations, columns as variables). However, this book does not precisely aspire to teach statistics or data analysis but rather to fill in the gap between how these subjects are typically taught and how problems present themselves in real research and industry. As such, past and concurrent experience working with any type of structured data with any computational tool (e.g. spreadsheet, SAS, Stata, SQL, R, etc.) will help readers relate to the material. Most of this books contents are illustrated with R code, and additional references are made to using R packages to solve basic problems. For this reason, some background in R may improve general comprehension of and interest in the material. However, programming is not a key focus of the book (apart from Chapter 3) and the amount of syntax used should be easily readable without much hands-on scripting experience. Furthermore, occasional SQL and python examples are also included where relevant to appeal to a broader audience and to celebrate the use of a wide range of analysis tools. Finally, since this book focuses mostly on basic concepts like wrangling, summary statistics, and visualization, little to no statistics background is required until chapters 7 and 8 where more advanced methods are discussed. For Chapter 8, in particular, a basic understanding of statistical modeling (e.g. regression and clustering) is assumed. The level of depth of the classic text ISLR (James et al. 2017) (TODO: add citation) would be far more than sufficient. Software information and conventions I used the knitr package (Xie 2015) and the bookdown package (Xie 2021) to compile my book. My R session information is shown below: xfun::session_info() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19042) ## ## Locale: ## LC_COLLATE=English_United States.1252 ## LC_CTYPE=English_United States.1252 ## LC_MONETARY=English_United States.1252 ## LC_NUMERIC=C ## LC_TIME=English_United States.1252 ## ## Package version: ## base64enc_0.1.3 bookdown_0.22 ## bslib_0.2.5.1 compiler_4.0.2 ## digest_0.6.27 evaluate_0.14 ## fs_1.5.0 glue_1.4.1 ## graphics_4.0.2 grDevices_4.0.2 ## highr_0.8 htmltools_0.5.1.1 ## jquerylib_0.1.4 jsonlite_1.7.0 ## knitr_1.30 magrittr_2.0.1 ## markdown_1.1 methods_4.0.2 ## mime_0.9 R6_2.4.1 ## rappdirs_0.3.1 rlang_0.4.10 ## rmarkdown_2.8 rstudioapi_0.13 ## sass_0.4.0 stats_4.0.2 ## stringi_1.4.6 stringr_1.4.0 ## tinytex_0.32 tools_4.0.2 ## utils_4.0.2 xfun_0.23 ## yaml_2.2.1 Package names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book()). Acknowledgments TODO: Thank reviewers, etc. References "],["about-the-author.html", "About the Author", " About the Author Emily Riederer is "],["introduction.html", "Chapter 1 Introduction 1.1 What is data? 1.2 What is analysis? 1.3 What is data analysis? 1.4 A case study", " Chapter 1 Introduction Statistics is not synonymous with data analysis; rigor vs practicality Evaluating the Success of a Data Analysis (Hicks and Peng 2019) Data Alone is not Ground Truth (Bassa 2017) 1.1 What is data? Data is 1.2 What is analysis? Analysis is the process of turning information into insight. 1.3 What is data analysis? Data analysis altogether is 1.4 A case study Now unplug your Internet cable, and start doing some serious work. We have a nice figure in Figure 1.1, and also a table in Table 1.1. par(mar = c(4, 4, 1, .1)) plot(cars, pch = 19) FIGURE 1.1: Hello World! knitr::kable( head(iris), caption = &#39;The boring iris data.&#39;, booktabs = TRUE ) TABLE 1.1: The boring iris data. Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa References "],["data-dal.html", "Chapter 2 Data Dalliances 2.1 The Four DGPs for Data Management 2.2 Data Collection 2.3 Data Loading 2.4 Data Transformation 2.5 Conclusion", " Chapter 2 Data Dalliances Statistics literature often makes reference to the data generating process (DGP): an idealized description of a real-world system responsible for producing observed data. This leads to a modeling approach focused on describing that system as opposed to blindly fitting observations to a common functional form.1 As a trivial example, if one wished to model the height of a group of adults, they might suppose that the height of women and the height of men each is normally distributed with separate means and standard deviations. Then the overall distribution of population heights could be models as a mixture of samples from these two distributions.2 However, DGPs are not only useful for modeling. Conceptualizing the DGP of our observations can also lead to more principled data validation if we broaden the scope of the DGP to include the subsequent manufacturing of the data not just the originating mechanism. Unfortunately, consumers of analytical data may not always be familiar with the craft of data production (including data engineering, data modeling, and data management). Without an understanding of the general flow of data processing between collection and publication to a data warehouse, data consumers are less able to theorize about failure modes. Instead, similarly to blindly fitting models without an underlying theory, consumers may default to cursory checks of summary statistics without hypotheses for the kind of errors they are trying to detect or how these checks might help them. This post explores the DGP of system-generated data and the common ways that these processes can introduce risks to data quality. As we discuss data validation, we will make reference to the six dimensions of data quality defined by DAMA: completeness, uniqueness, timeliness, validity, accuracy, and consistency. Along the way, we will explore how understanding how understanding key failure modes in the data production process can lead to more principled analytical data validation.3 2.1 The Four DGPs for Data Management To better theorize about data quality issues, its useful to think of four DGPs: the real-world DGP, the data collection/extraction DGP4, the data loading DGP, and the data transformation DGP. For example, consider the role of each of these four DGPs for e-commerce data: Real-world DGP: Supply, demand, marketing, and a range of factors motivate a consumer to visit a website and make a purchase Data collection DGP: Parts of the website are instrumented to log certain customer actions. This log is then extracted from the different operational system (login platforms, payment platforms, account records) to be used for analysis Data loading DGP: Data recorded by different systems is moved to a data warehouse for further processing through some sort of manual, scheduled, or orchestrated job. These different systems may make data available at different frequencies. Data transformation DGP: To arrive at that final data presentation requires creating a data model to describe domain-specific attributes with key variables crafted with data transformations Or, consider the role of each of these four DGPs for subway ridership data5: Real-world DGP: Riders are motivated to use public transportation to commute, run errands, or visit friends. Different motivating factors may cause different weekly and annual seasonality Data collection DGP: To ride the subway, riders go to a station and enter and exit through turnstiles. The mechanical rotation of the turnstile caused by a rider passing through is recorded Data loading DGP: Data recorded at each turnstile is collected through a centralized computer system at the station. Once a week, each station uploads a flat file of this data to a data lake owned by the citys Department of Transportation Data transformation DGP: Turnstiles from different companies may have different data formats. Transformation may include harmonizing disparate sources, coding system-generated codes (e.g. Station XYZ) to semantically meaningful names (e.g. Main Street Station), and publishing a final unified representation across stations and across time In the next sections, well explore how understanding key concepts about each of these DGPs can help build a consumers intuition on where to look for problems. 2.2 Data Collection Data collection is necessarily the first step in data production, but the very goal of data collection: translating complex human concepts into tabular data records is fraught with error. Data collection is effectively dimensionality reduction, and just like statistical dimensionality reduction, it must sometimes sacrifice accuracy for clarity. This tradeoff makes data collection vulnerable to one of the largest risks to data validity: not that the data itself is incorrect given its stated purpose but rather that users misconstrue the population or metrics it includes. Thus, understanding what systems are intending to capture, publish, and extract and how they chose to encode information for those observations is essential for data validation and subsequent analysis. Data collection can happen in countless different ways: experimentation, surveys, observation, sensors, etc. In many business settings, data is often extracted from source systems whose primary purpose is to execute some sort of real-world process.6 Such systems may naturally collect data for operational purposes or may be instrumented to collect and log data as they are used. This production data is then often extracted from a source system to an alternative location such as a data warehouse for analysis. 2.2.1 What Counts One of the tricky nuances of data collection is understanding what precisely is getting captured and logged in the first place. Consider something as simple as a login system where users must enter their credentials, endure a Captcha-like verification process to prove that they are not a robot, and enter a multi-factor authentication code. Which of these events gets collected and recorded has a significant impact on subsequent data processing. In a technical sense, no inclusion/exclusion decision here is incorrect, persay, but if the producers choices dont match the consumers understandings, it can lead to misleading results. For example, an analyst might seek out a logins table in order to calculate the rate of successful website logins. Reasonably enough, they might compute this rate as the sum of successful events over the total. Now, suppose two users attempt to login to their account, and ultimately, one succeeds in accessing their private information and the other doesnt. The analyst would probably hope to compute and report a 50% login success rate. However, depending on how the data is represented, they could quite easily compute nearly any value from 0% to 100%. Per Attempt: If data is logged once per overall login attempt, successful attempts only trigger one event, but a user who forgot their password may try (and fail) to login multiple times. In the case illustrated above, that deflates the successful login rate to 25%. Per Event: If the logins table contains a row for every login-related event, each success will trigger a large number of positive events and each failure will trigger a negative event preceded by zero or more positive events. In the case illustrated below, this inflates our successful login rate to 86%. Per Conditional: If the collector decided to only look at downstream events, perhaps to circumvent record duplication, they might decide to create a record only to denote the success or failure of the final step in the login process (MFA). However, login attempts that failed an upstream step would not generate any record for this stage because theyve already fallen out of the funnel. In this case, the computed rate could reach 100% Per Intermediate: Similarly, if the login was defined specifically as successful password verification, the computed rate could his 100% even if some users subsequently fail MFA Session Attempt Attempt Outcome Intermediate Success 1 1 6 1 2 Total 2 4 7 1 2 Rate 50% 25% 86% 100% 100% While humans have a shared intuition of what concepts like a user, session, or login are, the act of collecting data forces us to map that intuition onto an atomic event . Any misunderstanding in precisely what that definition is can have massive impact on the perceived data quality; per event data will appear heavily duplicated if it is assumed to be per session data. In some cases, this could be obvious to detect. If the system outputs fields that are incredibly specific (e.g. with some hyperbole, imagine a step_in_the_login_process field with values taking any of the human-readable descriptions of the fifteen processes listed in the image above), but depending how this source is organized (e.g. in contrast to above, if we only have fields like sourceid and processid with unintuitive alphanumeric encoded values) and defined, it could be nearly impossible to understand the nuances without uncovering quality metadata or talking to a data producer.7 2.2.2 What Doesnt Count Along with thinking about what does count (or gets logged), its equally important to understand what systemically does not generate a record. Consider users who have the intent or desire to login (motivated by a real-world DGP) but cannot find the login page, or users who load the login page but never click a button because they know that theyve forgotten their password and see no way to request it. Often, some of these corner cases may be some of the most critical and informative (e.g. here, demonstrating some major flaws in our UI). Its hard to computationally validate what data doesnt exist, so conceptual data validation is critical. 2.2.3 The Many Meanings of Null Related to the presence and absence of full records is the presence or absence of individual fields. If records contain some but not all relevant information, they may be published with explicitly missing fields or the full record may not be published at all. Understanding what the system implies by each explicitly missing data field is also critical for validation and analysis. Checks for data completeness usually include counting null values, but null data isnt always incorrect. In fact, null data can be highly informative if we know what it means. Some meanings of null data might include: Field is not relevant: Perhaps our logins table reports the mobile phone operating system (iOS or Android) that was used to access the login page to track platform-specific issues. However, there is no valid value for this Relevant value is not known: Our logins table might also have an account_id field which attempts to match login attempts to known accounts/customers using different metadata like cookies or IP addresses. In theory, almost everyone trying to log in should have an account identifier, but our methods may not be good enough to identify them in all cases Relevant value is null: Of course, sometimes someone without an account at all might try to log in for some reason. In this case, the correct value for an account_id field truly is null Relevant value was recorded incorrectly: Sometimes systems have glitches. Without a doubt, every single login attempt should have a timestamp, but such a field could be null if this data was somehow lost or corrupted at the source Similarly, different systems might or might not report out these nulls in different ways such as: True nulls: Literally the entry in the resulting dataset is null Null-like non-nulls: Blank values like an empty string ('') that contain a null amount of information but wont be detected when counting null values Placeholder values: Meaningless values like an account_id of 00000000 for all unidentified accounts which preserve data validity (the expected structure) but have no intrinsic meaning Sentinel/shadow values: Abnormal values which attempt to indicate the reasons for null-ness such as an account_id of -1 when no browser cookies were found or -2 when cookies were found but did not help link to any specific customer record Each of these encoding choices changes the definitions of appropriate completeness and validity for each field and, even more critically, impacts the expectations and assertions we should form for data accuracy. We cant expect 100% completeness if nulls are a relevant value; we cant check validity of ranges as easily if sentinel values are used with values that are outside the normal range (hopefully, or we have much bigger problems!) So, understanding how upstream systems should work is essential for assessing if they do work. 2.3 Data Loading Checking that data contains expected and only expected records (that is, completeness, uniqueness, and timeliness) is one of the most common first steps in data validation. However, the superficially simple act of loading data into a data warehouse or updating data between tables can introduce a variety of risks to data completeness which require different strategies to detect. Data loading errors can result in data that is stale, missing, duplicate, inconsistently up-to-date across sources, or complete but for only a subset of the range you think. While the data quality principles of completeness, uniqueness, and timeliness would suggest that records should exist once and only once, the reality of many haphazard data loading process means data may appear sometime between zero and a handful of times. Data loads can occur in many different ways. For example, they might be: manually executed scheduled (like a cron job) orchestrated (with a tool like Airflow or Prefect) No approach is free from challenges. For example, scheduled jobs risk executing before an upstream process has completed (resulting in stale or missing data); poorly orchestrated jobs may be prevented from working due to one missing dependency or might allow multiple stream to get out of sync (resulting in multisource missing data). Regardless of the method, all approaches must be carefully configured to handle failures gracefully to avoid creating duplicates, and the frequency at which they are executed may cause partial loading issues if it is incompatible with the granularity of the source data. 2.3.1 Data Load Failure Modes For example, suppose in the diagram below that each row of boxes represents one day of records in a table. Stale data occurs when the data is not as up-to-date as would be expected from is regular refresh cadence. This could happen if a manual step was skipped, a scheduled job was executed before the upstream source was available, or orchestrated data checks found errors and quarantined new records Missing data occurs when one data load fails but subsequent loads have succeeded Duplicate data occurs when one data load is executed multiple times Multisource missing data occurs when a table is loaded from multiple sources, and some have continued to update as expected while others have not Partial data occurs when a table is loaded correctly as intended by the producer but contains less data than expected by the consumer (e.g. a table loads ever 12 hours but because there is some data for a given date, the user assumes that all relevant records for that date have been loaded) The differences in these failure modes become important when an analyst attempts to assess data completeness. One of the first approaches an analyst might consider is simply to check the min() and max() event dates in their table. However, this can only help detect stale data. To catch missing data, an analyst might instead attempt to count the number of distinct days represented in the data; to detect duplicate data, that analyst might need to count records by day and examine the pattern. Metric Stale Missing Duplicate Multi Partial min(date) max(date) 13 14 14 14 14 count(distinct date) 3 3 4 4 4 count(1) by date 1001001000 1001000100 100100200100 1001006666 10010010050 count(1) count(distinct PKs) 300300 300300 400300 332332 350350 In a case like the toy example above where the correct number of rows per date is highly predictable and the number of dates is small, such eyeballing is feasible; however when the expected number of records varies day-to-day or time series are long, this approach becomes subjective, error-prone, and intractable. Additionally, it still might be hard to catch errors in mutli-source data or partial loads if the lower number of records was still within the bounds of reasonable deviation for a series. These last two types deserve further exploration. 2.3.2 Multi-Source A more effective strategy for assessing data completeness requires a better understanding of how data is being collected and loaded. In the case of multi-source data, one single source stopping loading may not be a big enough change to disrupt aggregate counts but could still jeopardize meaningful analysis. It would be more useful to conduct completeness checks by subgroup to identify these discrepancies. But not any subgroup will do; the subgroup must correspond to the various data sources. For example, suppose we run an e-commerce store and wish to look at sales from the past month by category. Naturally, we might think to check the completeness of the data by category. But what if sales data is sourced from three separate locations: our Shopify site (80%), our Amazon Storefront (15%), and phone sales (5%). Unless we explicitly check completeness by channel (a dimension we dont particularly care about for our analysis), it would be easy to miss if our data source for phone sales has stopped working or loads at a different frequency. Another interesting aspect of multi-source data, is multiple sources can contribute either to different rows/records or different columns/variables. Table-level frequency counts wont help us in the latter case since other sources might create the right total number of records but result in some specific fields in those records being missing or inaccurate. 2.3.3 Partial Loads Partial loads really are not data errors at all, but are still important to detect since they can jeopardize an analysis. A common scenario might occur if a job loads new data every 12 hours (say, data from the morning and afternoon of day n-1 loads on day n at 12AM and 12PM, respectively). An analyst retrieving data at 11AM may be concerned to see an approximate ~50% drop in sales in the past day, despite confirming that their data looks to be complete since the maximum record date is, in fact, day n-1.8 2.3.4 Delayed or Transient Records The interaction between choices made in the data collection and data loading phases can introduce their own sets of problems. Consider an orders table for an e-commerce company that analysts may use to track customer orders. It might contain one record per order_id x event (placement, processing, shipment), one record per order placed, one record per order shipping, or one record per order with a status field that changes over time to denote the orders current stage of life. Any of these modeling choices seem reasonable and the difference between them might appear immaterial. But consider the collection choice to record and report shipped events. Perhaps this might be operationally easier if shipment come from one source system whereas orders could come from many. However, an interesting thing about shipments is that they are often lagged in a variable way from the order date. Suppose the e-commerce company in question offers three shipping speeds at checkout. The chart below shows the range of possible shipment dates based on the order dates for the three different speeds (shown in different bars/colors). How might this effect our perceived data quality? Order data could appear stale or not timely since orders with a given order_date would only load days later once shipped Similar to missing or multisource data, the data range in the table could lead to deceptive and incomplete data validation because some orders from a later order date might ship (and thus be logged) before all orders from a previous order date Put another way, we could have multiple order dates demonstrating partial data loads These features of the data might behave inconsistently across time due to seasonality (e.g. no shipping on weekends or federal holidays), so heuristics developed to clean the data based on a small number of observations could fail From an analytical perspective, orders with faster shipping would be disproportionately overrepresented in the tail (most recent) data. If shipping category correlated with other characteristics like total order spend, this could create an artificial trend in the data Once again, understanding that data is collected at point of shipment and reasoning how shipment timing varies and impacts loading is necessary for successful validation. 2.4 Data Transformation Finally, once the data is roughly where we want it, it likely undergoes many transformations to translate all of the system-generated fields we discussed in data collection into semantically-relevant dimensions for analytical consumers. Of course, the types of transformations that could be done are innumerable with far more variation than data loading. So, well just look at a few examples of common failure patterns. 2.4.1 Pre-Aggregation Data transformations may include aggregating data up to higher levels of granularity for easier analysis. For example, a transformation might add up item-level purchase data to make it easier for an analyst to look at spend per order of a specific user. Data transformations not only transform our data, but they also transform how the dimensions of data quality manifest. If data with some of the completeness or uniqueness issues we discussed with data loading is pre-aggregated, these problems can turn into problems of accuracy. For example, the duplicate or partial data loads that we discussed when aggregated could suggest inaccurately high or low quantities respectively. 2.4.2 Field Encoding When we assess data consistency across tables, Categorical fields in a data set might be created in any number of ways including: Directly taken from the source Coded in a transformation script Transformed with logic in a shared user-defined function (UDFs) or macro Joined from a shared look-up table Each approach has different implications on data consistency and usability. Using fields from the source simply is what it is  theres no subjectivity or room for manual human error. If multiple tables come from the same source, its likely but not guaranteed that they will be encoded in the same way. Coding transformations in the ELT process is easy for data producers. Theres no need to coordinate across multiple processes or use cases, and the transformation can be immediately modified when needed. However, that same lack of coordination can lead to different results for fields that should be the same. Alternatively, macros, UDFs, and look-up tables provided centralized ways to map source data inputs to desired analytical data outputs in a systemic and consistent way. Of course, centralization has its own challenges. If something in the source data changes, the process of updating a centralized UDF or look-up table may be slowed down by the need to seek consensus and collaborate. So, data is more consistent but potentially less accurate. Regardless, such engineered values require scrutiny  paticularly if they are being used as a key to join multiple tables  and the distinct values in them should be carefully examined. 2.4.3 Updating Transformations Of course, data consistency is not only a problem across different data sources but within one data source. Regardless of the method of field encoding used in the previous step, the intersection of data loading and data transformation strategies can introduce data consistency errors over time. Often, for computation efficiency, analytical tables are loaded using an incremental loading strategy. This means that only new records (determined by time period, a set of unique keys, or other criteria) from the upstream source are loaded to the downstream table. This is in contrast to a full refresh where the entire downstream table is recreated on each update. Incremental loads have many advantages. Rebuilding tables in entirety can be very time consuming and computationally expensive. In particular, in non-cloud data warehouses that are not able to scale computing power on demand, this sort of heavy duty processing job can noticeably drain resources from other queries that are trying to run in the database. Additionally, if the upstream staging data is ephemeral, fully rebuilding the table could mean failing to retain history. However, in the case that our data transformations change, incremental loads may introduce inconsistency in our data overtime as only new records are created and inserted with the new logic. This is also a problem more broadly if some short-term error is discovered either with data loading or transformation in historical data. Incremental strategies may not always update to include the corrected version of the data. Regardless, this underscores the need to validate entire datasets and to re-validate when repulling data. 2.5 Conclusion In statistical modeling, the goal of considering the data generating process is not to understand an encode every single nuance of the complete DGP. After all, if all of that were known, we wouldnt need a model: we could simulate the universe. Similarly for data validation, data consumers cannot know everything about the data production DGP without taking over the data production process in its entirety. But understanding some of the key failure modes faced by data producers can support data validation by helping consumers develop more realistic theories and expectations for the ways data may break and how to refine strategies for detection them. Michael Betancourts tutorial is a lovely example. Thanks to Joseph Lewis on Twitter for the reference. The open-source text Modern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber contains more examples. Of course, strategies for collection, moving, transforming, storing, and validating data are innumerable. This is not intended to be a comprehensive guide on any of these topics but simply to illustrate why its important for analysts to keep in mind the interplay between these steps. I dont mean to imply statisticians do not regularly think about the data collection DGP! The rich literatures on missing data imputation, censored data in survival analysis, and non-response bias is survey data collection are just a few examples of how carefully statisticians think about how data collection impacts analysis. I chose to break it out here to discuss the more technical aspects of collection Like NYCs infamously messy turnstile data. I dont claim to know precisely how this dataset is created, but many of the specific challenges it contains are highly relevant. As Angela Bass so aptly writes: Data isnt ground truth. Data are artifacts of systems. Of course, this isnt the only potential type of issue in data collection. While instrumentation often leads to these definitional challenges, other types of data collection like sensors can have other types of challenges like systematically failing to capture certain observations. Consider, for example, bus ridership data collected as riders scan their pass upon entering. If students can ride free by showing the driver their student ID, these observations may be systemically not recorded. Again, relying on an operational system could lead analytics uses astray (like failing to account for peak usage times for this demographic.) Of course, this concern could be somewhat easily allayed if they then checked a timestamp field, but such a field might not exists or might not have been used for validation since its harder to anticipate the appropriate maximum timestamp than it is the maximum date. "],["comp-quan.html", "Chapter 3 Computational Quandries 3.1 Understanding data table structure 3.2 The meaning of a row 3.3 Meaning of a row example 3.4 Delayed data loads 3.5 The many forms of null 3.6 Null value aggregation 3.7 Null value filtering 3.8 Encoding with defaults 3.9 Working with strings 3.10 Working with dates 3.11 Assessing equality 3.12 Merging mayhem 3.13 Order of operations 3.14 Modifying versus copying 3.15 Assuming that Tools Know Best 3.16 Strategies", " Chapter 3 Computational Quandries In practice, data analysis requires using a number of advanced computational tools such as SQL, R, or python. Analysts must work in a partnerships with each tool to access and wrangle their data into an accessible form of information. However, the moment you as an analyst begins to use a tool, the conversation is no longer simply between you and the data; suddenly, thousands of developers who helper build your tools are now crowding into the room. Each may have contributed to the code behind your tool with a different mental model of how one would use it. In this chapter, we will explore common ways that tools may do something correct, reasonable, and as-intended but very much not what we would have liked as analysts. TODO: Introduce datasets head(registration) ## ID_CUSTOMER CD_DEVICE AMT_SPEND AMT_RETURN ## 1 1 1 10 NA ## 2 2 1 20 NA ## 3 3 1 30 NA ## 4 4 2 40 NA ## 5 5 2 50 NA ## 6 6 2 NA NA 3.1 Understanding data table structure Understanding the content and structure of the data you are using is a critical prerequisite to analysis. In this book, we focus on tabular, structured data like one might find in an Excel spreadsheet or relational database.9 In particular, many tools work best with what R developer Hadley Wickham descibes as tidy data (Wickham 2014). Namely: Each variable forms a column Each observation forms a row Each type of observational unit forms a table This is analogous to how one generally finds data arranged in a database and how statisticians are used to conceptualizing it. For example, the design matrix of a linear model consists of one column of data for each independent variable to be included in the model and one row for each observation.10 Tidy data (Wickham 2014) Things you need to know: Data columns/variables - what do they mean (dont assume from name!) Field encodings - what do subfields mean and what are their limits? Data rows - what is a unique observation? Data granularity - when is it refreshed? How deep does it look when its refreshed? Strategies: Data dictionaries Business rule checks Consult with experts 3.2 The meaning of a row Often, data comes with an accompanying data dictionary to describe the meaning of each variable (column). However, for some reason, describing what constitutes an observation (row) is less common practice. This may seem obvious. In two of Rs most popular built-in datasets, iris and mtcars, its somewhat evident from context that each row represents one flower or one car. However, sometimes trying to glean this information from context alone can be misleading. Imagine, for example, that we want to calculate the rate of successful log-ins to our e-commerce website. ## Warning: package &#39;tidyr&#39; was built under R version ## 4.0.3 summarize(logins, PROP_LOGIN = mean(IND_LOGIN)) ## # A tibble: 1 x 1 ## PROP_LOGIN ## &lt;dbl&gt; ## 1 0.4 logins %&gt;% group_by(ID_ACCT) %&gt;% summarize(IND_LOGIN = max(IND_LOGIN)) %&gt;% ungroup() %&gt;% summarize(PROP_LOGIN = mean(IND_LOGIN)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 1 x 1 ## PROP_LOGIN ## &lt;dbl&gt; ## 1 0.6 3.3 Meaning of a row example One of the tricky nuances of data collection is understanding what precisely is getting captured and logged in the first place. Consider something as simple as a login system where users must enter their credentials, endure a Captcha-like verification that they are not a robot, and enter a multi-factor authentication code. Which of these events gets collected and recorded has a significant impact on subsequent data processing. In a technical sense, no inclusion/exclusion decision here is incorrect, persay, but if the producers choices dont match the consumers understandings, it can lead to misleading results. An analyst might seek out such events recorded in a logins table in order to calculate the rate of successful logins. Reasonably enough, they might compute this rate as the some of successful events over the total. Now, suppose two users attempt to login to their account, and ultimately, one succeeds in accessing their private information and the other doesnt. Intuitively, the analyst would probably hope to compute and report a 50% login success rate. However, depending on how the data is represented, they could quite easily compute nearly any value from 0% to 100%. If data is logged once per overall login attempt, successful attempts only trigger on event, but a user who forgot their password may try (and fail) to login multiple times. In the case illustrated above, that deflates the successful login rate to 25%. If the logins table contains a row for every login-related event, each success will trigger a large number of positive events and each failure will trigger a negative event preceded by zero or more positive events. In the case illustrated below, this inflates our successful login rate to 86%. If the collector decided to only look at downstream events, perhaps to circumvent record duplication, they might decide to create a record only to denote the success or failure of the final step in the login process (MFA). However, login attempts that failed an upstream step would not generate any record for this stage because theyve already fallen out of the funnel. In this case, the computed rate could reach 100% Similarly, if the login was defined specifically as successful password verification, the computed rate could his 100% even if some users subsequently fail MFA Session Attempt Attempt Outcome Intermediate Success 1 1 6 1 2 Total 2 4 7 1 2 Rate 50% 25% 86% 100% 100% While humans as users have a shared intuition of what concepts like a user, session, or login are, the act of collecting data requires us to force that intuition into a concrete definition. Any misunderstanding in precisely what that definition is can have massive impact on the perceived data quality (e.g. per event data will appear heavily duplicated if it is assumed to be per session data.) In some cases, this could be obvious to detect. If the system outputs fields that are incredibly specific (e.g. with some hyperbole, imagine a step_in_the_login_process field with values taking any of the fifteen processes listed in the image above) 3.4 Delayed data loads 3.5 The many forms of null Frequently, real-world data sets suffer from at least some missing values. This missing data can cause complex computational and analytical challenges. Considering why ones data is missing and how such missing is encoded is critical before attempting an analysis. Its tempting to think of missingness as a binary status: either a datum exists or it does not. However, missingness can arise from a number of different situations  each with its own unique computational and analytical challenges. For example, in a tabular data representation, a variable (column) for an observation (row) might appear to be missing because: There exists a true value for that variable and entity but it is unknown There is no relevant value for that variable The relevant value for that variable is null For example, consider the registrations dataset with data on users that have created an account at an online e-commerce platform: Users might be asked but not required to provide their date of birth; so, while every user has a birthday, only those that provide it would have a value for this field. Those that do not would be encoded as a NULL; (could also be only started collecting after certain dates, at a certain set of stores, data load error) We might also wish to record for mobile users if they were on an Android or and iPhone device when they registered. However, for users registering from a computer, there is no relevant value for this field. Often, retailers want to attribute user traffic to different forms of advertising, so we might also have a field for the URL that directed users to our site. However, for users that truly typed in the URL directly and did not come through an affiliate link, the true value of the referring site is NULL. (Admittedly, there is significant overlap in the second two cases.) Just as there are many potential causes for missingness in our data, there can often be many potential encodings of missingness. The naniar package (Tierney et al. 2020) Talk about encoding as like 9999 R has NA, NaN, Inf, NULL, and typed NAs While this section focuses on the computational challenges of null values, no discussion of missingness would be complete without also mentioning the analytical consequences as well. Many statistical techniques such as linear regression are unable to accept null values in their inputs, so an analyst must somehow confront missingness before passing their data to the algorithm. Broadly speaking, analysts must either remove missing values or replace them with a proxy imputed value. There is a rich literature on missingness beyond the scope of this book, but briefly speaking this choice can be guided by the following framework. These different types of missingness must be handled differently because they insert different biases into our data. Someone (TODO: who and add citiation) classifies the level of missingness as: Missing Completely at Random (MCAR): TODO: finish defining these Missing at Random (MAR): Missing not a Random (MNAR): Comparing these categories to the examples above, Literature about imputation; mention mice package (???) One example of this is evident in the US Census Bureaus Medical Expenditure Panel Survey which uses the following reserved codes to denote different types of missingness. (TODO p10 https://www.meps.ahrq.gov/data_stats/download_data/pufs/h206a/h206adoc.pdf) -1 INAPPLICABLE Question was not asked due to skip pattern -7 REFUSED Question was asked and respondent refused to answer question -8 DK Question was asked and respondent did not know answer -14 NOT YET TAKEN/USED Respondent answered that the medicine has not yet been used -15 CANNOT BE COMPUTED Value cannot be derived from data 3.6 Null value aggregation As weve seen, null values in our data can mean many different things and be represented in many different ways. However, even once we have locked down the semantic meaning of nulls in our data and considered alternatives for their encoding, these values may still cause computational challenges.11 How null values are handled in the simple aggregation of data varies both across different languages and across different functions within a language. To better understand the problems this might cause, we will look at examples in R and SQL. First, consider the registration data set. To compute the average amount spent (AMT_SPEND) with the dplyr package, an analyst might first reasonably write the following summarize() statement. However, as we can see, due to the presence of null values within the AMT_SPEND column, the result of this aggregation is for the whole quantiaty of AVG_SPEND to be set to the value NA. summarize(registration, AVG_SPEND = mean(AMT_SPEND)) ## AVG_SPEND ## 1 NA A glance at the documentation for the mean() function reveals that it has an na.rm parameter which, when set to true, removes null values from our dataset. Adding this argument to the previous statement allows us to reach a numerical answer. summarize(registration, AVG_SPEND = mean(AMT_SPEND, na.rm = TRUE)) ## AVG_SPEND ## 1 48.57 However, is this the right numerical answer? What na.rm = TRUE does is drop the null values from the set of numbers being averaged. However, suppose the null values represent that no purchases were made. That is, zero dollars were spent. In effect, we have removed all non-purchasers from the data being averaged. More precisely, we have switched from taking the average \\[ \\frac{ \\sum_{1}^{n} Spend }{\\sum_{1}^{n} 1} \\] over all \\(n\\) customers to taking the average \\[ \\frac{ \\sum_{Spend &gt; 0} Spend }{\\sum_{Spend &gt; 0} 1} \\] over only those customers with spend At face value, we could say that the code above is giving the incorrect answer; by dropping some low (zero) purchase amounts, the average amount spend per customer is inflated. A second and even more troubling perspective is that this tiny change to the code which seemed like a reasonable attempt to fix an obvious problem has introduced a non-obvious problem by fundamentally changing the question that we are asking. By dropping all accounts from our table who made no purchases, we are no longer answering What is the average amount spent by a new registrant? but rather What is the average amount spent by an actively engaged customer? This technical quirk has significant analytical impact. To answer the real question at hand, we would instead have a couple of options. We could sum() the amount spent with the option to drop nulls but then divide by the correct denominator (all observations  not just those with spend) or we could explicitly recode null values in AMT_SPEND to zero before taking the average. This leads to a lower but correct conclusion. summarize(registration, AVG_SPEND_v1 = sum(AMT_SPEND, na.rm = TRUE) / n(), AVG_SPEND_v2 = mean(ifelse(is.na(AMT_SPEND), 0, AMT_SPEND)) ) ## AVG_SPEND_v1 AVG_SPEND_v2 ## 1 34 34 This is all well and good if we could just accept that the behaviors above are simply how nulls work, but further complexity comes as we see that there is no industry standard across tools. For example, as the SQL code below shows, SQLs avg() function behaves more like Rs mean() with the na.rm = TRUE option set. That is, the default behavior of SQL is to only operate on the valid and available values. SELECT avg(amt_spend) FROM registration ## avg(amt_spend) ## 1 48.57 However, this is not to suggest that null values cannot also be destructive in SQL. While aggregation functions (which compute across a row) like sum() and avg() drop nulls, operators like + and - working across columns in the same row do not exhibit the same behavior. Consider, for example, if we wish to calculate the average net purchase amount (purchases minus returns) instead of the gross (total) purchase amount. SELECT avg(amt_spend-amt_return) FROM registration ## avg(amt_spend-amt_return) ## 1 NA Despite what we learned above about the avg() function, the query above returns only a null value. What has happened? In our registration data set, the amt_return column is completely null (representing no returns). Because the subtraction occurs before the average is taken, subtracting real numbers in amt_spend with null values in amt_return creates a column of all null values which are then fed into the avg() function. This process is shown step-by-step below. SELECT amt_spend, amt_return, amt_spend-amt_return FROM registration ## AMT_SPEND AMT_RETURN amt_spend-amt_return ## 1 10 NA NA ## 2 20 NA NA ## 3 30 NA NA ## 4 40 NA NA ## 5 50 NA NA ## 6 NA NA NA ## 7 NA NA NA ## 8 NA NA NA ## 9 90 NA NA ## 10 100 NA NA 3.7 Null value filtering dplyr excludes NAs in filter data.frame(x = c(1, 0, NA)) %&gt;% filter(x != 1) ## x ## 1 0 SQL excludes NAs in WHERE select * from registration where amt_spend != 10 ## ID_CUSTOMER CD_DEVICE AMT_SPEND AMT_RETURN ## 1 2 1 20 NA ## 2 3 1 30 NA ## 3 4 2 40 NA ## 4 5 2 50 NA ## 5 9 3 90 NA ## 6 10 3 100 NA 3.8 Encoding with defaults Often, to faciliate an analysis, we wish to recode variables from one form to another. There are many reasons we might wish to recode variables. Sometimes, our data may be represented with system-generated codes that obscure the context-specific meaning of fields. For example, if you are analyzing US Census data, it might identify different regions by their FIPS code while a human might prefer to see the actual name of a state or a county. In other cases, we wish to recode fields in order to change their level of granularity. For example, we might wish to group categories such as apple, banana, and orange into the fruit category. Common functions for recoding include base::ifelse, dplyr::if_else, and dplyr::case_when in R and CASE statements in SQL. Generally, all of these work by: specifying one or more logical conditions based on other column(s) in the dataset for each logical condition, specifying the new value that the variable should take if none of the conditions are met, providing a default value However, analysts may often take on slight short-cut registration &lt;- mutate(registration, CAT_DEVICE = case_when( CD_DEVICE == 1 ~ &quot;IOS&quot;, TRUE ~ &quot;Android&quot; ) ) registration %&gt;% group_by(CAT_DEVICE) %&gt;% count() ## # A tibble: 2 x 2 ## # Groups: CAT_DEVICE [2] ## CAT_DEVICE n ## &lt;chr&gt; &lt;int&gt; ## 1 Android 7 ## 2 IOS 3 unique(registration$CD_DEVICE) ## [1] 1 2 3 registration &lt;- mutate(registration, IND_SMALL_PURCH = case_when( AMT_SPEND &lt; 50 ~ 1, TRUE ~ 0) ) registration %&gt;% group_by(IND_SMALL_PURCH) %&gt;% count() ## # A tibble: 2 x 2 ## # Groups: IND_SMALL_PURCH [2] ## IND_SMALL_PURCH n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 6 ## 2 1 4 registration &lt;- mutate(registration, IND_SMALL_PURCH = if_else(AMT_SPEND &lt; 50, 1, 0)) registration %&gt;% group_by(IND_SMALL_PURCH) %&gt;% count() ## # A tibble: 3 x 2 ## # Groups: IND_SMALL_PURCH [3] ## IND_SMALL_PURCH n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 3 ## 2 1 4 ## 3 NA 3 3.9 Working with strings 3.10 Working with dates 3.11 Assessing equality Automatic conversion of data types Dates versus timestamps df_dt &lt;- data.frame( DT_ENROLL = as.Date(&quot;2020-01-01&quot;), DT_PURCH = 20200101, DT_LOGIN = as.POSIXlt(&quot;2020-01-01T12:00:00&quot;) ) filter(df_dt, DT_ENROLL == DT_PURCH) %&gt;% nrow() ## [1] 0 sqldf(&quot;select * from df_dt where DT_ENROLL = DT_PURCH&quot;) ## [1] DT_ENROLL DT_PURCH DT_LOGIN ## &lt;0 rows&gt; (or 0-length row.names) as.numeric(df_dt$DT_ENROLL) ## [1] 18262 sqldf(&quot;select cast(DT_ENROLL as integer) from df_dt&quot;) ## cast(DT_ENROLL as integer) ## 1 18262 Note this this can affect both filters and joins 3.12 Merging mayhem 3.13 Order of operations 3.14 Modifying versus copying 3.15 Assuming that Tools Know Best Spark random forest example (https://issues.apache.org/jira/browse/SPARK-5133) I think we should calculate importance based on the learned model. The permutation test would be nice in the future but would be much more expensive (shuffling data). 3.16 Strategies Read the documentation Talk to experts Look at examples Try corner cases Write unit tests (have your computer try corner cases for you) testthat R Package (Wickham 2020) assertr R package (Fischetti 2020) pointblank R package (Iannone and Vargas 2021) References "],["eg-agg.html", "Chapter 4 Egregious Aggregations 4.1 Aggregating without visualizing 4.2 Believing in the average observation 4.3 Understanding the denominator 4.4 Small sample sizes 4.5 Relying on the wrong summary metrics 4.6 Dichotomization 4.7 Ignoring trend 4.8 Ignoring seasonality 4.9 Ignoring panel structure 4.10 Strategies", " Chapter 4 Egregious Aggregations 4.1 Aggregating without visualizing The datasauRus R package (Locke and DAgostino McGowan 2018) 4.2 Believing in the average observation data %&gt;% summarize( N_TRANS = n() / n_distinct(ID_ACCT), AMT_SPEND = sum(AMT_SPEND) / n() ) %&gt;% mutate(N_TRANS * AMT_SPEND) ## N_TRANS AMT_SPEND N_TRANS * AMT_SPEND ## 1 9.2 370 3404 data %&gt;% group_by(ID_ACCT) %&gt;% summarize(N_TRANS = n(), AMT_SPEND = sum(AMT_SPEND) / n()) %&gt;% summarize_at(vars(N_TRANS, AMT_SPEND), mean) %&gt;% mutate(N_TRANS * AMT_SPEND) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 1 x 3 ## N_TRANS AMT_SPEND `N_TRANS * AMT_SPEND` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9.2 364. 3345. summarize(data, sum(AMT_SPEND) / n_distinct(ID_ACCT)) ## sum(AMT_SPEND)/n_distinct(ID_ACCT) ## 1 3404 \\[ \\frac{ \\sum_{1}^{n} Spend }{\\sum_{1}^{n} 1} \\ * \\frac{ \\sum_{1}^{n} Trips }{\\sum_{1}^{n} 1} \\] over all \\(n\\) customers \\[ \\sum_{1}^{n} Spend \\ * \\sum_{1}^{n} Trips \\] set.seed(123) amt_spend &lt;- c(rnorm(10, 50, 5), rnorm(10, 250, 5)) ind_return &lt;- c(rbinom(10,1,0.1), rbinom(10,1,0.2)) mean(amt_spend) * mean(ind_return) ## [1] 7.535 mean(amt_spend * ind_return) ## [1] 12.68 4.3 Understanding the denominator 4.4 Small sample sizes 4.5 Relying on the wrong summary metrics The paper A Dirty Dozen: Twelve Common Metric Interpretation Pitfalls in Online Controlled Experiments (Dmitriev et al. 2017) Article Designing and evaluating metrics (Taylor 2020) Problem with Metrics - https://arxiv.org/abs/2002.08512 4.6 Dichotomization 4.7 Ignoring trend 4.8 Ignoring seasonality 4.9 Ignoring panel structure 4.10 Strategies References "],["vex-viz.html", "Chapter 5 Vexing Visualization 5.1 Selecting useful metrics 5.2 Picking appropriate reference point 5.3 Creating meaningful comparisons 5.4 Drawing attention to what matters 5.5 Strategies", " Chapter 5 Vexing Visualization Clause Wilkes book (Wilke 2019) 5.1 Selecting useful metrics 5.2 Picking appropriate reference point 5.3 Creating meaningful comparisons 5.4 Drawing attention to what matters 5.5 Strategies References "],["inc-inf.html", "Chapter 6 Incredible Inferences 6.1 No law to use ALL the data 6.2 Ascribing characteristics at wrong granularity 6.3 Finding policy-induced relationships 6.4 Ignoring heterogeneity 6.5 If trends continue 6.6 Analyzing time-to-event data 6.7 Answering the right question 6.8 Desparation for insight 6.9 Sample splitting 6.10 Strategies", " Chapter 6 Incredible Inferences 6.1 No law to use ALL the data 6.2 Ascribing characteristics at wrong granularity ecological fallacy (does this belong in causation chapter?) 6.3 Finding policy-induced relationships selection bias 6.4 Ignoring heterogeneity 6.5 If trends continue 6.6 Analyzing time-to-event data immortal time bias 6.7 Answering the right question Dont let available tools dictate the questions of interest The Cult of Statistical Significance (Ziliak and McCloskey 2008) Mindless Statistics (Gigerenzer 2004) 6.8 Desparation for insight Data dredging, p-hacking 6.9 Sample splitting The nullabor (Wickham et al. 2020) R package 6.10 Strategies References "],["cav-caus.html", "Chapter 7 Cavalier Causality 7.1 Common Cause, Same Effects 7.2 Common Cause, Opposing Effects 7.3 Sequential Cause 7.4 Abandoning observations 7.5 Strategies", " Chapter 7 Cavalier Causality We talk about the FOO method in this chapter. 7.1 Common Cause, Same Effects 7.2 Common Cause, Opposing Effects 7.3 Sequential Cause 7.4 Abandoning observations 7.5 Strategies "],["mind-mod.html", "Chapter 8 Mindless Modeling 8.1 Chosing the right target 8.2 Developing meaningful features 8.3 Selecting an algorithm 8.4 Evaluating the right performance metrics 8.5 Strategies", " Chapter 8 Mindless Modeling We talk about the FOO method in this chapter. 8.1 Chosing the right target How you chose to model depends not just on your target variable and the relationships in your data but also the question to be answered. For example, (Murray 2020) describes how the best approach to forecasting case growth in a pandemic varies depending on whether the goal is to plan population-level interventions and policies or organize short-term hospital capacity. 8.2 Developing meaningful features 8.3 Selecting an algorithm 8.4 Evaluating the right performance metrics 8.5 Strategies References "],["alt-alg.html", "Chapter 9 Alternative Algorithms 9.1 Modeling Counts 9.2 Modeling Time Until an Event 9.3 Modeling Repeated Measures on a Population 9.4 Modeling Observations in a Nested Hierarchy 9.5 Modeling Time &amp; Space Data", " Chapter 9 Alternative Algorithms 9.1 Modeling Counts 9.2 Modeling Time Until an Event 9.3 Modeling Repeated Measures on a Population 9.4 Modeling Observations in a Nested Hierarchy 9.5 Modeling Time &amp; Space Data "],["rec-rep.html", "Chapter 10 Reckless Irreproducibility 10.1 Automating your personal assistant 10.2 Looking before you leap 10.3 Organizing your files 10.4 Saving your work 10.5 Saving your environment 10.6 Working in public 10.7 Strategies", " Chapter 10 Reckless Irreproducibility Good Enough Practices in Computational Reproducibility (Wilson et al. 2017) Opinionated Analysis Development (Parker 2017) 10.1 Automating your personal assistant 10.2 Looking before you leap 10.3 Organizing your files 10.4 Saving your work 10.5 Saving your environment The renv R package (Ushey 2020) 10.6 Working in public 10.7 Strategies References "],["more-to-say.html", "A More to Say", " A More to Say Yeah! I have finished my book, but I have more to say about some topics. Let me explain them in this appendix. To know more about bookdown, see https://bookdown.org. "],["references.html", "References", " References "]]
