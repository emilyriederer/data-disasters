# Mindless Modeling (TODO) {#sec-mind-mode}

Excellent reproducibility paper: https://reproducible.cs.princeton.edu/
feature leakage, model sheets

Illusion of progress: https://arxiv.org/pdf/math/0606441.pdf

## Features

Ratio Variables: https://journals.sagepub.com/doi/abs/10.1177/1094428118773455?journalCode=orma


### Engineering the wrong features

```{r results = 'hold', warning = FALSE, echo = FALSE, eval = FALSE}
# data set-up ----
library(dplyr)
data(ames, package = "modeldata")
ames1 <- select(ames, where(is.numeric))
ames2 <- mutate(ames1, Lot_Area = sqrt(Lot_Area))
ames3 <- mutate(ames1, Lot_Area = Lot_Area**2)

# model building ----
library(randomForest)
set.seed(123)
rf1 <- randomForest(Sale_Price ~ ., mtry = 5, ntree = 100, data = ames1)
set.seed(123)
rf2 <- randomForest(Sale_Price ~ ., mtry = 5, ntree = 100, data = ames2)
set.seed(123)
rf3 <- randomForest(Sale_Price ~ ., mtry = 5, ntree = 100, data = ames3)

# predictions ----
pred1 <- predict(rf1)
pred2 <- predict(rf2)
pred3 <- predict(rf3)

# evaluate ----
mean(abs(pred1 - pred2) > 1e-5)
mean(abs(pred1 - pred3) > 1e-5)
```

```{r eval = FALSE, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, echo = FALSE}
train <- sample(nrow(ames1), size = floor(nrow(ames) * 0.5), replace = FALSE )
ames4 <- mutate(ames, Sqrt_Lot_Area = sqrt(Lot_Area), Sqrd_Lot_Area = Lot_Area**2)


library(caret)

grid <-  expand.grid(mtry = 1:20, splitrule = "variance", min.node.size = 10)
fitControl <- trainControl(method = "CV",
                           number = 5,
                           verboseIter = TRUE)

set.seed(123)
fit_orig = train(
  x = select(ames4, -Sale_Price, -Sqrt_Lot_Area, -Sqrd_Lot_Area),
  y = pull(ames4, Sale_Price),
  method = 'ranger',
  num.trees = 100,
  tuneGrid = grid,
  trControl = fitControl
)

set.seed(123)
fit_sqrt = train(
  x = select(ames4, -Sale_Price, -Sqrt_Lot_Area),
  y = pull(ames4, Sale_Price),
  method = 'ranger',
  num.trees = 100,
  tuneGrid = grid,
  trControl = fitControl
)

set.seed(123)
fit_sqrd = train(
  x = select(ames4, -Sale_Price, -Sqrd_Lot_Area),
  y = pull(ames4, Sale_Price),
  method = 'ranger',
  num.trees = 100,
  tuneGrid = grid,
  trControl = fitControl
)


set.seed(123)
fit_both = train(
  x = select(ames4, -Sale_Price),
  y = pull(ames4, Sale_Price),
  method = 'ranger',
  num.trees = 100,
  tuneGrid = grid,
  trControl = fitControl
)

bind_rows(
  mutate(fit_orig$bestTune, type = "Orig"),
  mutate(fit_sqrt$bestTune, type = "Orig + Sqrt"),
  mutate(fit_sqrd$bestTune, type = "Orig + Sqrd"),
  mutate(fit_both$bestTune, type = "Orig + Sqrt + Sqrd")
) %>%
  select(type, mtry) %>%
  knitr::kable(caption = "Number of variables to sample given inputs")
```



<!--

```{r eval = FALSE}
library(tidymodels)
library(tidyr)
library(ggplot2)
set.seed(123)

# set up data ----
n <- 2000
df <-
data.frame(
  x1 = runif(n),
  x2 = runif(n),
  x3 = runif(n)
)
df$y1 <- 10*df$x1 + rnorm(n)
df$y2 <- 10*(df$x1)**2 + rnorm(n)
df$y3 <- 10*(df$x1 / df$x2) + rnorm(n)
df$y4 <- df$y3 + 0.5*df$x3

df_test <- df[1001:2000,]
df <- df[1:1000,]

# model fits ----
lr <- 
  linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")
dt <- 
  decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("regression")
rf <- 
  rand_forest() %>%
  set_engine("randomForest") %>%
  set_mode("regression")
models <- list(lr, dt, rf)
formulas <- list(
  y1 ~ x1,
  y1 ~ I(x1**2),
  y2 ~ x1,
  y2 ~ I(x1**2),
  y3 ~ x1 + x2,
  y3 ~ I(x1/x2),
  y4 ~ x1 + x2 + x3,
  y4 ~ I(x1/x2) + x3
)
fml_target <- vapply(formulas, function(x) as.character(eval(x)[[2]]), character(1))
fml_rhs <- vapply(formulas, function(x) paste(labels(terms(x)), collapse = " + "), character(1))

formulas_df <- tibble(
  formulas = formulas, 
  target = fml_target,
  formula_desc = fml_rhs
)
models_df <- tibble(
  model_desc = factor(c("linear", "decision tree", "random forest"),
                      levels = c("linear", "decision tree", "random forest")),
  models = models
)
targets_df <- tibble(
  target = paste0("y", 1:4),
  target_true_spec = c("linear", "quadratic", "ratio", "ratio plus linear")
)

# model predictions ----
models_formulas_fits <-
tidyr::crossing(models_df, formulas_df) %>%
  left_join(targets_df, by = "target") %>%
  rowwise() %>%
  mutate(
    target = as.character(eval(formulas)[[2]]),
    fits = list(fit(models, formulas, data = df)),
    preds = list(predict(fits, new_data = df_test)[[".pred"]]),
    mse = sqrt(sum(preds - df_test[[target]])**2)
    )

models_formulas_fits <- 
    models_formulas_fits %>%
    group_by(model_desc) %>%
    mutate(mse_factor = mse / min(mse))

# plot results ----
ggplot(data = models_formulas_fits) +
  aes(x = model_desc, y = mse, col = formula_desc) +
  geom_point() + 
  facet_wrap(target_true_spec ~ ., ncol = 1, scales = "free") +
  theme(
    axis.title = element_blank()
  )
```

```{r eval = FALSE}
tree_rec <- recipe(x = df, formula = y1 ~ x1)

tune_spec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) %>%
  set_mode("regression") %>%
  set_engine("ranger")

tune_wf <- 
  workflow() %>%
  add_recipe(tree_rec) %>%
  add_model(tune_spec)

set.seed(123) 
trees_fold <- vfold_cv(df)

set.seed(456)
tune_res <- tune_grid(
  tune_wf,
  resamples = trees_fold,
  grid = 20
)

final_rf <-
  finalize_model(
    tune_spec,
    select_best(tune_res, metric = "rmse")
  )

final_res <-
  final_rf %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(formula = y1 ~ x1, data = df)

predict(final_res, new_data = df_test)
```

--> 

## Targets

## Evaluation Metrics 

## Unsupervised Learning

## Lifecycle Management

<!--

We talk about the _FOO_ method\index{FOO} in this chapter.

## Chosing the right target

How you chose to model depends not just on your target variable and the relationships in your data but also the question to be answered. For example, [@murray2020] describes how the "best" approach to forecasting case growth in a pandemic varies depending on whether the goal is to plan population-level interventions and policies or organize short-term hospital capacity.

## Developing meaningful features



## Selecting an algorithm

## Evaluating the right performance metrics

## Strategies

-->

## Fair and Ethical Modeling


