{
  "hash": "f183a83487519847c1b2842631e2eb19",
  "result": {
    "engine": "knitr",
    "markdown": "# Egregious Aggregations (WIP) {#eg-agg}\n\nOnce armed with an understanding of the data and tools available for analysis, a common start to analysis is exploring data with *aggregation*. \nAt its heart, any sort of data analysis is the process of condensing raw data into something more manageable and useful while giving up as little of the information as possible.\nFrom linear regressions and hypothesis testing to random forests and beyond, much of data analysis could truly be called \"applied sums an averages\".\n\nMany elementary tools for this task are much better at the comprehension task than the preservation one. \nWe learn rigorous assumptions to consider and validate when studying linear regression, but basic arithmetic aggregation presents itself as agnostic and welcome to any type of data. \nHowever, the underlying distributions of our variables and the relationships between them have a significant impact on the how informative and interpretable various summarizations are.\n\nIn this chapter, we will explore different ways that univariate and multivariate aggregations can be naive or uninformative.\n\n## Motivating Example: Similar in Summary\n\nTo begin, we will look at a whimsical toy example. \nThis may feel trite or manufactured, but the subsequent sections will aim to convince you that these issues are not just esoteric.\nConsider the \"datasaurus dozen\" dataset [@datasaurus] which is available within the `datasauRus` R package [@R-datasauRus]. \n\n\n::: {.cell eror='false'}\n\n```{.r .cell-code}\nlibrary(datasauRus)\n```\n:::\n\n\nThis dataset contains 12 sets of data stacked on top of one another and identified by the `dataset` column.^[If you are following along in R, you might run `unique(df$dataset)` to see all the values of this column. We won't do that now as to not ruin the surprise].\nBesides the identifier column, the data is fairly small and contains only two variables `x` and `y`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- datasauRus::datasaurus_dozen\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  dataset       x       y\n1    dino 55.3846 97.1795\n2    dino 51.5385 96.0256\n3    dino 46.1538 94.4872\n4    dino 42.8205 91.4103\n5    dino 40.7692 88.3333\n6    dino 38.7179 84.8718\n```\n\n\n:::\n:::\n\n\nA quick analysis of summary statistics reveals that each of the 12 datasets is very consistent in its summary statistics. \nThe means and variances of `x` and `y` and even their correlations are nearly identifcal.\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Summary statistics for Datasaurus Dozen datasets\n\n|dataset    | mean(x)| mean(y)|  var(x)|  var(y)| cor(x, y)|\n|:----------|-------:|-------:|-------:|-------:|---------:|\n|away       |  54.266|  47.835| 281.227| 725.750|    -0.064|\n|bullseye   |  54.269|  47.831| 281.207| 725.533|    -0.069|\n|circle     |  54.267|  47.838| 280.898| 725.227|    -0.068|\n|dino       |  54.263|  47.832| 281.070| 725.516|    -0.064|\n|dots       |  54.260|  47.840| 281.157| 725.235|    -0.060|\n|h_lines    |  54.261|  47.830| 281.095| 725.757|    -0.062|\n|high_lines |  54.269|  47.835| 281.122| 725.763|    -0.069|\n|slant_down |  54.268|  47.836| 281.124| 725.554|    -0.069|\n|slant_up   |  54.266|  47.831| 281.194| 725.689|    -0.069|\n|star       |  54.267|  47.840| 281.198| 725.240|    -0.063|\n|v_lines    |  54.270|  47.837| 281.232| 725.639|    -0.069|\n|wide_lines |  54.267|  47.832| 281.233| 725.651|    -0.067|\n|x_shape    |  54.260|  47.840| 281.231| 725.225|    -0.066|\n\n\n:::\n:::\n\n\nHowever, as shown in @fig-dino, when we visualize this data, we find that the 12 datasets reveal remarkably different patterns.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatterplots for Datasaurus Dozen datasets](egre-aggr_files/figure-html/fig-dino-1.png){#fig-dino width=672}\n:::\n:::\n\n\nThis dataset is a more elaborate version of Anscombe's Quartet, a well-known set of four datasets which exhibit similar properties. \nExamining a simiilar plot for Anscombe's Quartet (with data from the `anscombe` dataset which ships in R's `datasets` package), we can get better intuition for how the phenomenon is manufactured. \n@fig-anscombe shows as similar plot to @fig-dino.\nComparing datasets 1 and 3, for example, we can see a trade-off between a semi-strong trend with a consistent-seeming amount of noise and an nearly perfect linear trend with a single outlier.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tidyr' was built under R version 4.0.5\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Values are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = length` to identify where the duplicates arise\n* Use `values_fn = {summary_fun}` to summarise duplicates\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(x, y)`\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Scatterplots for Anscombe's Quartet](egre-aggr_files/figure-html/fig-anscombe-1.png){#fig-anscombe width=672}\n:::\n:::\n\n\n@fig-anscombe also plots the simple linear regression line for each dataset.\nSimilar to the summary statistics, these are also identical. \nWe know this because the regression coefficient for a simple linear regression is given by `cov(x,y)`/`sd(x)sd(y)`. You'll notice I do not write \"we can see that...\" because, in fact, we can only *see* similarity not equality. \nThe message of this section may seem to be \"don't summarize your data without plotting it\", but conducting \"visual analytics\" without looking at the numbers is also problematic.\nWe'll explore the latter topic more in Chapter -@sec-vexi-visu (Vexing Visualiztions).\n\nWhile there are clearly a contrived example (and, if you so chose to check out the \"Datasaurus Dozen\" paper, a very cleverly contrived example!), its also a cautionary tale. \nSummary statistics are not just insufficient when they focus on central tendency (e.g. mean) instead of spread. \nIn this example, even an examination of variation and covariation led to an overly simplistic view of the underlying data. \n\n## Averages (WIP)\n\n### Implicit assumptions (TODO)\n\nWhen statistics students study linear regression, they are introduced to a number of canonical assumptions including:\n\n- The true functional form between the dependent and independent variables is linear / additive\n- Errors are independent\n- Errors have constant variance (that is, they are homoskedastic)\n- Errors have a normal distribution\n\nOf course, whether or not these assumptions hold, there's nothing stopping anyone from *mechanically* fit at linear regression^[In fact, the only mechanical constraint to computing linear regression output is that no column of the design matrix (no independent variable) is a precise linear combination of the other columns. Yet this constraint is not typically included among the standard statement of assumptions.]. Instead, these assumptions are required to make the output of a linear regression *meaningful* and, more specifically, for conducting correct inference.\n\nSimilarly, there are no limitations on mechanically computing an average\n\n\n### Averaging skewed data\n\nArithmetic average versus colloquial meaning of average as \"typical\" \n\nSkewed data\n\nMultimodal data / mixture models\n\n### No \"average\" observation\n\nIn the previous section, the average represented a point in the relevant data *range* even if it was not perhaps the one most representative of a \"typical\" observation. \nWe discussed how in some situations this quantity may be a reasonable answer to certain types of questions and an aid for certain types of decisions. \n\nHowever, when we seek an average *profile* over multiple variables, the problems of averages are further compounded. \nWe may end up with a set of \"average\" summary statistics that are not representative of any part of our population.\n\nTo see this, let's assume we are working with data for a company with a subscription business model. \nWe might be interested in profiling the age of each account (how long they have been a subscriber) and their activity (measured by amount spent on an e-commerce platform, files downloaded on a streaming service, etc.)\n\nThe following code simulates a set of observations: \n80% of accounts are between 0 to 3 years in age and have an average activity level of 100 while 20% of accounts are older than 3 years in age and have an average activity level of 500.\n(Don't over-think the specific probability distributions lived here. \nWe are concerned with interrogating the properties of the average and not with simulating a realistic data generating process. \nGiving ourselves permission to be wrong or \"lazy\" about unimportant things gives us more energy to focus on what matters.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\n# define simulation parameters ----\n## n: total observations\n## p: proportion of observations in group 1\nn <- 5000\np <- 0.8\nn1 <- n*p\nn2 <- n*(1-p)\n\n# generate fake dataset with two groups ----\ndf <- \n  data.frame(\n    age = c(runif(n1,   0,  3), runif(n2,   3, 10)),\n    act = c(rnorm(n1, 100, 10), rnorm(n2, 500, 10))\n  )\n```\n:::\n\n\n@fig-multivar-avg shows a scatterplot of the relationship between account age (x-axis) and activity level (y-axis).\nMeanwhile, the marginal rug plots shows the univariate distribution of each variable. \nThe sole red dot denotes the coordinates of the average age and average activity. \nNotably, this dot exists in a region of \"zero density\";\nthat is, it is not representative of *any* customer.\nStrategic decisions made with this sort of observation in mind as the \"typical\" might not be destined for success.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![A scatterplot of two variables and their averages](egre-aggr_files/figure-html/fig-multivar-avg-1.png){#fig-multivar-avg width=672}\n:::\n:::\n\n\n### The product of averages \n\nAs the above example shows, averages of multivariate data can produce poor summaries -- particularly when these variables are interrelated^[We intentionally avoid the word *correlated* here to emphasize the fact that *correlation* refers more strictly to linear relationships].\n\nA second implication of this observation is that deriving additional computations based on pre-averaged numbers is likely to obtain inaccurate results. \n\nFor example, consider that we wish to estimate the average dollar amount of returns per any e-commerce order.\nOrders may generally be a mixture of low-price orders (around \\$50 on average) and high-price orders (around \\$250 on average). \nLow-price orders may have a 10% probability of being returned while high price orders have a 20% probability.\n(Again, are these numbers, distributions, or relationships hyper-realistic? \nNot at all. \nHowever, once again we are telling ourselves a story just to reason about numerical properties, so we have to give ourselves permission to not focus on irrelevant details.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\n# define simulation parameters ----\n## n: observations per group\n## pr[1|2]: mean price per group\nn <- 100\npr1 <- 50\npr2 <- 250\npr_sd <- 5\nre1 <- 0.1\nre2 <- 0.2\n\n# simulate spend amounts and return indicators ----\namt_spend  <- c(rnorm(n, pr1, pr_sd), rnorm(n, pr2, pr_sd))\nind_return <- c(rbinom(n, 1, re1),    rbinom(n, 1, re2))\n\n# compute summary statistics ----\naverage_of_product <- mean(amt_spend * ind_return)\nproduct_of_average <- mean(amt_spend) * mean(ind_return)\n```\n:::\n\n\nThe *true* average amount returned across all of our orders is 36.0438333 (from the `average_of_product` variable). \nHowever, if instead we already knew an average spend amount and an average return proportion, we might be inclined to compute the `product_of_average` method which returns a value of 26.9922866. \n(This is a difference of 9.05 relative to an average purchase amount of 150.)\n\nAt first, this may seem unintuitive until we write out the formulas and realize that these metrics are, in fact, two very different quantities:\n\n$$\n  \\frac{ \\sum_{1}^{n} Spend }{\\sum_{1}^{n} 1} \\ * \\frac{ \\sum_{1}^{n} I(Return) }{\\sum_{1}^{n} 1}\n$$ over all $n$ orders\n\nversus\n\n$$\n  \\frac{\\sum_{1}^{n} Spend * I(Return)}{\\sum_{1}^{n} 1}\n$$\n\nIf this still feels counterintuitive, we can see how much of the difference is accounted for by the interrelation between our two variables. \nIn the following code, we break the relationship between the variables by randomly reordering the `ind_return` variable so it is no longer has any true relationship to the corresponding `amt_spend` variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# randomly reorder one of two variables to break relationships ----\nind_return <- sample(ind_return, size = 200)\n\n# recompute variables ----\naverage_of_product <- mean(amt_spend * ind_return)\nproduct_of_average <- mean(amt_spend) * mean(ind_return)\n```\n:::\n\n\nAfter redoing the calculations, we find that th two values are much closer. \n`average_of_product` is now 24.1041313 and `product_of_average` is now 26.9922866.\nThese are notably still not the same number so that does not mean that these two equations are equivalent if variables are unrelated; \nhowever, this second result once again illustrates the extent to which interrelations can defy our naive intuitions.\n\n### Average over what? (TODO)\n\nno such thing as an unweighted average (just sometimes weights are equal)\n\nformal definition of expected value forces you to pick a probability distribution\n\neg avg mpg by time vs by mileage?\n\nnot strictly an error but our language allows an ill-defined problem\n\n### Dichotomization and distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000\n\n# simulate x and y: uniformly distributed x ----\nx1 <- runif(n)\ny1 <- 5 + 3*x1 + rnorm(n)\n\n# simulate x and y: same relationship, more concentrated distribution of x ----\nx2 <- c( runif(n*0.1, 0.00, 0.44), \n         runif(n*0.8, 0.45, 0.55), \n         runif(n*0.1, 0.55, 1.00) \n        )\ny2 <- 5 + 3*x2 + rnorm(n)\n\n# com\ng1 <- ifelse(x1 < 0.5, 0, 1)\nmeans1 <- c(mean(y1[g1 == 0]), mean(y1[g1 == 1]))\nmeans1 \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.813698 7.254438\n```\n\n\n:::\n\n```{.r .cell-code}\ng2 <- ifelse(x2 < 0.5, 0, 1)\nmeans2 <- c(mean(y2[g2 == 0]), mean(y2[g2 == 1]))\nmeans2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.218347 6.762943\n```\n\n\n:::\n\n```{.r .cell-code}\nmeans1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.813698 7.254438\n```\n\n\n:::\n\n```{.r .cell-code}\nmeans2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.218347 6.762943\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(x1, y1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6424276\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(x2, y2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3687667\n```\n\n\n:::\n:::\n\n\n### Small sample sizes\n\n\n## Proportions (WIP)\n\nnote that these are of course just a type of average (average of indicators) but helpful to examine challenges separately\n\n### Picking the right denominator\n\n### Sample size effects\n\n## Variation (TODO)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(-20, -10, -5, 0, 5, 10, 20)\nmean(x)\nvar(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n[1] 175\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(-15, -15, -5, 0, 5, 15, 15)\nmean(x)\nvar(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n[1] 158.3333\n```\n\n\n:::\n:::\n\n\n## Correlation (WIP)\n\nAs shown in @fig-fun-corr, \n\n\n::: {.cell}\n::: {.cell-output-display}\n![Plots of x from 1 to 10 over a range of common functions](egre-aggr_files/figure-html/fig-fun-corr-1.png){#fig-fun-corr width=672}\n:::\n:::\n\n\n### Linear relationships only\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1:10\ny <- list(linear = x, quadratic = x**2, cubic = x**3, quartic = x**4)\n\nvapply(y, FUN = function(z) cor(x, z, method = \"pearson\"), FUN.VALUE = numeric(1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   linear quadratic     cubic   quartic \n1.0000000 0.9745586 0.9283912 0.8816779 \n```\n\n\n:::\n:::\n\n\n### Multiple forms\n\nTraditional (Pearson) correlation depends on specific values whereas Spearman and Kendall focus on order statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# polynomials ----\nx <- 1:10\ny <- list(linear = x, quadratic = x**2, cubic = x**3, quartic = x**4)\n\nvapply(y, FUN = function(z) cor(x, z, method = \"pearson\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"spearman\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"kendall\"), FUN.VALUE = numeric(1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   linear quadratic     cubic   quartic \n1.0000000 0.9745586 0.9283912 0.8816779 \n   linear quadratic     cubic   quartic \n        1         1         1         1 \n   linear quadratic     cubic   quartic \n        1         1         1         1 \n```\n\n\n:::\n:::\n\n\nSimilar results with a different set of functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# other functional forms ----\nx <- 1:10\ny <- list(sin(x), sqrt(x), exp(x), log(x))\n\nvapply(y, FUN = function(z) cor(x, z, method = \"pearson\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"spearman\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"kendall\"), FUN.VALUE = numeric(1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.1705046  0.9891838  0.7168704  0.9516624\n[1] -0.1393939  1.0000000  1.0000000  1.0000000\n[1] -0.1111111  1.0000000  1.0000000  1.0000000\n```\n\n\n:::\n:::\n\n\n### Sensitivity to domain\n\nThe \"strength of relationship\" (completely deterministic) is the same in both cases    \nHowever, the summarization of the relationships changes\n\nHere's same case as before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# polynomials ----\nx <- 1:10\ny <- list(linear = x, quadratic = x**2, cubic = x**3, quartic = x**4)\n\nvapply(y, FUN = function(z) cor(x, z, method = \"pearson\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"spearman\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"kendall\"), FUN.VALUE = numeric(1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   linear quadratic     cubic   quartic \n1.0000000 0.9745586 0.9283912 0.8816779 \n   linear quadratic     cubic   quartic \n        1         1         1         1 \n   linear quadratic     cubic   quartic \n        1         1         1         1 \n```\n\n\n:::\n:::\n\n\nAnd here's a different range:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# polynomials, diff range ----\nx <- -10:10\ny <- list(linear = x, quadratic = x**2, cubic = x**3, quartic = x**4)\n\nvapply(y, FUN = function(z) cor(x, z, method = \"pearson\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"spearman\"), FUN.VALUE = numeric(1))\nvapply(y, FUN = function(z) cor(x, z, method = \"kendall\"), FUN.VALUE = numeric(1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   linear quadratic     cubic   quartic \n1.0000000 0.0000000 0.9179069 0.0000000 \n   linear quadratic     cubic   quartic \n        1         0         1         0 \n   linear quadratic     cubic   quartic \n        1         0         1         0 \n```\n\n\n:::\n:::\n\n\n### Partial correlation\n\nA lot of EDA starts with some sort of correlation matrix    \nThis won't always account for the fact that some variables can mask correlation between others\n\nConsider two groups with trends in different directions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_obsvs <- 10\nn_group <-  2\n\ngroup <- rep(1:n_group, each = n_obsvs)\nvar1  <- rep(1:n_obsvs, times = n_group)\nvar2  <- var1 * rep(c(5, -5), each = n_obsvs)\nvar3  <- var1 * rep(c(1,  5), each = n_obsvs)\n```\n:::\n\n\nAs @fig-cor-0 shows\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Subgroups demonstrating opposing linear relationships](egre-aggr_files/figure-html/fig-cor-0-1.png){#fig-cor-0 width=672}\n:::\n:::\n\n\nBecause of the opposing trends, their correlation becomes zero\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(var1, var2, method = \"pearson\")\ncor(var1, var2, method = \"spearman\")\ncor(var1, var2, method = \"kendall\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n[1] 0\n[1] 0\n```\n\n\n:::\n:::\n\nHowever, by group the correlation is 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(var1[group == 1], var2[group == 1])\ncor(var1[group == 2], var2[group == 2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n[1] -1\n```\n\n\n:::\n:::\n\n\nA similar thing happens when the relationship has the same sign but different slopes\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(var1, var3, method = \"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5703518\n```\n\n\n:::\n:::\n\n\nwhile the correlation is still one within group\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(var1[group == 1], var3[group == 1])\ncor(var1[group == 2], var3[group == 2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n[1] 1\n```\n\n\n:::\n:::\n\n\n\nEven partial correlation doesn't help in case of opposing signs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ppcor)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ppcor' was built under R version 4.0.5\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: MASS\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'MASS'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:patchwork':\n\n    area\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n\n\n:::\n\n```{.r .cell-code}\npcor(data.frame(var1, var2, group))$estimate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      var1       var2      group\nvar1     1  0.0000000  0.0000000\nvar2     0  1.0000000 -0.8864053\ngroup    0 -0.8864053  1.0000000\n```\n\n\n:::\n:::\n\n\nIt *improves* the strength of the estimated correlation in the case of `var3` but still failes to estimate it correctly. \nPartial correlation would be assuming a form like `var3 ~ var1 + group` and not `var3 ~ var1 * group`\n\n\n::: {.cell}\n\n```{.r .cell-code}\npcor(data.frame(var1, var3, group))$estimate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            var1      var3      group\nvar1   1.0000000 0.8320503 -0.7375338\nvar3   0.8320503 1.0000000  0.8864053\ngroup -0.7375338 0.8864053  1.0000000\n```\n\n\n:::\n:::\n\n\n## Trends\n\n### \"If trends continue...\"\n\n@fig-if-trends-continue shows that...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nn <- 300\nx <- runif(n)\ny <- x + rnorm(n)\n\nggplot(\n  data.frame(x, y),\n  aes(x, y)\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ I(x**1), se = FALSE, fullrange = TRUE) +\n  geom_smooth(method = \"lm\", formula = y ~ I(x**2), se = FALSE, fullrange = TRUE) +\n  geom_smooth(method = \"lm\", formula = y ~ I(x**3), se = FALSE, fullrange = TRUE) +\n  scale_x_continuous(limit = c(0,2))\n```\n\n::: {.cell-output-display}\n![Extrapolated linear, quadratic, and cubic fits of data](egre-aggr_files/figure-html/fig-if-trends-continue-1.png){#fig-if-trends-continue width=672}\n:::\n:::\n\n\n### Seasonality \n\n\n::: {.cell}\n\n```{.r .cell-code}\nseas_factor <- c(0.7, 0.8, 0.8,\n                 0.9, 1.0, 1.1,\n                 1.1, 1.1, 1.2,\n                 1.2, 1.5, 1.8\n                 )\nbase <- 1000\nn_visits <- base * seas_factor\n\nmean(df$n_visits[1:12])\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in mean.default(df$n_visits[1:12]): argument is not numeric or logical:\nreturning NA\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(df$n_visits[8:12])\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in mean.default(df$n_visits[8:12]): argument is not numeric or logical:\nreturning NA\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(df$n_visits[c(8:12, 1:12)])\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in mean.default(df$n_visits[c(8:12, 1:12)]): argument is not numeric or\nlogical: returning NA\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] NA\n[1] NA\n[1] NA\n```\n\n\n:::\n:::\n\n\n\n\n## Comparisons (TODO)\n\n### Percents versus percentage points\n\n\n::: {.cell}\n\n:::\n\n\n### Changes with small bases\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(0.015 - 0.010) / 0.010\n(0.805 - 0.800) / 0.800\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5\n[1] 0.00625\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n0.015 / 0.010\n0.805 / 0.800\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.5\n[1] 1.00625\n```\n\n\n:::\n:::\n\n\n## Segmentation (TODO)\n\ntransient segmentation means can't track trends within\n\n## Outliers (TODO)\n\n## Strategies (TODO)\n\n## Real World Disasters (TODO)\n\nStraight vs weighted averages in COVID positivity rates [@indy_rates]\n\n> The changes could result in real-world differences for Hoosiers, because the state uses a county's positivity rate as one of the numbers to determine which restrictions that county will face. Those restrictions determine how many people may gather, among other items.\n\n> Some Hoosiers may see loosened restrictions because of the changes. While Box said the county-level impact will be mixed, she predicted some smaller counties will see a decline in positivity rate after the changes.\n\n> \"The change to the methodology is how we calculate the seven-day positivity rate for counties. In the past, similar to many states, we've added each day's positivity rate for seven days and divided by seven to obtain the week's positivity rate. Now we will add all of the positive tests for the week and divide by the total tests done that week to determine the week's positivity rate. This will help to minimize the effect that a high variability in the number of tests done each day can have on the week's overall positivity, especially for our smaller counties.\"\n\nthree issues here\n\nfirst straight versus weighted averages\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_of_ratios <- (10/100 + 90/100) / 2\n\nratio_of_sums <- (10 + 90) / (100 + 100)\n\navg_of_ratios == ratio_of_sums\n\navg_of_ratios_uneq <- (10/100 + 180 / 200) / 2\n\nratio_of_sums_uneq <- (10 + 180) / (100 + 200)\n\navg_of_ratios_uneq == ratio_of_sums_uneq\n\nweightavg_of_ratios_uneq <- (100/300)*(10/100) + (200/300)*(180/200)\n\nratio_of_sums_uneq == weightavg_of_ratios_uneq\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n[1] FALSE\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nthen back to the data for why it matters. \n\nif data is from same distribution, this could increase variance but shouldn't effect mean\n\nRecall that the standard deviation of sample proportion is $\\sqrt(p*(1-p)/n)$\n\nlink to discussions of sample size and different types of averages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\n# define simulation parameters ----\n## n: total draws from binomial distribution\n## p: proportion of successes\np <- 0.5\nn <- 1000\n\n# sample from binomials of different size ----\ns010 <- rbinom(n,  10, p) /  10\ns100 <- rbinom(n, 100, p) / 100\ns500 <- rbinom(n, 500, p) / 500\n\n# set results as dataframe for inspection ----\ndf <- data.frame(\n  s = rep(c(10, 100, 500), each = n),\n  x = c(s010, s100, s500)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nggplot(data = df) +\n  aes(x = x, col = as.character(s)) +\n  geom_density() +\n  geom_vline(xintercept = p, col = 'darkgrey', linetype = 2) +\n  labs(\n    title = \"Sampling Distribution for p = 0.5\",\n    col = \"Sample Size\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = \"bottom\"\n  )\n```\n\n::: {.cell-output-display}\n![](egre-aggr_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n\nbut low sample days based on real world are probably also a sign of a different distribution (only very urgent cases get tested?) \n\n\n\n\n<!--\n\n## Trends\n\n### If trends continue\n\nData analysis undoubtedly relies on assumptions, and the quality of the resulting analysis depends in a large part on how reasonable these assumptions are. However, our heuristics for assessing assumptions are sometimes lacking. Assumptions that are lengthy, complex, or jargon-filled may be perceived as less conservative than those simple and straightforward. Yet this is a bad measure for fidelity. One such example of a simple but poor assumption is operating under the premise of \"if the current trends continue\". \n\nFirst, in the real world, this is rarely (if ever) the case. Complex, dynamic systems have feedback loops and constraints, and few natural systems^[whether they be the predator and prey of the Lotka-Volterra differential equations, the infected population of an epidemiolgical SIR model, or pricing dynamics of supply and demand] and trend unabated in the same direction. \n\nSecond, and perhaps event more critically, there is no singular, objective definiton of what the current \"trend\" of a dataset event is. To illustrate this @fig-lin-quad-cub shows the fits of linear, quadratic, and cubic regressions on a set of data and its out-of-sample extrapolation. Even if we *believed* the premise that \"trends will continue\", the subjective determination of that thrend has massive implications on the resulting conclusions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nn <- 300\nx <- runif(n)\ny <- c(x[1:100], x[101:200]**2, x[201:300]**3) + rnorm(n)\n\nggplot(\n  data.frame(x = x, y = y),\n  aes(x, y)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ I(x**1), se = FALSE, fullrange = TRUE) +\n  geom_smooth(method = \"lm\", formula = y ~ I(x**2), se = FALSE, fullrange = TRUE) +\n  geom_smooth(method = \"lm\", formula = y ~ I(x**3), se = FALSE, fullrange = TRUE) +\n  geom_smooth(method = \"lm\", formula = y ~ I(x**4), se = FALSE, fullrange = TRUE) +\n  scale_x_continuous(limits = c(0,2)) \n```\n\n::: {.cell-output-display}\n![Plot of data extrapolated from linear, quadratic, and cubic fits](egre-aggr_files/figure-html/fig-lin-quad-cub-1.png){#fig-lin-quad-cub width=672}\n:::\n:::\n\n\n\n<!--\n\n## Aggregating without visualizing\n\nThe **datasauRus** R package [@R-datasauRus] \\index{R package!datasauRus}\n\n## Believing in the \"average\" observation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rnorm(1000, 0, 5)\ny1 <- rnorm(1000, 10, 6)\ny2 <- 10 + x + rnorm(1000, 0, 1)\ny3 <- 10 + x + rlnorm(1000, 0, 1)\ncor(x,y1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0318291\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(x,y2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9801136\n```\n\n\n:::\n\n```{.r .cell-code}\n(mx <- mean(x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.04878271\n```\n\n\n:::\n\n```{.r .cell-code}\n(my1 <- mean(y1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.94201\n```\n\n\n:::\n\n```{.r .cell-code}\n(my2 <- mean(y2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.960487\n```\n\n\n:::\n\n```{.r .cell-code}\n(my3 <- mean(y3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 11.5651\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\ndf <- data.frame(x=x,y=y1,y2=y2)\ngg1 <- ggplot(df, aes(x,y1)) + geom_bin2d() + geom_point(aes(mx,my1), col = 'red')\ngg2 <- ggplot(df, aes(x,y2)) + geom_bin2d() + geom_point(aes(mx,my2), col = 'red')\ngg3 <- ggplot(df, aes(x,y3)) + geom_bin2d() + geom_point(aes(mx,my3), col = 'red')\n\nx <- c(runif(700, 20, 50), runif(300, 50, 70))\ny <- x**3 + rnorm(1000)\ndf <- data.frame(x=x,y=y)\ngg <- ggplot(df, aes(x,y)) + geom_bin2d() + geom_point(aes(mean(x), mean(y)), col = 'red')\n```\n:::\n\n\n\n\n## Product of averages\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>%\n  summarize(\n    N_TRANS = n() / n_distinct(ID_ACCT),\n    AMT_SPEND = sum(AMT_SPEND) / n()\n  ) %>%\n  mutate(N_TRANS * AMT_SPEND)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  N_TRANS AMT_SPEND N_TRANS * AMT_SPEND\n1    10.8  347.7623            3755.833\n```\n\n\n:::\n\n```{.r .cell-code}\ndata %>%\n  group_by(ID_ACCT) %>%\n  summarize(N_TRANS = n(), AMT_SPEND = sum(AMT_SPEND) / n()) %>%\n  summarize_at(vars(N_TRANS, AMT_SPEND), mean) %>%\n  mutate(N_TRANS * AMT_SPEND)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 3\n  N_TRANS AMT_SPEND `N_TRANS * AMT_SPEND`\n    <dbl>     <dbl>                 <dbl>\n1    10.8      341.                 3686.\n```\n\n\n:::\n\n```{.r .cell-code}\nsummarize(data, sum(AMT_SPEND) / n_distinct(ID_ACCT))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  sum(AMT_SPEND)/n_distinct(ID_ACCT)\n1                           3755.833\n```\n\n\n:::\n:::\n\n\n$$\n  \\frac{ \\sum_{1}^{n} Spend }{\\sum_{1}^{n} 1} \\ * \\frac{ \\sum_{1}^{n} Trips }{\\sum_{1}^{n} 1}\n$$ over all $n$ customers\n\n$$\n  \\sum_{1}^{n} Spend \\ * \\sum_{1}^{n}\n$$ trips\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\namt_spend <- c(rnorm(10, 50, 5), rnorm(10, 250, 5))\nind_return <- c(rbinom(10,1,0.1), rbinom(10,1,0.2))\nmean(amt_spend) * mean(ind_return)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.535406\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(amt_spend * ind_return)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12.67534\n```\n\n\n:::\n:::\n\n\n\n\n## Understanding the denominator\n\n## Small sample sizes\n\n## Relying on the wrong summary metrics\n\nThe paper \"A Dirty Dozen: Twelve Common Metric Interpretation Pitfalls in Online Controlled Experiments\" [@dmitriev_gupta_kim_vaz_2017]\n\nArticle \"Designing and evaluating metrics\" [@taylor_2020]\n\nProblem with Metrics - https://arxiv.org/abs/2002.08512\n\n## Dichotomization\n\n## Ignoring trend\n\n## Ignoring seasonality\n\n## Ignoring panel structure\n\n## Correlation\n\n## Strategies\n\n-->\n",
    "supporting": [
      "egre-aggr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}