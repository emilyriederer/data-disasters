{
  "hash": "c6882b8c1bd506a5496c15e0a40cdee7",
  "result": {
    "engine": "knitr",
    "markdown": "# Incredible Inferences (TODO) {#sec-incr-infe}\n\nBMJ statistician sent to me: https://www.bmj.com/content/379/bmj-2022-072883 \n\nPreviously, we have seen how different inputs like data, tools, and methods can add risks to our data analysis. However, the battle is not won simply when we get our first set of *outputs*. In this chapter, we will explore common errors in interpreting the results of our analysis by exploring aspects of bias, missingness, and confounding. \n\n## Common Biases\n\n## Policy-induced relationships\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\nn <- 1000\nx1 <- runif(n)\nx2 <- runif(n)\ny <- x1 + x2 > 1\ndf <- data.frame(x1, x2, y)\n\nwith(df, cor(x1, x2))\nwith(df[df$y,], cor(x1, x2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.05927849\n[1] -0.5003105\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nggplot(df) +\n  aes(x = x1, y = x2, col = y) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](incr-infe_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n## Feature leakage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000\nminutes_month1 <- runif(n, 60, 1200)\nminutes_month2 <- runif(n, 60, 1200) \nminutes_tot <- minutes_month1 + minutes_month2\ndf <- data.frame(minutes_month1, minutes_month2, minutes_tot)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n@fig-sep-sum shows...\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Correlation of independent versus cumulative quantities](incr-infe_files/figure-html/fig-sep-sum-1.png){#fig-sep-sum fig-align='center' width=90%}\n:::\n:::\n\n\n## \"Diligent\" data dredging\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\nn <- 1000\nx <- rnorm(n)\n\nrandom_test <- function(x) {\n  \n  indices <- sample(1:length(x), length(x)/2, replace = FALSE)\n  group1 <- x[indices]\n  group2 <- x[-indices]\n  tt <- t.test(group1, group2)\n  return(tt$p.value)\n  \n}\n\np <- vapply(1:10000, FUN = function(...) {random_test(x)}, FUN.VALUE = numeric(1))\nsum(p < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 500\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nn_obsv <- 1000\nn_vars <- 100\nmat_cat <- matrix(\n  data = rbinom(n_obsv * n_vars, 1, 0.5),\n  nrow = n_obsv,\n  ncol = n_vars\n  )\nmat_all <- cbind(x, mat_cat)\ndf <- as.data.frame(mat_all)\nnames(df) <- c(\"x\", paste0(\"v\", 1:n_vars))\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            x v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 v11 v12 v13 v14 v15 v16 v17 v18\n1 -0.56047565  1  1  0  1  1  0  1  0  0   1   1   0   1   1   0   1   0   0\n2 -0.23017749  1  1  0  0  0  0  0  1  1   1   1   0   0   1   1   1   1   1\n3  1.55870831  1  0  0  0  1  0  1  1  0   1   0   0   0   1   1   1   1   1\n4  0.07050839  1  0  1  1  0  0  1  0  1   1   1   0   0   0   0   0   1   0\n5  0.12928774  1  1  1  0  0  0  0  0  0   1   1   1   1   0   1   1   0   0\n6  1.71506499  1  0  0  1  1  0  1  1  1   1   1   0   1   1   0   0   0   0\n  v19 v20 v21 v22 v23 v24 v25 v26 v27 v28 v29 v30 v31 v32 v33 v34 v35 v36 v37\n1   0   1   0   0   1   1   1   0   0   0   0   1   0   0   1   0   0   0   1\n2   0   0   1   0   1   0   1   0   1   0   0   1   1   0   1   0   0   0   1\n3   0   0   1   0   1   1   0   1   0   0   0   1   0   1   0   0   1   1   1\n4   0   0   1   1   0   1   1   0   0   0   0   1   1   1   1   0   0   1   0\n5   0   1   1   0   0   0   1   0   1   0   0   1   0   0   0   0   1   0   1\n6   1   1   0   0   0   1   1   0   0   1   1   0   1   1   0   0   0   1   1\n  v38 v39 v40 v41 v42 v43 v44 v45 v46 v47 v48 v49 v50 v51 v52 v53 v54 v55 v56\n1   1   1   0   0   0   1   1   1   0   1   0   0   1   1   0   0   0   0   1\n2   0   0   0   0   0   0   0   0   0   1   0   1   0   1   1   0   0   1   0\n3   1   0   0   0   1   0   1   1   0   0   1   0   1   1   1   1   1   1   0\n4   0   0   0   1   0   1   0   0   0   1   0   0   0   0   1   1   1   0   0\n5   0   1   0   0   1   0   1   0   1   1   1   1   0   1   0   0   0   1   1\n6   0   1   1   1   1   1   0   0   0   1   0   0   1   0   0   1   1   0   0\n  v57 v58 v59 v60 v61 v62 v63 v64 v65 v66 v67 v68 v69 v70 v71 v72 v73 v74 v75\n1   0   1   1   0   0   1   0   0   0   1   1   1   0   0   0   0   1   0   0\n2   1   0   1   1   0   1   0   1   1   1   1   0   1   1   0   1   1   1   1\n3   0   1   0   1   0   1   1   1   1   1   0   1   0   1   1   0   1   1   0\n4   0   0   1   0   1   1   1   1   1   0   1   0   0   0   1   0   1   0   0\n5   1   0   0   0   0   0   1   0   1   0   0   0   1   0   1   1   1   0   1\n6   1   0   1   1   0   1   1   0   1   0   1   0   1   0   0   0   1   1   0\n  v76 v77 v78 v79 v80 v81 v82 v83 v84 v85 v86 v87 v88 v89 v90 v91 v92 v93 v94\n1   1   0   0   1   0   1   0   0   0   0   1   1   1   0   0   1   1   1   0\n2   1   1   1   1   1   0   1   1   1   0   0   1   1   0   0   0   0   1   0\n3   0   0   0   1   0   0   1   0   0   0   0   1   0   1   0   0   1   0   0\n4   0   0   0   0   0   0   0   1   0   1   1   0   0   0   0   0   0   0   1\n5   0   0   0   1   1   1   0   0   1   1   0   0   1   1   0   1   0   1   1\n6   1   0   0   1   0   1   1   0   0   0   0   1   1   0   1   0   0   0   0\n  v95 v96 v97 v98 v99 v100\n1   0   0   1   1   0    0\n2   1   1   0   1   0    1\n3   0   1   0   1   0    1\n4   1   0   0   1   1    1\n5   1   1   1   0   1    0\n6   0   1   1   0   0    1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(x ~ v1, data = df)$p.value\nt.test(x ~ v2, data = df)$p.value\nt.test(x ~ v3, data = df)$p.value\nt.test(x ~ v4, data = df)$p.value\n# etc.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09770958\n[1] 0.8733535\n[1] 0.02182194\n[1] 0.1525164\n```\n\n\n:::\n:::\n\n\nSuccess! ..Or success?\n\nsample splitting with \"train\"\n\n(obviously a very ugly way to do this, but that's the point)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(x ~ v1, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v2, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v3, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v4, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v5, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v6, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v7, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v8, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v9, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v10, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v11, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v12, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v13, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v14, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v15, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v16, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v17, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v18, data = df[1:(n_obsv/2),])$p.value\nt.test(x ~ v19, data = df[1:(n_obsv/2),])$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6022476\n[1] 0.4946592\n[1] 0.1959636\n[1] 0.368161\n[1] 0.2115401\n[1] 0.7112932\n[1] 0.3126737\n[1] 0.8141703\n[1] 0.9032519\n[1] 0.8211763\n[1] 0.4415702\n[1] 0.2564228\n[1] 0.5292002\n[1] 0.1714863\n[1] 0.08549674\n[1] 0.2284842\n[1] 0.6277407\n[1] 0.01318489\n[1] 0.2556232\n```\n\n\n:::\n:::\n\n\nand \"test\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(x ~ v18, data = df[(n_obsv/2 + 1):n_obsv,])$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1691076\n```\n\n\n:::\n:::\n\n\n## Superficial stories\n\n### Regression to the mean\n\nsimulate truly independent spend amounts across two periods\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\nn  <- 1000\nmu <- 100\nsd <- 10\nspend1 <- rnorm(n, mu, sd)\nspend2 <- rnorm(n, mu, sd)\n\ndf <- data.frame(spend1, spend2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'dplyr' was built under R version 4.0.5\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\ndf %>% \n  group_by(spend1 > mu) %>%\n  summarize_at(vars(starts_with(\"spend\")), mean) %>%\n  mutate(pct_change = round((spend2 - spend1) / spend1, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 4\n  `spend1 > mu` spend1 spend2 pct_change\n  <lgl>          <dbl>  <dbl>      <dbl>\n1 FALSE           92.2   99.7      0.081\n2 TRUE           108.   101.      -0.063\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>%\n  mutate(spend1_bin = cut(spend1, 5)) %>%\n  group_by(spend1_bin) %>%\n  summarize_at(vars(starts_with(\"spend\")), mean) %>%\n  mutate(pct_change = round((spend2 - spend1) / spend1, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 4\n  spend1_bin spend1 spend2 pct_change\n  <fct>       <dbl>  <dbl>      <dbl>\n1 (71.8,84]    80.5   97.8      0.215\n2 (84,96.1]    91.1  100.       0.098\n3 (96.1,108]  102.   101.      -0.012\n4 (108,120]   113.   101.      -0.101\n5 (120,132]   124.   103.      -0.167\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>%\n  mutate(spend1_bin = cut(spend1, 5)) %>%\n  group_by(spend1_bin) %>%\n  summarize(corr = cor(spend1, spend2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 2\n  spend1_bin    corr\n  <fct>        <dbl>\n1 (71.8,84]   0.281 \n2 (84,96.1]  -0.0149\n3 (96.1,108]  0.0438\n4 (108,120]   0.101 \n5 (120,132]  -0.165 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(spend1 > spend2)\nmean(spend1 < spend2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.49\n[1] 0.51\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((spend1 > mu) * (spend1 > spend2)) / sum(spend1 > mu)\nsum((spend1 < mu) * (spend1 < spend2)) / sum(spend1 > mu)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7168317\n[1] 0.7267327\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nggplot(df) +\n  aes(x = spend1, y = spend2) + \n  geom_point()\n```\n\n::: {.cell-output-display}\n![](incr-infe_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n### Distribution shifts\n\n\n::: {.cell}\n\n:::\n\n\n@fig-shift shows that...\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Trends within and between customer behavioral groups](incr-infe_files/figure-html/fig-shift-1.png){#fig-shift width=672}\n:::\n:::\n\n\n@fig-shift-v2 shows that...\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1 row containing missing values (`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 3 rows containing missing values (`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Possible subgroup trends contributing to aggregate trend](incr-infe_files/figure-html/fig-shift-v2-1.png){#fig-shift-v2 width=672}\n:::\n:::\n\n\n\nThe code used to generate this mock dataset is shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhi_engagement <- 10\nlo_engagement <- 2\npr_engagement <- 0.85^(0:24)\navg_engagement <- 10*pr_engagement + 2*(1-pr_engagement)\n\ndf <- \n  data.frame(\n    t = 1:length(avg_engagement), \n    avg_engagement, \n    hi_engagement, \n    lo_engagement\n    )\n```\n:::\n\n\n\n\n## Tricky timing issues (WIP)\n\n### Censored data\n\nSuppose we are wondering how long our subscription customers will stay put    \nmean lifetime of customers in 24 and uses exponential distrib (see appendix on distribs)    \nwe are analyzing a cohort of customers 18 months after they first subscribed \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# time-to-event censored ----\nset.seed(123)\nn <- 1000\ncurr_time <- 18\nmean_lifetime <- 24\n\nlifetime <- rexp(n, rate = 1 / mean_lifetime)\nmean(lifetime)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 24.7195\n```\n\n\n:::\n:::\n\n\nBecause we are only 18 months in, we cannot observe the lifetimes of all customers     \nfor those that left before 18 months we have complete data    \nbut for those who left after 18 months we only know their lifetime exceeds 18 months.    \nThus, if we look at the mean only where we can observe it, it's biased towards lower lifetimes.\n(Recall that we know what the correct value is)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#> observed ----\nlifetime_observed <- lifetime\nlifetime_observed[lifetime > curr_time] <- NA\nmean(lifetime_observed, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.918621\n```\n\n\n:::\n:::\n\n\nOf course, we do know more than nothing (null) about the \"surviving customers\". \nWe know that their lifetime is *at least* as large as the current time. \nSo alternatively, we could use the current time in our calculations.\nThis makes for a slightly less biased estimate, but it is still wrong and guaranteed to underestimate the actual average.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#> max ----\nlifetime_max <- pmin(lifetime, curr_time)\nmean(lifetime_max)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12.87866\n```\n\n\n:::\n:::\n\n\nThis scenario illustrates the concept of **censored data**. @fig-censor illustrates the fundamental problem more clearly. \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![A sample of observations of customer lifetimes showing observed and censored data](incr-infe_files/figure-html/fig-censor-1.png){#fig-censor width=672}\n:::\n:::\n\n\nSo what can we do instead? \nA common approach is to examine *quantiles* (such as the median) which can make more full use of the data we have.\nSince we know that rank of our observations (that is, that the censored observations are all larger than the observed datapoints),\nwe can reliable calculate the p-th quantile so long as p percent of the data is not censored.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#> quantile ----\nsum(!is.na(lifetime_observed)) / n\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.508\n```\n\n\n:::\n\n```{.r .cell-code}\nlifetime_quantile <- lifetime_observed\nlifetime_observed[is.na(lifetime_observed)] <- 100*curr_time\nquantile(lifetime_observed, p = c(0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   50% \n17.548 \n```\n\n\n:::\n:::\n\n\n### Immortal time bias\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrollout_time <- 12\nused_feature <- (lifetime > rollout_time) * rbinom(n, size = 1, prob = 0.5)\naggregate(lifetime, by = list(used_feature), FUN = mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Group.1        x\n1       0 19.51319\n2       1 36.80991\n```\n\n\n:::\n:::\n\n\n## \n\n<!--\n\n## No law to use ALL the data\n\n## Ascribing characteristics at wrong granularity\n\necological fallacy\n\n(does this belong in causation chapter?)\n\n## Finding policy-induced relationships\n\nselection bias\n\n## Ignoring heterogeneity\n\n## \"If trends continue\"\n\n## Analyzing time-to-event data\n\nimmortal time bias\n\n## Answering the right question\n\nDon't let available tools dictate the questions of interest\n\n*The Cult of Statistical Significance* [@ziliak_mccloskey]\n\n\"Mindless Statistics\" [@GIGERENZER2004587]\n\n## Misguided Rigor\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nt <- t.test(rnorm(100), rnorm(100))\nprint(t)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  rnorm(100) and rnorm(100)\nt = 1.4886, df = 197.35, p-value = 0.1382\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.06428618  0.46019159\nsample estimates:\n  mean of x   mean of y \n 0.09040591 -0.10754680 \n```\n\n\n:::\n\n```{.r .cell-code}\nt$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1381836\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npvals <- vapply(1:1000, FUN = function(x) t.test(rnorm(100), rnorm(100))$p.value, FUN.VALUE = numeric(1)) \nalpha <- 0.05\nsum(pvals > (1-alpha/2) | pvals < alpha/2) / length(pvals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.051\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prop_sign <- function(n = 1000, alpha = 0.05) {\n\n  pvals <- vapply(1:n, FUN = function(x) t.test(rnorm(100), rnorm(100))$p.value, FUN.VALUE = numeric(1))\n  prop <- sum(pvals > (1-alpha/2) | pvals < alpha/2) / length(pvals)\n  return(prop)\n\n}\n```\n:::\n\n\n\nData dredging, p-hacking\n\n## Sample splitting\n\nThe **nullabor** [@R-nullabor] R package\n\n## Age Period Cohort\n\n## Strategies\n\n-->\n",
    "supporting": [
      "incr-infe_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}