{
  "hash": "4e2f439a43ccc445a18150a74291e6a9",
  "result": {
    "engine": "knitr",
    "markdown": "# Computational Quandaries (WIP) {#sec-comp-quan}\n\nAfter gaining confidence in one's data (or, at least, making peace with it), the next step in a data analysis is often to start cleaning and exploring that data with summary statistics, plots, and models. \nGenerally, this requires a computational tool like SQL, R, or python. \n\nThe process of computation itself can be fraught with challenges. \nComputational tools are extremely literal; they are excellent at doing *precisely what they were told to do* but not often what analysts might have *meant* or *wished* that they would do. \nAdditionally, the moment an analyst begins to use a tool, the conversation is no longer between them and the data; \nsuddenly, the mental model of how every single tool developer thought you might want to do analysis affects the tools' behaviors and the analysts' results.\n\nIn this chapter, we will explore common ways that tools may do something technically correct, reasonable, and as-intended but very much not what analysts may expect. \nAlong the way, we will see how computational methods interact with the data encoding choices we discussed in @sec-data-dall (Data Dalliances).\n\n\n\n\n\n## Preliminaries - Data Computation\n\nBefore we think about specific tools or failure modes, we can first consider the common types of operations that the analytical tools allow us to do with our data. \n\n### Single Table Operations\n\nGiven a single data table, we may wish to do operations (illustrated in @fig-filt-aggr-tran) such as:\n\n- **Filtering**: Extracting a subset of a dataset for analysis based on certain inclusion criteria for each record\n- **Aggregation**: Grouping our data table by one or more variables and condensing information across records with *aggregate functions* like counts, sums, and averages \n- **Transformation**: Create new columns or modifying existing columns to represent more complex or domain-specific context\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Illustration of basic single-table data wrangling operations](figures/comp-quan/filt-aggr-tran.png){#fig-filt-aggr-tran fig-align='center' width=90%}\n:::\n:::\n\n\n### Multiple Table Operations\n\nOften, we can get additional value in an analysis by combining multiple types of information from difference tables. \nWhen working with multiple tables, we may be interested in:\n\n- **Combining Row-wise**: Taking multiple tables with the same schemas (column names and data types) and creating a single table which contains the union (all records), intersection (only matching), or difference (only in one) of the records in the two tables\n- **Combining Column-wise**: Appending additional fields to existing records through joining (also known as merging) multiple tables \n\n### Mechanics\n\nAll of these operations rely on a few core computational tasks:\n\n- **Arithmetic**: Basic addition, subtraction, multiplication, and division to aggregate and transform data\n- **Mapping**: Similar to arithmetic, other one-to-one or many-to-one transformations of numerical or categorical data (such as binning into categories)\n- **Equality**: Comparing whether or not two values are equal is critical for data filtering, column-wise combination, and certain types of data transformation \n- **Casting**: Converting data types of different elements into a comparable format is necessary for row-wise combination and often a prerequisite to certain equality and arithmetic tasks\n\nWhile these operations may seem simple, their behavior within certain tools and when employed for certain data types may sometimes lead to unintuitive or misleading results. \n\n## Null Values\n\nIn @sec-data-dall (Data Dalliances), we discuss how null values may represent many different concepts and be encoded in multiple different ways. \nIn addition to those semantic challenges, various representations of null values may cause different computational problems.^[This problem is not isolated to data analysis tools. For an entertaining example, see the 2019 WIRED article \"How a 'NULL' License Plate Landed One Hacker in Ticket Hell\" [@barrett] which a real-world software system producing unintended and undesirable behavior when asked to deal with a word `'NULL'`.] \nIn this section, we will explore these potential failure modes. \n\n### Types of Null Values\n\nNot only can null values represent many different things (as explored in @sec-data-dall), they also may be represented in many different ways. Understanding how nulls are encoded in one's dataset is a critical prerequisite to attempting any of the computations described in the subsequent sections.\n\n#### Language representations \n\nDifferent programming languages each offer their own versions of null values -- and sometimes more than one. For example, the R language includes `NA`, typed `NA`s (e.g. `NA_integer_`, `NA_character_`), `NaN`, and `NULL`; meanwhile, core python has `None` and the `numpy` module provides a `nan`. \n\nThese different values carry different semantic and functional meanings. For example R's `NA` generally means \"the presence of an absence\" whereas `NULL` is \"the absence of a presence\". This is articulated more clearly if we examine the lengths of these objects and observe that `NA` has a length 1 whereas `NULL` has a length 0.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nis.finite(NA)\nis.infinite(NA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n[1] FALSE\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(NA)\nlength(NULL)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n[1] 0\n```\n\n\n:::\n:::\n\n\nAs further proof that these are not interchangeable, we may use the helper functions `is.na()` and `is.null()`. It's false that `NA` is `NULL` and essentially unable to be evaluated if `NULL` is `NA` because `NULL`s are truly nothing.^[You'll notice that the code chunk below contains four commands but only three boolean outputs. Why is that? `is.na(NULL)` returns `logical(0)`, a zero-length value. It's yet another form of missingness!]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nis.na(NA)\nis.null(NULL)\nis.na(NULL)\nis.null(NA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n[1] TRUE\nlogical(0)\n[1] FALSE\n```\n\n\n:::\n:::\n\n\nTo further complicate matters, we have `NaN` (\"not a number\"), along with `-Inf` and `Inf`, which generally arise when we attempt to abuse R's calculator. Somewhat charmingly, `Inf` and `-Inf` may be used in some rudimentary calculations where the limit is returned.^[From calculus, we know 1/Inf approaches 0, but Inf/Inf is undefined.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1/0\n0/0\n1/Inf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] Inf\n[1] NaN\n[1] 0\n```\n\n\n:::\n:::\n\n\n#### Sentinel value encoding\n\nBeyond these null types offered natively by different programming languages, there are also many different data management *conventions* for null values. Because null values can have many meanings, sometimes missing fields are encoded with \"out of range\" values which intend to suggest a type of missingness. \n\nFor example, the US Census Bureau's Medical Expenditure Panel Survey uses the following reserved codes to denote different types of missingness: (TODO: cite p10 https://www.meps.ahrq.gov/data_stats/download_data/pufs/h206a/h206adoc.pdf)\n\n```\n- -1 INAPPLICABLE Question was not asked due to skip pattern\n- -7 REFUSED Question was asked and respondent refused to answer question\n- -8 DK Question was asked and respondent did not know answer\n- -14 NOT YET TAKEN/USED Respondent answered that the medicine has not yet been used\n- -15 CANNOT BE COMPUTED Value cannot be derived from data\n```\n\nThis approach preserves a lot of relevant information while, at the same time, being readily apparent that these values are not valid when the data is manually inspect. Unfortunately, manually inspecting every data field is rarely possible, and such sentinel values may go undetected when looking at higher-level summaries.\n\nConsider a survey of a population of retired adults where age is coded as `999` if not provided. Below, we simulate 100,000 such observations that are uniformly distributed between the age of 65 and 95 (hence, have an expected value of 80). Next, we replace merely *half of a percent* with our \"null\" values of `999`. Taking the mean with these false values results in a mean of about 85. This number alone might not raise the alarm; after all, we know the dataset's population is older adults. However, accidentally treating these as valid values biases our results by a somewhat remarkable five years.\n \n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\nn <- 100000\np <- 0.01 / 2\nages <- runif(n, 65, 95)\n\nages_nulls <- ages\nages_nulls[1:(n*p)] <- 999\n\nmean(ages)\nmean(ages_nulls)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 79.97897\n[1] 84.57468\n```\n\n\n:::\n:::\n\n\nSo, the first order of business with null values is understanding how they are encoded and translating them to the most computationally appropriate form. However, that is only the beginning of the story.\n\n#### Other types of nulls (TODO)\n\nmight be blank string which won't get detected in standard null checks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- \"\"\nis.na(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\nsame thing is true in dataframes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\ndf <- data.frame(x = c(\"a\", \"b\", \"\", \"d\"))\nsummarize_all(df, ~sum(is.na(.)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  x\n1 0\n```\n\n\n:::\n:::\n\n\nin such cases, need to explicitly check for such alternative encodings like blank strings\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- \"\"\nis.na(x) | x == \"\"\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nof course, a blank string isn't the only choice    \ncould also have an empty string of any length\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- \"  \"\nis.na(x) | x == \"\"\nis.na(x) | trimws(x) == \"\"\nis.na(x) | nchar(trimws(x)) == 0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n[1] TRUE\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nwe'll see more about how fluid strings can be in the string section below\n\n### Aggregation\n\nOnce null values are coded as \"true\" nulls, how these nulls are handled in the simple aggregation of data varies both across different languages and across different functions within a language. \nTo better understand the problems this might cause, we will look at examples in R and SQL.\n\nTo explore aggregation, let's build a simple dataset. We will suppose that we are working with a subscription-based e-commerce service and that we are looking at a `spend` dataset with one record per customer and information about the amount they spent and returned in a given month:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspend <-\n  data.frame(\n    AMT_SPEND = c(10, 20, NA),\n    AMT_RETURN = rep(NA, 3)\n  )\n\nhead(spend)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  AMT_SPEND AMT_RETURN\n1        10         NA\n2        20         NA\n3        NA         NA\n```\n\n\n:::\n:::\n\n\n\n\nTo compute the average amount spent (`AMT_SPEND`) with the `dplyr` package, an analyst might first reasonably write the following `summarize()` statement. \nHowever, as we can see, due to the presence of null values within the `AMT_SPEND` column, the result of this aggregation is for the whole quantity of `AVG_SPEND` to be set to the value `NA`. \n\nA glance at the documentation for the `mean()` function^[This documentation can be obtained in R by typing `?mean` in the console.] reveals that the function has a parameter called `na.rm`.\nThis parameter's default value is `FALSE`, but, when it is set to `TRUE`, it removes null values from our dataset. \nAdding this argument to the previous statement allows us to reach a numerical answer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarize(spend, \n          AVG_SPEND = mean(AMT_SPEND),\n          AVG_SPEND_NARM = mean(AMT_SPEND, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  AVG_SPEND AVG_SPEND_NARM\n1        NA             15\n```\n\n\n:::\n:::\n\n\nHowever, is this the *right* numerical answer? \nRecall that what `na.rm = TRUE` does is *drop* the null values from the set of numbers being averaged. \nHowever, suppose the null values represent that no purchases were made for a given customer in a given month. \nThat is, zero dollars were spent. \nIn effect, we have removed all non-purchasers from the data being averaged. \n\nMore precisely, we have switched from taking the average\n\n$$\n  \\frac{ \\sum_{1}^{n} Spend }{\\sum_{1}^{n} 1}\n$$ over all $n$ customers\n\nto taking the average\n\n$$\n  \\frac{ \\sum_{Spend > 0} Spend }{\\sum_{Spend > 0} 1}\n$$ over only those customers with spend.\n\nAt face value, we could say that the code above is giving the incorrect answer; by dropping some low (zero) purchase amounts, the average amount spend per customer is inflated. \nA second perspective, which is someone more philosophically troubling, is that this tiny change to the code which fixed the *obvious* problem (returning a null value) has introduced a *non-obvious* problem by fundamentally changing the question that we are asking. \nBy dropping all accounts from our table who made no purchases, we are no longer answering \"What is the average amount spent by all of our customer?\" but rather \"What is the average amount spent by an actively engaged customer?\" \nIn the language of probability theory, we might say that we have then changed our estimand from the expected value of spend to expected value of spend conditional on spend being positive. \nThis technical quirk has significant analytical impact.\n\nTo answer the real question at hand, we have a couple of options. \nWe could manually `sum()` the amount spent with the option to drop nulls but then divide by the correct denominator (all observations -- not just those with spend) or we could explicitly recode null values in `AMT_SPEND` to zero before taking the average.^[Recoding can be done with a number of different general purpose functions like `ifelse` or `dplyr::case_when` in R. Different SQL varaints often offer different options for this purpose with functions such as `nvl()` or `zeroifnull()`. A common version across many language is `coalesce()` which takes the first non-null argument listed.]\nEither of these options lead to the correct conclusion of a lower average spend amount.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarize(\n    spend,\n    AVG_SPEND_MANUAL = sum(AMT_SPEND, na.rm = TRUE) / n(),\n    AVG_SPEND_RECODE = mean(coalesce(AMT_SPEND, 0))\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  AVG_SPEND_MANUAL AVG_SPEND_RECODE\n1               10               10\n```\n\n\n:::\n:::\n\n\nThis is all well and good if we could just accept that the behaviors above are simply how nulls work, but further complexity comes as we see that there is no industry standard across tools. \nFor example, as the SQL code below shows, SQL's `avg()` function behaves more like R's `mean()` *with* the `na.rm = TRUE` option set (whereas, you may recall that R's `mean()` behaves with `na.rm = FALSE` by default). \nThat is, the default behavior of SQL is to only operate on the valid and available values. \nThe result of this default may mean that it is less obvious when our dataset has null values.\nSQL, unlike R, does not ask for \"permission\" to drop out nulls;\ninstead, it unilaterally makes a decision how to handle these variables.\n\n\n::: {.cell output.var='null_col'}\n\n```{.sql .cell-code}\nSELECT \n  avg(amt_spend) as AVG_SPEND\nFROM spend\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  AVG_SPEND\n1        15\n```\n\n\n:::\n:::\n\n\nHowever, this is not to suggest that null values cannot also be \"destructive\" in SQL (that is, returning null). \nWhile aggregation functions (which compute over the *rows/records*) like `sum()` and `avg()` drop nulls, operators like `+` and `-` (which compute *across columns/variables* in the *same row/record*) do not exhibit the same behavior. \nConsider, for example, if we wish to calculate the average net purchase amount (purchases minus returns) instead of the gross (total) purchase amount. \n\n\n::: {.cell output.var='null_cols_0'}\n\n```{.sql .cell-code}\nSELECT \n  avg(amt_spend - amt_return) as AVG_SPEND_NET\nFROM spend\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  AVG_SPEND_NET\n1            NA\n```\n\n\n:::\n:::\n\n\nDespite what we learned above about SQL's `avg()` function, the query above returns only a null value. \nWhat has happened? \nIn our `spend` dataset, the `amt_return` column is completely null (representing no return purchases). \nBecause the subtraction occurs before the average is taken, subtracting null values in the `amt_return` variable from valid numbers in `amt_spend` variable creates a new variable of all null values.\nThis new variable, which is already all null, is passed to the `avg()` function. \nThis process is shown step-by-step below.\n\n\n::: {.cell output.var='null_cols'}\n\n```{.sql .cell-code}\nSELECT\n  amt_spend, \n  amt_return, \n  amt_spend-amt_return \nFROM spend\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  AMT_SPEND AMT_RETURN amt_spend-amt_return\n1        10         NA                   NA\n2        20         NA                   NA\n3        NA         NA                   NA\n```\n\n\n:::\n:::\n\n\n### Comparison\n\nNull values don't just introduce complexity when doing arithmetic. \nDifficulties also arise any time multiple variables are assessed for equality or inequality. \nSince a null value is unknown, most programming languages generally will *not* consider nulls to be comparable with other nulls.\n\nWe can observe simple examples of this in both R and SQL. In neither language can a null value be assessed for equality or inequality versus either another number or another null. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nNA == 3 \nNA > 10 \nNA == NA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] NA\n[1] NA\n[1] NA\n```\n\n\n:::\n:::\n\n::: {.cell output.var='null_compare'}\n\n```{.sql .cell-code}\nSELECT\n  (NULL = 3) as NULL_EQ_NUM,\n  (NULL > 10) as NULL_GT_NUM,\n  (NULL = NULL) as NULL_EQ_NULL\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  NULL_EQ_NUM NULL_GT_NUM NULL_EQ_NULL\n1          NA          NA           NA\n```\n\n\n:::\n:::\n\n\nIn these toy examples, such outcomes may seem perfectly logical. \nHowever, this same reasoning can arise in sneakier ways and lead to uninteded results when equality evaluations are *implicit* in the task at hand instead of the singular focus. \nWe'll now see examples from data filtering, joining, and transformation.\n\n:::rmdnote\n\nIf you're familiar with SQL, you may have been surprised to notice that there is no `FROM` clause in the query above. In fact, SQL queries can treat values just like variables containing only a single record. \n\nWe will use this trick throughout the chapter for exploring how SQL works when we don't have an ideal sample dataset to test certain scenarios. Beyond exposition in this book, this trick is also useful in practice.\n\n:::\n\n#### Filtering\n\nSuppose we want to split our dataset into two datasets based on high or low values of spend. \nWe might assume the following two lines of code will create a clear partition (implying that each record would fall into exactly one group.) \n\n\n::: {.cell}\n\n```{.r .cell-code}\nspend_lt20 <- filter(spend, AMT_SPEND < 20)\nspend_gte20 <- filter(spend, AMT_SPEND >= 20)\n```\n:::\n\n\nHowever, if we examine the resulting datasets, we see that *neither* contains the third record which had a null value for the `AMT_SPEND` variable.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nspend_lt20\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  AMT_SPEND AMT_RETURN\n1        10         NA\n```\n\n\n:::\n\n```{.r .cell-code}\nspend_gte20\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  AMT_SPEND AMT_RETURN\n1        20         NA\n```\n\n\n:::\n:::\n\n\nThe same situation results in SQL.\n\n\n::: {.cell output.var='sql_spend_lt20'}\n\n```{.sql .cell-code}\nSELECT *\nFROM spend\nWHERE AMT_SPEND < 20\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  AMT_SPEND AMT_RETURN\n1        10         NA\n```\n\n\n:::\n:::\n\n::: {.cell output.var='sql_spend_gte20'}\n\n```{.sql .cell-code}\nSELECT *\nFROM spend\nWHERE AMT_SPEND >= 20\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  AMT_SPEND AMT_RETURN\n1        20         NA\n```\n\n\n:::\n:::\n\n\nThis is a direct result of the fact that nulls cannot be compared for equality any inequality. \nWe can think of data filtering as a two-step process: \nfirst evaluate whether the condition is TRUE or FALSE, \nthen return only the records for which the condition holds true. \nWhen we conduct the more manual process of filtering step-by-step, we see that the null value of `AMT_SPEND` does not get a \"truth value\" when compared with a number.\nThus, it is not contained in either \"truth value\" subset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmutate(spend, is_lt20 = (AMT_SPEND < 20))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  AMT_SPEND AMT_RETURN is_lt20\n1        10         NA    TRUE\n2        20         NA   FALSE\n3        NA         NA      NA\n```\n\n\n:::\n:::\n\n\nThus, whenever our data has null values, the very common act of data filtering risks excluding important information.\n\n#### Joining\n\nThe same phenomenon as described above also happens when joining multiple datasets. \n\nSuppose we have multiple datasets we wish to merge based on columns denoting a record's name and date of birthday. \nFor ease of exploration, we will make the simplest possible such dataset and simply try to merge it to itself. \n(This may seem silly, but often when trying to understand *computationally* complex things, it is a good idea to make the scenario as simple as possible.\nIn fact, this idea is core to the concept of computational unit tests which we will discuss at the end of this chapter.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbday <- data.frame(NAME = c('Anne', 'Bob'), BIRTHDAY = c('2000-01-01', NA))\nbday\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  NAME   BIRTHDAY\n1 Anne 2000-01-01\n2  Bob       <NA>\n```\n\n\n:::\n:::\n\n\n\n\nIn SQL, if we try to join this table, the records in row 1 will match because `'Anne' == 'Anne'` and `'2000-01-01' == '2000-01-01'`. \nHowever, poor Bob's record is eliminated because his birth date is logged as null, and `NA == NA` is false.\n\n\n::: {.cell output.var='bday_join'}\n\n```{.sql .cell-code}\nSELECT a.*\nFROM\n  bday as a\n  INNER JOIN\n  bday as b\n  ON\n  a.NAME = b.NAME and\n  a.BIRTHDAY = b.BIRTHDAY\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  NAME   BIRTHDAY\n1 Anne 2000-01-01\n```\n\n\n:::\n:::\n\n\nIn contrast, R's `dplyr::inner_join()` function will not do this by default. \nThis function lets us specifically control how nulls are matches with the `na_matches` argument, with a default option to match on `NA` values. \n(You may read more about the argument by typing `?dplyr::inner_join` in the R console to pull up the documentation.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_join(bday, bday, by = c('NAME', 'BIRTHDAY'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  NAME   BIRTHDAY\n1 Anne 2000-01-01\n2  Bob       <NA>\n```\n\n\n:::\n:::\n\n\nThis example then is not only a cautionary tale for how null values may unintentionally corrupt our data transformations but also how \"brittle\" our knowledge and intuition may be when moving between tools.\nNeither of these default behaviors is strictly better or worse, but they are definitely different and have real implications on our analysis.\n\n#### Transformation\n\nA common task in data analysis is to aggregate results by subgroup. \nFor example, we might want to summarize how many customers (rows/records) spent more or less than \\$10. To discern this, we might create a categorical variable for high versus low purchase amounts, group by this variable and count.\n\nThe psuedocode would read something like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>%\n  mutate(HIGH_LOW = << transform AMT_SPEND >>) %>%\n  group_by(HIGH_LOW) %>%\n  count()\n```\n:::\n\n\nTo define the `HIGH_LOW` variable, we might use a function like `ifelse()`, `dplyr::if_else()`, or `dplyr::case_when()`. \nHowever, once again, we have the issue of how values are *partitioned* when nulls are included. \nIf we recode any records with `AMT_SPEND` of less than or equal to 10 to \"Low\" and default the rest to \"High\", we will accidentally count all null values in the \"High\" group.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspend %>%\n  mutate(HIGH_LOW = case_when(\n    AMT_SPEND <= 10 ~ \"Low\", \n    TRUE ~ \"High\")\n    ) %>%\n  group_by(HIGH_LOW) %>% \n  count()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n# Groups:   HIGH_LOW [2]\n  HIGH_LOW     n\n  <chr>    <int>\n1 High         2\n2 Low          1\n```\n\n\n:::\n:::\n\n\nInstead, it is more accurate and transparent (unless we know specifically what null values mean and what group they should be part of) to not let one of our \"core\" categories by the \"default\" case in our logic. \nWe can explicitly encode any residual values as something like \"OTHER\" or \"ERROR\" to help us see that there is a problem requiring extra attention.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspend %>%\n  mutate(HIGH_LOW = case_when(\n    AMT_SPEND <= 10 ~ \"Low\", \n    AMT_SPEND > 10 ~ \"High\",\n    TRUE ~ \"OTHER\")\n    ) %>%\n  group_by(HIGH_LOW) %>% \n  count()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 2\n# Groups:   HIGH_LOW [3]\n  HIGH_LOW     n\n  <chr>    <int>\n1 High         1\n2 Low          1\n3 OTHER        1\n```\n\n\n:::\n:::\n\n\nWhile nulls contribute to this issue, it's important to realize that nulls are not the only factor causing this error nor or they the solution.\nThe more substantial issue is the careless use of defaults and *implicit* encoding versus *explicit* encoding. \nIn the second form of the SQL query above, we are more specific about exactly what is allowed in each category which ensures any unexpected inputs will not be allowed to \"sneak\" into ordinary outputs.\n\n## Logicals (TODO)\n\nMuch like the different versions of nulls that we met in the last section, logical data types use reserved keywords to represent `TRUE` and `FALSE`. \n(The exact formats of logical reserved keywords vary by language. R and SQL use `TRUE` and `FALSE` and python uses `TRUE` and `FALSE`.)\nThis means that these names function like a number or a letter which intrinsically hold one specific value and cannot take on a different value besides their own. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nTRUE = 5\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in TRUE = 5: invalid (do_set) left-hand side to assignment\n```\n\n\n:::\n\n```{.r .cell-code}\n2 = 5\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in 2 = 5: invalid (do_set) left-hand side to assignment\n```\n\n\n:::\n:::\n\n\nAcross languages, `TRUE` and `FALSE` are considered equivalent to the numeric representations of 1 and 0 respectively. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.numeric(TRUE)\nas.numeric(FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n[1] 0\n```\n\n\n:::\n:::\n\n\nA consequence of this numerical equivalency is that `TRUE` and `FALSE` may be compared to each other or other numbers and be included in mathematical expressions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTRUE > FALSE\nTRUE < 5\nFALSE > -1\nTRUE + 1\nTRUE * 5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] 2\n[1] 5\n```\n\n\n:::\n:::\n\nsimilar in SQL\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nselect\n  TRUE > FALSE as a,\n  TRUE < 5 as b,\n  FALSE > -1 as c,\n  TRUE + 1 as d,\n  TRUE * 5 as e\n```\n\n\n<div class=\"knitsql-table\">\n\n\nTable: 1 records\n\n|  a|  b|  c|  d|  e|\n|--:|--:|--:|--:|--:|\n|  1|  1|  1|  2|  5|\n\n</div>\n:::\n\n\n### Language-specific nuances (CUT?)\n\n#### Keyword abbrevations\n\nR also recognizes the abbreviations of `T` and `F` as `TRUE` and `FALSE` respectively; \nhowever this is not recommended. \n`T` and `F` are *not* reserved keywords, so they can be overwritten with a different value.\nThis makes code using such abbreviations \"brittle\" and less reliable.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (T) {\"Hello\"} else {\"Goodbye\"}\nif (F) {\"Hello\"} else {\"Goodbye\"}\nT = 0\nif (T) {\"Hello\"} else {\"Goodbye\"}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Hello\"\n[1] \"Goodbye\"\n[1] \"Goodbye\"\n```\n\n\n:::\n:::\n\n\n#### Alternative representations\n\nWe've previously seen that logicals have associated numerical values. \nDifferent languages may also treat their string representations differently. \n\nFor example, R believes that the string `\"TRUE\"` is equal to the logical value `TRUE` when directly compared.\nHowever, this relationships breaks the transitive property of mathematics^[If a = b and b = c then a = c] because `TRUE` equals both `1` and `\"TRUE\"`, yet `\"TRUE\"` does not equal `1` so the mathematical operations that can be done with logical `TRUE` cannot be done with string `\"TRUE\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTRUE == 1\nTRUE == \"TRUE\"\nTRUE == \"True\"\nTRUE * 5\n\"TRUE\" * 5\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in \"TRUE\" * 5: non-numeric argument to binary operator\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n[1] TRUE\n[1] FALSE\n[1] 5\n```\n\n\n:::\n:::\n\n\nIn contrast neither SQL not python honor the string forms of their respective logical reserved keywords at all.\n\n\n::: {.cell output.var='bool_math'}\n\n```{.sql .cell-code}\nselect \n  (TRUE == 1) as is_int_true,\n  (TRUE == 'TRUE') as is_char_true,\n  TRUE*5 as true_times_five,\n  'TRUE'*5 as true_str_time_five\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  is_int_true is_char_true true_times_five true_str_time_five\n1           1            0               5                  0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nTrue == 1\nTrue == \"True\"\nTrue == \"TRUE\"\nTrue * 5\n\"True\" * 5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\nFalse\nFalse\n5\n'TrueTrueTrueTrueTrue'\n```\n\n\n:::\n:::\n\n\n### Comparison (TODO)\n\nThe nuances of logical representation and handling seem straightforward in isolation.\nHowever, when encountered in real-world data problems, they are not isolated and are unlikely to be our main focus.\n\nImagine two datasets which all encode the same information but use boolean, string, and integer representations of a logical respectively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf1 <- data.frame(ID = 1:3, IND = rep(TRUE, 3), X = 1:3)\ndf2 <- data.frame(ID = 1:5, IND = rep('TRUE', 5), Y = 11:15)\ndf3 <- data.frame(ID = 1:5, IND = rep(1, 5), Z = 21:25)\n```\n:::\n\n\nBy simple inspection, the logical and string representations in particular look superficially similar and yet they will behave differently.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(df1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ID  IND X\n1  1 TRUE 1\n2  2 TRUE 2\n3  3 TRUE 3\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(df2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ID  IND  Y\n1  1 TRUE 11\n2  2 TRUE 12\n3  3 TRUE 13\n4  4 TRUE 14\n5  5 TRUE 15\n```\n\n\n:::\n:::\n\n\nIf we use R's `dplyr::filter()` or `base::subset()` function to subset the data, the value of `df1` will correctly subset based on the boolean values of `IND`. However, R will not know how to interpret the string version in `df2`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(df1, IND)\nfilter(df2, IND)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in `filter()`:\n! Problem while computing `..1 = IND`.\nx Input `..1` must be a logical vector, not a character.\n```\n\n\n:::\n\n```{.r .cell-code}\nsubset(df2, df2$IND)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in subset.data.frame(df2, df2$IND): 'subset' must be logical\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ID  IND X\n1  1 TRUE 1\n2  2 TRUE 2\n3  3 TRUE 3\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(df1, isTRUE(IND))\nfilter(df2, isTRUE(IND))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] ID  IND X  \n<0 rows> (or 0-length row.names)\n[1] ID  IND Y  \n<0 rows> (or 0-length row.names)\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(df2, isTRUE(IND))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] ID  IND Y  \n<0 rows> (or 0-length row.names)\n```\n\n\n:::\n\n```{.r .cell-code}\nleft_join(df1, df2, by = c(\"ID\", \"IND\"))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in `left_join()`:\n! Can't join on `x$IND` x `y$IND` because of incompatible types.\ni `x$IND` is of type <logical>>.\ni `y$IND` is of type <character>>.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmerge(df1, df2, by.x = c(\"ID\", \"IND\"), by.y = c(\"ID\", \"IND\"), all.x = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ID  IND X  Y\n1  1 TRUE 1 11\n2  2 TRUE 2 12\n3  3 TRUE 3 13\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.sql .cell-code}\nselect df1.*, df2.*\nfrom \n df1 \n left join\n df2\n on\n df1.id = df2.id and\n df1.ind = df2.ind\n```\n\n\n<div class=\"knitsql-table\">\n\n\nTable: 3 records\n\n|ID | IND|  X| ID|IND |  Y|\n|:--|---:|--:|--:|:---|--:|\n|1  |   1|  1| NA|NA  | NA|\n|2  |   1|  2| NA|NA  | NA|\n|3  |   1|  3| NA|NA  | NA|\n\n</div>\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd \n\ndata1 = {'ID': list(range(1, 4)),\n        'IND': [True] * 3,\n        'X': list(range(1,4))\n        }\ndata2 = {'ID': list(range(1, 6)),\n         'IND': ['True'] * 5,\n        'Y': list(range(11, 16))}\n\ndf1 = pd.DataFrame(data1)\ndf2 = pd.DataFrame(data2)\n\ndf1\ndf2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   ID   IND  X\n0   1  True  1\n1   2  True  2\n2   3  True  3\n   ID   IND   Y\n0   1  True  11\n1   2  True  12\n2   3  True  13\n3   4  True  14\n4   5  True  15\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npd.merge(left = df1, right = df2, how = \"left\", on = ['ID', 'IND'])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   ID   IND  X   Y\n0   1  True  1 NaN\n1   2  True  2 NaN\n2   3  True  3 NaN\n```\n\n\n:::\n:::\n\n\n### Logical Encoding (`pandas` case study)\n\nhttps://twitter.com/pandas_dev/status/1579886907286491137?s=20&t=GXV9OMXUcJiPWyES6CUKoA\n\n## Numbers (TODO)\n\n### Integer division\n\nIn R (mostly what we'd expect)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1/2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nifelse(3/4 > 0.5, 'High', 'Low')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"High\"\n```\n\n\n:::\n:::\n\n\nIn some SQL dialect SQL (SQLite shown. \"Modern\" interfaces like Snowflake and BigQuery don't do this)\n\n\n::: {.cell output.var='one_div_two'}\n\n```{.sql .cell-code}\nSELECT (1/2) as one_div_two\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  one_div_two\n1           0\n```\n\n\n:::\n:::\n\n\nThis rounding can get masked when we are recoding or doing subsequent calculations\n\n\n::: {.cell output.var='high_low'}\n\n```{.sql .cell-code}\nSELECT\n  case\n    when 3/4 > 0.5 then 'High' \n    else 'Low'\n  end as high_low_int,\n  case\n    when 3.0 / 4 > 0.5 then 'High'\n    else 'Low'\n  end as high_low_float\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  high_low_int high_low_float\n1          Low           High\n```\n\n\n:::\n:::\n\n\nThe above works because of implicit casting. We can also do explicit casting, but where we do this matters\n\n\n::: {.cell output.var='cast_when'}\n\n```{.sql .cell-code}\nSELECT\n  case \n    when cast(3 as float) / 4 > 0.5 then 'High'\n    else 'Low'\n  end as cast_first,\n  case\n    when cast(3/4 as float) > 0.5 then 'High'\n    else 'Low'\n  end as cast_last\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  cast_first cast_last\n1       High       Low\n```\n\n\n:::\n:::\n\n\n### Inexact storage and comparison\n\nIn R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(3 - 2.9) <= 0.1\n(2 - 1.9) <= 0.1\n(1 - 0.9) <= 0.1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n[1] FALSE\n[1] TRUE\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(3 - 2.9) == 0.1\n(2 - 1.9) == 0.1\n(1 - 0.9) == 0.1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n[1] FALSE\n[1] FALSE\n```\n\n\n:::\n:::\n\n\ndespite the fact that we see no difference\n\n\n::: {.cell}\n\n```{.r .cell-code}\n3 - 2.9\n2 - 1.9\n1 - 0.9\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1\n[1] 0.1\n[1] 0.1\n```\n\n\n:::\n:::\n\n\nIn python - same problem but in slightly different cases\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(3 - 2.9) <= 0.1\n(2 - 1.9) <= 0.1\n(1 - 0.9) <= 0.1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse\nFalse\nTrue\n```\n\n\n:::\n:::\n\n\nhere we can see the differences\n\n\n::: {.cell}\n\n```{.python .cell-code}\n3 - 2.9\n2 - 1.9\n1 - 0.9\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.10000000000000009\n0.10000000000000009\n0.09999999999999998\n```\n\n\n:::\n:::\n\n\nSame thing with SQL where differences are masked \n\n\n::: {.cell output.var='float_math'}\n\n```{.sql .cell-code}\nselect\n  3 - 2.9 as sub_three,\n  2 - 1.9 as sub_two,\n  1 - 0.9 as sub_one,\n  (3 - 2.9) <= 0.1 as sub_compare_three,\n  (2 - 1.9) <= 0.1 as sub_compare_two,\n  (1 - 0.9) <= 0.1 as sub_compare_one\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  sub_three sub_two sub_one sub_compare_three sub_compare_two sub_compare_one\n1       0.1     0.1     0.1                 0               0               1\n```\n\n\n:::\n:::\n\n\nInstead, can use either built-in equality checker (python equivalent is `math.isclose`) or check that difference between two numbers is in very small window\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall.equal(1 - 0.9, 0.1)\n\nabs( (1-0.9) - 0.1 ) <= 1e-10\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nAnother example\n\n\n::: {.cell}\n\n```{.r .cell-code}\n0.6 + 0.3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9\n```\n\n\n:::\n\n```{.r .cell-code}\n0.6 + 0.3 == 0.9\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(.1 + .2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(.1 + .2, digits = 18)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.30000000000000004\n```\n\n\n:::\n:::\n\n\n\n### Division by zero\n\nmore of a design issue about the right way to handle\n\nwe've seen before how we have to understand other peoples use of null values\n\nthis is a case where we he to decide what makes most sense\n\n## Strings (WIP)\n\nString data can be inherently appealing. At their best, strings are used to bring more readable and human interpretable values into a dataset. However, string data and the processing thereof comes with its own challenges. \n\nFirst, unlike numbers, human language strings can be ambiguously defined. `2` is the only number to represent the value of two. However, the incorporation of human language means that many different words, phrases, and formatting choices can represent the same concept. This is confounded by instances where string data was manually entered, as is the case with user-input data.^[I can accurately say my name is \"Emily\", \"Emily Riederer\", \"Emily E. Riederer\", \"Ms. Emily Riederer\". Additionally, if I spell my name over the phone, I can likely end up \"Emily Rieder\" or if I type it and mindlessly accept autocorrect, I'm \"Emily Reindeer\". This inconsistently may be problematic if you are trying to combine my data across different sources where I provided my name in different ways.]\n\nSecondly, string data is one of the most flexible datatypes and can contain any other types of information -- from should-be-logical values (`\"yes\"/\"no\"`, `\"true\"/\"false\"`), should-be-numeric values (`\"27\"`), should-be-date values (`\"2020-01-01\"`), and even complex data encodings like JSON blobs (`\"{\"name\":{\"first\":\"emily\",\"last\":\"riederer\"},\"social\":{\"twitter\":\"emilyriederer\",\"github\":\"emilyriederer\",\"linkedin\":\"emilyriederer\"}}\"` with hideous formatting for emphasis.) For a data publisher, this may be a convenience, but as we will see it can turn into a frustration or a liability when functions and comparison operations are attempted with strings that semantically represent a different type of value.\n\n### Dirty Strings (TODO)\n\nwhitespace\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\"a\" == \"a\"\n\"a b\" == \"a b\"\n\"a b\" == \"a  b\"\n\"a b\" == \"a b \"\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n[1] TRUE\n[1] FALSE\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n\"fancy\" characters (alternate encodings like ms word)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n' \" ' == ' \" '\n' â€œ ' == ' \" '\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n[1] FALSE\n```\n\n\n:::\n:::\n\n\nspecial characters and display versus values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- \"a\\tb\"\ncat(x) # what you see...\nx == \"a    b\" # ...is not what you get\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\na\tb[1] FALSE\n```\n\n\n:::\n:::\n\n\n### Regular Expressions (TODO)\n\nwe promised not to be solution oriented, but\n\n*not* knowing regex is a disaster when trying to work with string data...\n\n### Comparison\n\nTODO\n\n#### String ordering\n\nStrings are ranked based on *alphabetical order* just like a dictionary. Some properties of this ordering include that:\n\n- numbers are smaller than letters (`1 < \"a\"`)\n- lower-case is smaller than upper case (`\"a\" < \"A\"`)\n- fewer characters are smaller than more characters (`\"a\" < \"aa\"`)\n\nSuch rules make perfect sense for true characters. However, when strings are used as a \"catch all\" to represent other structures, typical comparison operators can produce odd results. For example, it is generally uncontroverisal that ninety-one is less than one hundred twenty. However, the string `\"91\"` is *greater than* `\"120\"` because only the character `\"9\"` is compared to the character `\"1\"`.^[Similarly, `\"91\" > \"905\"` because since they both start with 9, we move on to compare 1 which is greater than 0.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n91 < 120\n\"91\" < \"120\"\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n[1] FALSE\n```\n\n\n:::\n:::\n\n\nWhen strings are used to represent dates and times, comparison operators may or may not work depending on the precise formatting conversions. Below, we see that \"YYYYQQ\"-formats sort correctly because the information is hierarchically nested; millenia are compared before centuries, centuries before decades, decades before years, and years before quarters. However, many other string representations of dates, like \"QQ-YYYY\" will not order correctly. Related topics will be discussed in the \"Dates and Times\" section.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\"20190Q4\" < \"2020Q3\" # string (alphabetic) ordering same as semantic ordering\n\"Q4-2019\" < \"Q3-2020\" # string and semantic orderings are different\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n[1] FALSE\n```\n\n\n:::\n:::\n\n\nThese examples demonstrate that we shouldn't rely on sorting schemes that follow different rules. Before doing comparisons on such types, its a safer bet to cast them to the format most truly representative of their types. If, for some reason, you do wish to keep them as strings, the second example shows that its is wise to format them in the most conducive format possible so things just work. \n\n#### Type coercion \n\nWe discussed string comparison before when looking at \"dirty\" strings. More unexpected behavior arises when strings are compared across different data types. Many computing programs will attempt to *coerce* the objects to a similar and comparable type. Sometimes, this can be convenient as operations \"just work\", but as always there is a cost for convenience. As we'll see, delegating important decisions to our computing engine may not always capture the semantic relationships that we are most interested in. \n\nFor example, consider compare a string and a number. To make them more comparable, R will convert them both to strings before checking for equality. Thus, the number `2020` is equivalent to the string `\"2020\"` but not the string `\"02020\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\"2020\" == 2020\n\"02020\" == 2020\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n[1] FALSE\n```\n\n\n:::\n:::\n\n\nIn contrast, SQLite^[Different versions of SQL may differ] thinks that the string `'2020'` is greater than the number 2020 and that these two quantities are not equal.\n\n\n::: {.cell output.var='data_date'}\n\n```{.sql .cell-code}\nSELECT\n  case when     '2020' =  2020 then 1 else 0 end as is_eq,\n  case when not '2020' == 2020 then 1 else 0 end as not_eq,\n  case when     '2020' <  2020 then 1 else 0 end as is_lt,\n  case when     '2020' >  2020 then 1 else 0 end as is_gt\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  is_eq not_eq is_lt is_gt\n1     0      1     0     1\n```\n\n\n:::\n:::\n\n\n^TODO: where this could cause problems (FIPS example?)\n\n### Transformation (TODO)\n\nbasic things like addition differ by language\n\nin R, returns error:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n'a' + 'b'\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in \"a\" + \"b\": non-numeric argument to binary operator\n```\n\n\n:::\n\n```{.r .cell-code}\n'a' * 5\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in \"a\" * 5: non-numeric argument to binary operator\n```\n\n\n:::\n:::\n\n\nin SQLite, goes to zero:\n\n\n::: {.cell output.var='string_math'}\n\n```{.sql .cell-code}\nSELECT\n  'a' + 'b' as string_add,\n  'a'*5 as string_mult\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  string_add string_mult\n1          0           0\n```\n\n\n:::\n:::\n\n\nin python, does concatenation for `+` and analogous (concatenation of repeat) for `*`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n'a' + 'b'\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'ab'\n```\n\n\n:::\n\n```{.python .cell-code}\n'a' * 5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'aaaaa'\n```\n\n\n:::\n:::\n\n\n## Dates and Times (WIP)\n\nUnlike character strings, dates and times seem like they should be well defined with distinct, quantifiable components like years, months, and days. However, many different conventions for date formatting and underlying storage formats exist. This leads to similar challenges with dates and times as we saw with strings before.\n\nSome common formats in the wild are:\n\n- YYYYMMDD\n- YYYYMM\n- MMDDYYYY\n- DDMMYYYY\n- MM/DD/YYYY\n- MM/DD/YY\n- DD/MM/YYYY\n- YYYY-MM-DD (ISO8601)\n\nIn addition to how dates are *formatted*, they may be stored in a variety of different ways \"under the hood\" such as Unix time (seconds since 1970-01-01 00:00:00 UTC) and Julian time (days since noon in Greenwich on November 24, 4714 B.C) (TODO). \n\nTo complicate matters further, many of these formats may be represented either by native date types in various programs or by more basic data types (such as integers for the first four and strings for the last four). In addition, analogous formats exist for *timestamps* which encode both calendar date and time of day (hour, minute, and second information).\n\nTODO: why ISO8601?\n\n### Comparison\n\nAutomatic conversion of data types\nDates versus timestamps \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_dt <-\ndata.frame(\n  DT_ENROLL = as.Date(\"2020-01-01\"),\n  DT_PURCH  = 20200101,\n  DT_LOGIN  = as.POSIXlt(\"2020-01-01T12:00:00\") \n  )\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\nnone of these are equal so nothing returns on filtering\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(df_dt, DT_ENROLL == DT_PURCH) %>% nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\nfilter(df_dt, DT_ENROLL == DT_LOGIN) %>% nrow()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in mask$eval_all_filter(dots, env_filter): Incompatible methods\n(\"Ops.Date\", \"Ops.POSIXt\") for \"==\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\nthe same thing happens in sql \n\n\n::: {.cell output.var='date1'}\n\n```{.sql .cell-code}\nselect * from df_dt where DT_ENROLL = DT_PURCH\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] DT_ENROLL DT_PURCH  DT_LOGIN \n<0 rows> (or 0-length row.names)\n```\n\n\n:::\n:::\n\n::: {.cell output.var='date2'}\n\n```{.sql .cell-code}\nselect * from df_dt where DT_ENROLL = DT_LOGIN\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] DT_ENROLL DT_PURCH  DT_LOGIN \n<0 rows> (or 0-length row.names)\n```\n\n\n:::\n:::\n\n\nin what way aren't they equal? to understand this its helpful to know how the computer encodes these dates\n\nwith `as.numeric()` in R we can see the numeric representation of the date\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.numeric(df_dt$DT_ENROLL)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18262\n```\n\n\n:::\n:::\n\n\nthis works the same way in SQL\n\n\n::: {.cell output.var='date_num'}\n\n```{.sql .cell-code}\nselect cast(DT_ENROLL as integer), cast(DT_PURCH as integer) from df_dt\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  cast(DT_ENROLL as integer) cast(DT_PURCH as integer)\n1                      18262                  20200101\n```\n\n\n:::\n:::\n\n\nthis has the implication that things that are on the same date have an inequality relationship in both languages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(df_dt, DT_ENROLL < DT_PURCH) %>% nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n::: {.cell output.var='date_ineq'}\n\n```{.sql .cell-code}\nselect \n  cast(DT_ENROLL as integer), \n  case when DT_ENROLL < 18000 then 1 else 0 end as lt_18000,\n  case when DT_ENROLL < 19000 then 1 else 0 end as lt_19000,\n  case when DT_ENROLL < DT_PURCH then 1 else 0 end as lt_purch\nfrom df_dt\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  cast(DT_ENROLL as integer) lt_18000 lt_19000 lt_purch\n1                      18262        0        1        1\n```\n\n\n:::\n:::\n\n\nNote this this can affect both filters and joins\n\nand this similarly causes a more general problem when comparing a date to a date-as-an-integer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.Date(\"2020-01-01\") > 20160501\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n::: {.cell output.var='date_comp'}\n\n```{.sql .cell-code}\nselect cast('2020-01-01' as date) > 20160501\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  cast('2020-01-01' as date) > 20160501\n1                                     0\n```\n\n\n:::\n:::\n\n\n<!--\n\n### Timestamps versus dates\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoday <- as.Date(Sys.Date()) \nnow <- as.POSIXct(Sys.time(), \"America/New_York\")\ntoday == now\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Incompatible methods (\"Ops.Date\", \"Ops.POSIXt\") for \"==\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\ntoday > now\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Incompatible methods (\"Ops.Date\", \"Ops.POSIXt\") for \">\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\ntoday < now\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Incompatible methods (\"Ops.Date\", \"Ops.POSIXt\") for \"<\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom datetime import date\nfrom datetime import datetime\n\ntoday = date.today()\nnow = datetime.now()\ntoday == now\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse\n```\n\n\n:::\n\n```{.python .cell-code}\ntoday < now\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in py_call_impl(callable, dots$args, dots$keywords): TypeError: can't compare datetime.datetime to datetime.date\n\nDetailed traceback:\n  File \"<string>\", line 1, in <module>\n```\n\n\n:::\n\n```{.python .cell-code}\ntoday > now\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in py_call_impl(callable, dots$args, dots$keywords): TypeError: can't compare datetime.datetime to datetime.date\n\nDetailed traceback:\n  File \"<string>\", line 1, in <module>\n```\n\n\n:::\n\n```{.python .cell-code}\ntoday_str = today.strftime('%Y-%m-%d')\nnow_str = now.strftime('%Y-%m-%d %H:%M:%S')\ntoday_str == now_str\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse\n```\n\n\n:::\n\n```{.python .cell-code}\ntoday_str < now_str\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\ntoday_str > now_str\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse\n```\n\n\n:::\n:::\n\n\n-->\n\n## Changing Data Types (TODO)\n\ndata types can incidentally change between programs\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(DT = as.Date(\"2020-10-01\"))\nprint(\"-- At creation --\")\nsprintf(\"Class of DT variable: %s\", class(df$DT))\ntmp <- tempfile()\nwrite.csv(df, file = tmp, row.names = FALSE)\ndf <- read.csv(file = tmp)\nprint(\"-- After saving and reloading with read.csv --\")\nsprintf(\"Class of DT variable: %s\", class(df$DT))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"-- At creation --\"\n[1] \"Class of DT variable: Date\"\n[1] \"-- After saving and reloading with read.csv --\"\n[1] \"Class of DT variable: character\"\n```\n\n\n:::\n:::\n\n\nsome tools try to make \"smart\" guesses based on format\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\n\ndf <- readr::read_csv(file = tmp)\nprint(\"-- After saving and reloading with readr --\")\nsprintf(\"Class of DT variable: %s\", class(df$DT))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"-- After saving and reloading with readr --\"\n[1] \"Class of DT variable: Date\"\n```\n\n\n:::\n:::\n\n\nhowever, this can make things slower or unideal (one example: https://github.com/tidyverse/readr/issues/1094#issuecomment-628612430). \nyou can also specify your own types manually\n\n## Factors in R (TODO)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfactor(c(0,1,0,1)) == 1\nas.integer(factor(c(0,1,0,1))) == 1\nas.integer(factor(c(0,1,0,1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE  TRUE FALSE  TRUE\n[1]  TRUE FALSE  TRUE FALSE\n[1] 1 2 1 2\n```\n\n\n:::\n:::\n\n\n## Programming Errors (TODO)\n\n### Default Cases (WIP)\n\nsee case-when example in nulls section\n\n### Order of Operations (WIP)\n\nPEMDAS but sometimes still ambiguous\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 + 1  * 2 / 3 - 1\n(1 + 1) * 2 / 3 - 1\n1 + 1 * 2 / (3 - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6666667\n[1] 0.3333333\n[1] 2\n```\n\n\n:::\n:::\n\n\nSQL clause order of evaluations\n\n### Object References (WIP)\n\nCopying and modifying object overview\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Different relationships between named variables and their values](figures/comp-quan/ref-val.PNG){fig-align='center' width=90%}\n:::\n:::\n\n\nWhen might each be preferred?\n\nWhat risks are there if we don't understand which we are doing?\n\nIn Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = [1,2,3]\ny = x\ny.append(4)\nprint(y)\nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1, 2, 3, 4]\n[1, 2, 3, 4]\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nz = x.copy()\nz.append(5)\nprint(z)\nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1, 2, 3, 4, 5]\n[1, 2, 3, 4]\n```\n\n\n:::\n:::\n\n\npandas DataFrame methods with `inplace` arg (`False` is default)\n\nIn R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\n\nDT <- data.table(a=c(1,2), b=c(11,12))\nprint(DT)\n\nnewDT <- DT        # reference, not copy\nnewDT[1, a := 100] # modify new DT\n\nprint(DT)          # DT is modified too.\n\nDT = data.table(a=c(1,2), b=c(11,12))\nnewDT <- DT        \nnewDT$b[2] <- 200  # new operation\nnewDT[1, a := 100]\n\nprint(DT)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   a  b\n1: 1 11\n2: 2 12\n     a  b\n1: 100 11\n2:   2 12\n   a  b\n1: 1 11\n2: 2 12\n```\n\n\n:::\n:::\n\n\nFrom https://stackoverflow.com/questions/10225098/understanding-exactly-when-a-data-table-is-a-reference-to-vs-a-copy-of-another\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\n# set-up sample data ----\ndata = {'a': [1, 2], \n        'b': [11, 12]}\ndf = pd.DataFrame(data = data)\n\n# rename columns without replacing ----\ndf.rename(columns = {'a':'x'})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x   b\n0  1  11\n1  2  12\n```\n\n\n:::\n\n```{.python .cell-code}\ndf\n\n# rename columns with replacing ----\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   a   b\n0  1  11\n1  2  12\n```\n\n\n:::\n\n```{.python .cell-code}\ndf.rename(columns = {'a':'x'}, inplace = True)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x   b\n0  1  11\n1  2  12\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_ones <- function(data) {\n  \n  data$x0 <- rep(0, nrow(data))\n  \n}\n\ndf <- data.frame(x1 = 1:5)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  x1\n1  1\n2  2\n3  3\n4  4\n5  5\n```\n\n\n:::\n\n```{.r .cell-code}\nadd_ones(df)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  x1\n1  1\n2  2\n3  3\n4  4\n5  5\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_ones <- function(data) {\n  \n  data$x0 <- rep(0, nrow(data))\n  return(data)\n  \n}\n\ndf <- data.frame(x1 = 1:5)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  x1\n1  1\n2  2\n3  3\n4  4\n5  5\n```\n\n\n:::\n\n```{.r .cell-code}\nadd_ones(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  x1 x0\n1  1  0\n2  2  0\n3  3  0\n4  4  0\n5  5  0\n```\n\n\n:::\n\n```{.r .cell-code}\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  x1\n1  1\n2  2\n3  3\n4  4\n5  5\n```\n\n\n:::\n\n```{.r .cell-code}\ndf2 <- add_ones(df)\ndf2 \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  x1 x0\n1  1  0\n2  2  0\n3  3  0\n4  4  0\n5  5  0\n```\n\n\n:::\n:::\n\n\n\n## Trusting Tools\n\n### Delegating decisions\n\nA theme throughout this book is the fundamentally *social* nature of data analysis. Data analysis is fraught without understanding the countless decisions made along the way by those who generated it (whose data is reflected), those who collected it, those who migrated it, and those who have posed questions of it. On one hand, this is a beautiful aspect of analysis; on the other hand, it means that analysts and their analyses are subject to all of the cognitive and social psychological biases of everyday humans.\n\nOne such bias is \"social proof\": assuming that if a tool behaves a certain way, it must be because it is correct. \n\nAssuming that our tools know best is admittedly an attractive proposition. It appeals to a desire to think that someone, somewhere is \"in charge\" and, perhaps more critically, helps us avoid a domino effect of distrust (If we *don't* trust our tools how can we trust our results? And if we can't trust our results, how can we trust anything at all?) Unfortunately, there are many reasons are tools might not know best. For example, the tool's developer might have:\n\n- Made a mistake\n- Had a different analysis problem in mind with a different optimal approach\n- Been optimizing for a different constraint (e.g. explainability vs. accuracy, speed vs. theoretical properties)\n- Come from a community with different norms\n- Been affording users the flexibility to do things many ways even if they don't agree\n- Built a certain feature for a different purpose than how you are using it\n- Not thought about it at all\n\nAs a few concrete examples from popular open source tools. We'll look briefly at the prominent python library `scikitlearn` for machine learning and Apache Spark, an engine for large-scale distributed data processing.\n\n#### Defaults in `scikitlearn`\n\n`scikitlearn`'s default behavior for logistic regression modeling^[A classic modeling technique for predicting binary (yes/no) outcomes] automatically applies L2 regularization. You might or might not know what this means, and you might or might not want to apply it to your problem. That's fine. The important thing is that it *will* change your estimates and predictions, and it is *not* a part of the classical definition of that algorithm (for modelers coming from a statistical background.) \n\nOf course, there's nothing inherently wrong about this choice; the library authors just had different goals than a typical statistical. `scikitlearn` developer Olivier Grisel explains [on Twitter](https://twitter.com/ogrisel/status/1167438229655773186?s=20) that this choice (and others in the library) is explained because \"Scikit-learn was always designed to make it easy to get good predictive accuracy (eg as measured by CV) rather than as statistical inference library.\" Additionally, this choice is documented in bold [in the function documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). \n\nHowever, an analyst could easily miss this nuance if they do not *read* the documentation. Or, if they *misinterpret* this choice as social proof that regularization is always the right approach, they might not make the best choice for their own analysis.\n\n#### Algorithms in `Spark`\n\nAs a second example, according to a 2015 [Jira ticket](https://issues.apache.org/jira/browse/SPARK-5133), developers of Spark considered multiple methodologies they could use when adding the functionality to compute feature importance for a random forest. Ultimately, a core contributor advised against permutation importance due to its computational cost. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![JIRA ticket for Spark with a discussion of which random forest variable importance algorithm to implement](figures/comp-quan/spark-jira.PNG){fig-align='center' width=90%}\n:::\n:::\n\n\nClearly, no one wants a workflow that is too costly or timely to run. So, once again, there is no right or wrong. However, since every approach to feature importance has its own biases, pitfalls, and challenges in interpretation, it's a mistake for an end-user to not carefully understand which algorithm is used and why.\n\n#### Null handling in `ggplot2` (TODO)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\ndf <- data.frame(x1 = 1:5, \n                 x2 = c(1, 2, NA, NA, 5),\n                 y1 = 1:5,\n                 y2 = c(1, 2, NA, NA, 5))\nggplot(df, aes(x1, y1)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](comp-quan_files/figure-html/unnamed-chunk-234-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(df, aes(x1, y2)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](comp-quan_files/figure-html/unnamed-chunk-234-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(df, aes(x2, y1)) + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](comp-quan_files/figure-html/unnamed-chunk-234-3.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(df, aes(x2, y2)) + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](comp-quan_files/figure-html/unnamed-chunk-234-4.png){width=672}\n:::\n:::\n\n\n\n#### Boxplots in `ggplot2` (TODO)\n\ndepending on how you change the scale it also changes the calculations\n\nhttps://stackoverflow.com/questions/5677885/ignore-outliers-in-ggplot2-boxplot\n\n### \"Off-Label\" Use (TODO)\n\ncoined in https://www.rstudio.com/resources/rstudioglobal-2021/maintaining-the-house-the-tidyverse-built/\n\n### Security (TODO)\n\nnamespace squatting\n\nexecutable code\n\n## Inefficient Processing (TODO)\n\n## Strategies (WIP)\n\nParagraph 1 TODO\n\nSome computational quandaries are inherent to our tools themselves, but often they are a function both of the tools and the ways we chose to use them. More strategies related to writing robust and resilient code will be discussed in @sec-comp-code (Complexify Code).\n\n### Understand the intent\n\n- read the docs\n- look at examples\n- don't carry default knowledge between languages\n\n### Understand the execution\n\n- test out simple examples (like we've been doing)\n- specificlly try out corner cases\n\n### Be explicit not implicit\n\n- default arguments\n- examples above with casting, coalescing\n\n## Real World Disasters (WIP)\n\nhttps://www.theguardian.com/politics/2020/oct/05/how-excel-may-have-caused-loss-of-16000-covid-tests-in-england\n\n> The data error, which led to 15,841 positive tests being left off the official daily figures, means than 50,000 potentially infectious people may have been missed by contact tracers and not told to self-isolate.\n\nhttps://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008984\n\nThe Excel gene error has not been corrected\n\n\n::: {.cell}\n\n:::\n\n\n<!--\n\nIn practice, data analysis requires using a number of advanced computational tools such as SQL, R, or python. Analysts must work in a partnerships with each tool to access and wrangle their data into an accessible form of information. However, the moment you as an analyst begins to use a tool, the conversation is no longer simply between you and the data; suddenly, thousands of developers who helper build your tools are now crowding into the room. Each may have contributed to the code behind your tool with a different mental model of how one would use it.\n\nIn this chapter, we will explore common ways that tools may do something correct, reasonable, and as-intended but very much not what we would have liked as analysts.\n\n\n\n\n\nTODO: Introduce datasets\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(registration)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ID_CUSTOMER CD_DEVICE AMT_SPEND AMT_RETURN\n1           1         1        10         NA\n2           2         1        20         NA\n3           3         1        30         NA\n4           4         2        40         NA\n5           5         2        50         NA\n6           6         2        NA         NA\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n1+1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n## The meaning of a row\n\nOften, data comes with an accompanying data dictionary to describe the meaning of each variable (column). However, for some reason, describing what constitutes an observation (row) is less common practice. This may seem obvious. In two of R's most popular built-in datasets, `iris` and `mtcars`, it's somewhat evident from context that each row represents one flower or one car. \n\nHowever, sometimes trying to glean this information from context alone can be misleading. Imagine, for example, that we want to calculate the rate of successful log-ins to our e-commerce website.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tidyr' was built under R version 4.0.5\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarize(logins, PROP_LOGIN = mean(IND_LOGIN))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 1\n  PROP_LOGIN\n       <dbl>\n1        0.4\n```\n\n\n:::\n\n```{.r .cell-code}\nlogins %>%\n  group_by(ID_ACCT) %>% \n  summarize(IND_LOGIN = max(IND_LOGIN)) %>%\n  ungroup() %>% \n  summarize(PROP_LOGIN = mean(IND_LOGIN))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 1\n  PROP_LOGIN\n       <dbl>\n1        0.6\n```\n\n\n:::\n:::\n\n\n## The many forms of null\n\nFrequently, real-world data sets suffer from at least some missing values. This missing data can cause complex computational and analytical challenges. Considering why one's data is missing and how such missing is encoded is critical before attempting an analysis.\n\nIt's tempting to think of missingness as a binary status: either a datum exists or it does not. However, missingness can arise from a number of different situations -- each with its own unique computational and analytical challenges. For example, in a tabular data representation, a variable (column) for an observation (row) might appear to be missing because:\n\n- There exists a true value for that variable and entity but it is unknown\n- There is no relevant value for that variable\n- The relevant value for that variable *is* null\n\nFor example, consider the `registrations` dataset with data on users that have created an account at an online e-commerce platform:\n\n- Users might be asked but not required to provide their date of birth; so, while every user *has* a birthday, only those that provide it would have a value for this field. Those that do not would be encoded as a `NULL`; (could also be only started collecting after certain dates, at a certain set of stores, data load error)\n- We might also wish to record for mobile users if they were on an Android or and iPhone device when they registered. However, for users registering from a computer, there is no relevant value for this field. \n- Often, retailers want to attribute user traffic to different forms of advertising, so we might also have a field for the URL that directed users to our site. However, for users that truly typed in the URL directly and did not come through an affiliate link, the \"true value\" of the referring site is `NULL`. (Admittedly, there is significant overlap in the second two cases.)\n\nJust as there are many potential causes for missingness in our data, there can often be many potential *encodings* of missingness.\n\nThe **naniar** package [@R-naniar] \\index{R package!naniar}\n\nTalk about encoding as like 9999\n\nR has `NA`, `NaN`, `Inf`, `NULL`, and typed `NA`s\n\nWhile this section focuses on the computational challenges of null values, no discussion of missingness would be complete without also mentioning the analytical consequences as well. Many statistical techniques such as linear regression are unable to accept null values in their inputs, so an analyst must somehow confront missingness before passing their data to the algorithm. Broadly speaking, analysts must either remove missing values or replace them with a proxy imputed value. There is a rich literature on missingness beyond the scope of this book, but briefly speaking this choice can be guided by the following framework. \n\nThese different types of missingness must be handled differently because they insert different biases into our data. Someone (TODO: who and add citiation) classifies the level of missingness as:\n\n- Missing Completely at Random (MCAR): TODO: finish defining these\n- Missing at Random (MAR):\n- Missing not a Random (MNAR):\n\nComparing these categories to the examples above, \n\nLiterature about imputation; mention **mice** package (???)\n\nOne example of this is evident in the US Census Bureau's Medical Expenditure Panel Survey which uses the following reserved codes to denote different types of missingness. (TODO p10 https://www.meps.ahrq.gov/data_stats/download_data/pufs/h206a/h206adoc.pdf)\n\n- -1 INAPPLICABLE Question was not asked due to skip pattern\n- -7 REFUSED Question was asked and respondent refused to answer question\n- -8 DK Question was asked and respondent did not know answer\n- -14 NOT YET TAKEN/USED Respondent answered that the medicine has not yet been used\n- -15 CANNOT BE COMPUTED Value cannot be derived from data\n\n## Null value aggregation\n\nAs we've seen, null values in our data can mean many different things and be represented in many different ways. However, even once we have locked down the *semantic* meaning of nulls in our data and considered alternatives for their *encoding*, these values may still cause *computational* challenges.^[This problem is not isolated to data analysis tools. For an entertaining example, see the 2019 WIRED article \"How a 'NULL' License Plate Landed One Hacker in Ticket Hell\" [@barrett] which a real-world software system producing unintended and undesirable behavior when asked to deal with a word `'NULL'`.] How null values are handled in the simple aggregation of data varies both across different languages and across different functions within a language. To better understand the problems this might cause, we will look at examples in R and SQL.\n\nFirst, consider the `registration` data set. To compute the average amount spent (`AMT_SPEND`) with the `dplyr` package, an analyst might first reasonably write the following `summarize()` statement. However, as we can see, due to the presence of null values within the `AMT_SPEND` column, the result of this aggregation is for the whole quantiaty of `AVG_SPEND` to be set to the value `NA`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarize(registration, AVG_SPEND = mean(AMT_SPEND))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  AVG_SPEND\n1        NA\n```\n\n\n:::\n:::\n\n\nA glance at the documentation for the `mean()` function reveals that it has an `na.rm` parameter which, when set to true, removes null values from our dataset. Adding this argument to the previous statement allows us to reach a numerical answer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarize(registration, \n          AVG_SPEND = mean(AMT_SPEND, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  AVG_SPEND\n1  48.57143\n```\n\n\n:::\n:::\n\n\nHowever, is this the *right* numerical answer? What `na.rm = TRUE` does is *drop* the null values from the set of numbers being averaged. However, suppose the null values represent that no purchases were made. That is, zero dollars were spent. In effect, we have removed all non-purchasers from the data being averaged. \n\nMore precisely, we have switched from taking the average\n\n$$\n  \\frac{ \\sum_{1}^{n} Spend }{\\sum_{1}^{n} 1}\n$$ over all $n$ customers\n\nto taking the average\n\n$$\n  \\frac{ \\sum_{Spend > 0} Spend }{\\sum_{Spend > 0} 1}\n$$ over only those customers with spend\n\nAt face value, we could say that the code above is giving the incorrect answer; by dropping some low (zero) purchase amounts, the average amount spend per customer is inflated. A second and even more troubling perspective is that this tiny change to the code which seemed like a reasonable attempt to fix an *obvious* problem has introduced a *non-obvious* problem by fundamentally changing the question that we are asking. By dropping all accounts from our table who made no purchases, we are no longer answering \"What is the average amount spent by a new registrant?\" but rather \"What is the average amount spent by an actively engaged customer?\" This technical quirk has significant analytical impact.\n\nTo answer the real question at hand, we would instead have a couple of options. We could `sum()` the amount spent with the option to drop nulls but then divide by the correct denominator (all observations -- not just those with spend) or we could explicitly recode null values in `AMT_SPEND` to zero before taking the average. This leads to a lower but correct conclusion.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarize(registration,\n    AVG_SPEND_v1 = sum(AMT_SPEND, na.rm = TRUE) / n(),\n    AVG_SPEND_v2 = mean(ifelse(is.na(AMT_SPEND), 0, AMT_SPEND))\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  AVG_SPEND_v1 AVG_SPEND_v2\n1           34           34\n```\n\n\n:::\n:::\n\n\nThis is all well and good if we could just accept that the behaviors above are simply how nulls work, but further complexity comes as we see that there is no industry standard across tools. For example, as the SQL code below shows, SQL's `avg()` function behaves more like R's `mean()` *with* the `na.rm = TRUE` option set. That is, the default behavior of SQL is to only operate on the valid and available values. \n\n\n::: {.cell output.var='sql1'}\n\n```{.sql .cell-code}\nSELECT avg(amt_spend) \nFROM registration\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  avg(amt_spend)\n1       48.57143\n```\n\n\n:::\n:::\n\n\nHowever, this is not to suggest that null values cannot also be destructive in SQL. While aggregation functions (which compute across a *row*) like `sum()` and `avg()` drop nulls, operators like `+` and `-` working *across columns* in the *same row* do not exhibit the same behavior. Consider, for example, if we wish to calculate the average net purchase amount (purchases minus returns) instead of the gross (total) purchase amount. \n\n\n::: {.cell output.var='sql2'}\n\n```{.sql .cell-code}\nSELECT avg(amt_spend-amt_return) \nFROM registration\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  avg(amt_spend-amt_return)\n1                        NA\n```\n\n\n:::\n:::\n\n\nDespite what we learned above about the `avg()` function, the query above returns only a null value. What has happened? In our `registration` data set, the `amt_return` column is completely null (representing no returns). Because the subtraction occurs before the average is taken, subtracting real numbers in `amt_spend` with null values in `amt_return` creates a column of all null values which are then fed into the `avg()` function. This process is shown step-by-step below.\n\n\n::: {.cell output.var='sql3'}\n\n```{.sql .cell-code}\nSELECT\n  amt_spend, \n  amt_return, \n  amt_spend-amt_return \nFROM registration\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   AMT_SPEND AMT_RETURN amt_spend-amt_return\n1         10         NA                   NA\n2         20         NA                   NA\n3         30         NA                   NA\n4         40         NA                   NA\n5         50         NA                   NA\n6         NA         NA                   NA\n7         NA         NA                   NA\n8         NA         NA                   NA\n9         90         NA                   NA\n10       100         NA                   NA\n```\n\n\n:::\n:::\n\n\n## Null value filtering\n\n`dplyr` excludes `NA`s in `filter`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(x = c(1, 0, NA)) %>%\n  filter(x != 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  x\n1 0\n```\n\n\n:::\n:::\n\n\n`SQL` excludes `NA`s in `WHERE`\n\n\n::: {.cell output.var='sql4'}\n\n```{.sql .cell-code}\nselect *\nfrom registration\nwhere amt_spend != 10\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  ID_CUSTOMER CD_DEVICE AMT_SPEND AMT_RETURN\n1           2         1        20         NA\n2           3         1        30         NA\n3           4         2        40         NA\n4           5         2        50         NA\n5           9         3        90         NA\n6          10         3       100         NA\n```\n\n\n:::\n:::\n\n\n```\nimport pandas as pd\nd = {'col1': [1, None], 'col2': [3, 4]}\ndf = pd.DataFrame(data=d)\ndf\ndf.query('col1 > 0')\ndf[df['col1'] > 0]\n```\n\n## Encoding with defaults\n\nOften, to faciliate an analysis, we wish to **recode** variables from one form to another. There are many reasons we might wish to recode variables. Sometimes, our data may be represented with system-generated codes that obscure the context-specific meaning of fields. For example, if you are analyzing US Census data, it might identify different regions by their FIPS code while a human might prefer to see the actual name of a state or a county. In other cases, we wish to recode fields in order to change their level of granularity. For example, we might wish to group categories such as \"apple\", \"banana\", and \"orange\" into the \"fruit\" category.\n\nCommon functions for recoding include `base::ifelse`\\index{base!ifelse}\\index{R!ifelse}, `dplyr::if_else`\\index{dplyr!if\\_else}\\index{R!if\\_else}, and `dplyr::case_when`\\index{dplyr!case\\_when}\\index{R!case\\_when} in R and `CASE` statements in SQL\\index{SQL!case}. Generally, all of these work by:\n\n- specifying one or more logical conditions based on other column(s) in the dataset\n- for each logical condition, specifying the new value that the variable should take\n- if none of the conditions are met, providing a default value\n\nHowever, analysts may often take on slight short-cut\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregistration <-\nmutate(registration,\n       CAT_DEVICE = case_when(\n         CD_DEVICE == 1 ~ \"IOS\",\n         TRUE ~ \"Android\"\n       )\n       )\n\nregistration %>%\n  group_by(CAT_DEVICE) %>%\n  count()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n# Groups:   CAT_DEVICE [2]\n  CAT_DEVICE     n\n  <chr>      <int>\n1 Android        7\n2 IOS            3\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nunique(registration$CD_DEVICE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 2 3\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nregistration <- \n  mutate(registration, \n         IND_SMALL_PURCH = case_when(\n           AMT_SPEND < 50 ~ 1,  \n           TRUE ~ 0)\n         )\n                       \nregistration %>%\n  group_by(IND_SMALL_PURCH) %>%\n  count()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n# Groups:   IND_SMALL_PURCH [2]\n  IND_SMALL_PURCH     n\n            <dbl> <int>\n1               0     6\n2               1     4\n```\n\n\n:::\n\n```{.r .cell-code}\nregistration <-\n  mutate(registration, \n         IND_SMALL_PURCH = if_else(AMT_SPEND < 50, 1, 0))\n                       \nregistration %>%\n  group_by(IND_SMALL_PURCH) %>%\n  count()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 2\n# Groups:   IND_SMALL_PURCH [3]\n  IND_SMALL_PURCH     n\n            <dbl> <int>\n1               0     3\n2               1     4\n3              NA     3\n```\n\n\n:::\n:::\n\n\n## Working with strings\n\n## Data Types - Max\n\n\n::: {.cell}\n\n```{.r .cell-code}\nany(\n  \"a\" == \"A\",\n  \" a\" == \"a\",\n  \"a \" == \"a\",\n  \"a  b\" == \"a b\",\n  \"â€˜zâ€™\" == \"'z'\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmax(c(\"20200101\", \"20190301\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"20200101\"\n```\n\n\n:::\n\n```{.r .cell-code}\nmax(c(\"03012019\", \"01012020\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"03012019\"\n```\n\n\n:::\n\n```{.r .cell-code}\nmax(c(\"120\",\"99\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"99\"\n```\n\n\n:::\n\n```{.r .cell-code}\nmax(c(\"120\",\"099\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"120\"\n```\n\n\n:::\n\n```{.r .cell-code}\nmax(c(120,99))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 120\n```\n\n\n:::\n\n```{.r .cell-code}\nmax(c(120,099))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 120\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.sql .cell-code}\nselect \n  max('20200101', '20190301'),\n  max('01012020', '03012019'),\n  max('120','99'),\n  max('120','099'),\n  max(120,99),\n  max(120,099)\n```\n\n\n<div class=\"knitsql-table\">\n\n\nTable: 1 records\n\n|max('20200101', '20190301') |max('01012020', '03012019') |max('120','99') |max('120','099') | max(120,99)| max(120,099)|\n|:---------------------------|:---------------------------|:---------------|:----------------|-----------:|------------:|\n|20200101                    |03012019                    |99              |120              |         120|          120|\n\n</div>\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmax([\"20200101\", \"20190301\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'20200101'\n```\n\n\n:::\n\n```{.python .cell-code}\nmax([\"03012019\", \"01012020\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'03012019'\n```\n\n\n:::\n\n```{.python .cell-code}\nmax([\"120\",\"99\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'99'\n```\n\n\n:::\n\n```{.python .cell-code}\nmax([\"120\",\"099\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'120'\n```\n\n\n:::\n\n```{.python .cell-code}\nmax([120,99])\n#max([120,099]) Python won't allow leading zeros on integers\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n120\n```\n\n\n:::\n:::\n\n\n## Data Types - Equality\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\"2020\" == 2020\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n\"02020\" == 2020\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.sql .cell-code}\nselect \n  '2020' == 2020,\n  '02020' == 2020,\n  cast('2020' as numeric) == 2020,\n  cast('2020' as integer) == 2020\n```\n\n\n<div class=\"knitsql-table\">\n\n\nTable: 1 records\n\n| '2020' == 2020| '02020' == 2020| cast('2020' as numeric) == 2020| cast('2020' as integer) == 2020|\n|--------------:|---------------:|-------------------------------:|-------------------------------:|\n|              0|               0|                               1|                               1|\n\n</div>\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n\"2020\" == 2020\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse\n```\n\n\n:::\n\n```{.python .cell-code}\n\"02020\" == 2020\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse\n```\n\n\n:::\n:::\n\n\n`python 1+1`\n\n## Working with dates and times\n\n### Many different formats\n\n- YYYYMMDD\n- MMDDYYYY\n- MM/DD/YYYY\n- MM/DD/YY\n- DDMMYYYY\n- DD/MM/YYYY\n\n### Timestamps versus dates\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoday <- as.Date(Sys.Date()) \nnow <- as.POSIXct(Sys.time(), \"America/New_York\")\ntoday == now\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Incompatible methods (\"Ops.Date\", \"Ops.POSIXt\") for \"==\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\ntoday > now\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Incompatible methods (\"Ops.Date\", \"Ops.POSIXt\") for \">\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\ntoday < now\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Incompatible methods (\"Ops.Date\", \"Ops.POSIXt\") for \"<\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom datetime import date\nfrom datetime import datetime\n\ntoday = date.today()\nnow = datetime.now()\ntoday == now\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse\n```\n\n\n:::\n\n```{.python .cell-code}\ntoday < now\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in py_call_impl(callable, dots$args, dots$keywords): TypeError: can't compare datetime.datetime to datetime.date\n\nDetailed traceback:\n  File \"<string>\", line 1, in <module>\n```\n\n\n:::\n\n```{.python .cell-code}\ntoday > now\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in py_call_impl(callable, dots$args, dots$keywords): TypeError: can't compare datetime.datetime to datetime.date\n\nDetailed traceback:\n  File \"<string>\", line 1, in <module>\n```\n\n\n:::\n\n```{.python .cell-code}\ntoday_str = today.strftime('%Y-%m-%d')\nnow_str = now.strftime('%Y-%m-%d %H:%M:%S')\ntoday_str == now_str\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse\n```\n\n\n:::\n\n```{.python .cell-code}\ntoday_str < now_str\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\ntoday_str > now_str\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse\n```\n\n\n:::\n:::\n\n\n## Assessing equality\n\nAutomatic conversion of data types\nDates versus timestamps \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_dt <-\ndata.frame(\n  DT_ENROLL = as.Date(\"2020-01-01\"),\n  DT_PURCH  = 20200101,\n  DT_LOGIN  = as.POSIXlt(\"2020-01-01T12:00:00\") \n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(df_dt, DT_ENROLL == DT_PURCH) %>% nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsqldf(\"select * from df_dt where DT_ENROLL = DT_PURCH\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] DT_ENROLL DT_PURCH  DT_LOGIN \n<0 rows> (or 0-length row.names)\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nas.numeric(df_dt$DT_ENROLL)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18262\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsqldf(\"select cast(DT_ENROLL as integer) from df_dt\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  cast(DT_ENROLL as integer)\n1                      18262\n```\n\n\n:::\n:::\n\n\nNote this this can affect both filters and joins\n\n## Merging mayhem\n\n\n## Modifying versus copying\n\n\n\n\n## Inefficient processing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000000\nx1 = 1:n\nx2 = 1:n\n\nstart_time <- Sys.time()\nfor (i in 1:n) {\n  x1[i] = x1[i] + 1\n}\nend_time <- Sys.time()\nt_for <- end_time - start_time\n\nstart_time <- Sys.time()\nx2 = x2 + 1\nend_time <- Sys.time()\nt_vec <- end_time - start_time\n\nt_for\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime difference of 0.131424 secs\n```\n\n\n:::\n\n```{.r .cell-code}\nt_vec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime difference of 0.256995 secs\n```\n\n\n:::\n\n```{.r .cell-code}\nas.numeric(t_for)/as.numeric(t_vec)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5113873\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport time\n\nn = 1000000\nx1 = list(range(n))\nx2 = np.array(x1)\n\nt0 = time.time()\nfor i in range(n):\n  x1[i] += 1\nt1 = time.time()\nt_for = t1-t0\n\nt0 = time.time()\nx2 += 1\nt1 = time.time()\nt_vec = t1-t0\n\nt_for\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.289048433303833\n```\n\n\n:::\n\n```{.python .cell-code}\nt_vec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.025002479553222656\n```\n\n\n:::\n\n```{.python .cell-code}\nt_for / t_vec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n11.560790708319029\n```\n\n\n:::\n:::\n\n\n-->\n",
    "supporting": [
      "comp-quan_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}